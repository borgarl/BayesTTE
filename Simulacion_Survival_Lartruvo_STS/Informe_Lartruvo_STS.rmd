---
title: "Assessment of the ANNOUNCE trial -LY3012207 (Olaratumab)- in patients with Advanced Soft Tissue Sarcoma (STS)"
author: "Borja G. López-Rey"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    code_folding: hide
vignette: >
  %\VignetteIndexEntry{Model checking with simulated data (survival model example)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, eval = T, results = 'hide', echo = F}
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE,
                      fig.width = 8.5,  
                      fig.height = 6,
                      echo = TRUE)
```


```{r load-packages, eval = T, echo = F}

library(ggplot2)
library(survival)
library(tidyr)
library(gsDesign)
library(gsDesign2)
library(dplyr)
library(stringr)
library(tibble)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = min(8, parallel::detectCores()))
library(loo)
library(openxlsx)
library(readxl)
options(digits=7)
library(flexsurv)
library(stringr)
library(SurvRegCensCov)
library(metafor)
library(knitr)
library(flexmix)
library(brms)
library(meta)
library(tidybayes)
library(ggdist)
library(forcats)
library(ggridges)
library(glue)
library(stats4)
library(kableExtra)
library(gt)

```

```{r load-functions, eval = T, echo = F}

source("sim_trials.r")
source("sim_freq.r")
source("sim_bayes.r")
source("surv_analysis_freq.r")
source("surv_analysis_bayes.r")
source("rand.r")
source("gen_surv_data.r")
source("get_boundaries_IA.r")
source("analysis_surv_data_freq.r")
source("analysis_surv_data_bayes.r")
source("plot_bayes.r")
source("plot_bayes_sep.r")

```

# Ejercicio de simulación

Partimos de la hipótesis que el uso de diseños Bayesianos podría haber optimizado el desarrollo de los ensayos clínicos oncológicos en aquellos donde se ha usado una diseño frecuentista. Para ello, se van a evaluar las características operantes de diferentes diseños Bayesianos con respecto al frecuentista para comprobar que podríamos haber necesitado un menor número de pacientes asegurando que no se infle el Error de Tipo I (ET1) a lo largo de diferntes escenarios.

Para ello, se van a usar 3 ensayos reales y vamos a simular los datos cogiendo como parámetros lo que el Sponsor tuvo en cuenta cuando justificó su tamaño muestral en el Statistical Analysis Plan (SAP). Así mismo, como los resultados de esos ensayos ya están disponibles, también se van como parámetros los datos reales obtenidos de estos ensayos usando como análisis los modelos escogidos anteriormente en la primera parte para ver el poder que hubiéramos tenido.

El segundo ensayo tipo, es un ensayo con resultados muy limítrofes, siendo estadísticamente significativo. Este es el Blincyto indicado en pacientes adultos con R/R B-precursor Acute Lymphoblastic Leukemia (ALL). 

# ANNOUNCE Study -> Olaratumab indicado en STS avanzado

## Fase 1b/2

Este estudio tuvo resultados muy prometedores de su estudio aleatorizado Fase 1b/2. Este estudio tenía como variable primaria PFS con OS como key secondary. El estudio contó con 133 pacientes aleatorizados 1:1. El estudio estaba diseñado para que saliera significativo si PFS era positivo con un alfa del 0.1999 (2-sided), muy poco conservador pero adecuado para la fase en la que estaba. El HR obtenido fue de 0.672 (0.442, 1.021), así mismo, la variable secundaria de OS arrojó un HR de 0.46 (0.30, 0.71).

Ya que la variable de OS es la que nos interesa en este caso de estudio, aquí se muestra el Kaplan-Meier de esta variable secundaria:

```{r `Final results on OS of the P2`, eval = T, echo = F}

knitr::include_graphics("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Graficos/Lartruvo_STS_P2_OS.png")

```

Estos resultados tan buenos para una fase tan temprana fueron suficiente para otorgarles la autorización condicional en la FDA el 19/10/206 y en la EMA el 09/11/2016. Para entonces, ya se habían reclutado todos los pacientes para la Fase 3, requisito para recibir la autorización de comercialización completa.


## Fase 3

Este Fase 3 ya tiene como la variable primaria de supervivencia global (OS). Este estudio contó con un total de 509 pacientes aleatorizados 1:1. Sin embargo, los resultados fueron muy malos y, por lo que se concluyó en el Assessment Report de la FDA y de la EMA, no se encontraron indicios de que el estudio estubiera mal diseñado o algún problema estadístico. Es por eso que se retiró este producto del mercado al no haber eficacia. Se diseñó con 2 variables primarias duales, donde el alfa se distribuyó con un 2% para OS en la población ITT y un 0.5% en la población LMS. Ninguno salió.

El HR de OS para la población ITT fue de 1.047 [95%CI, 0.841-1.303] con un alfa del 5% (2-sided). El HR para el subgrupo de OS en LMS tampoco salió mejor, con un (HR=0.951 [95% CI: 0.690, 1.312]; p=.7618).

A continuación se muestra el Kaplan-Meier de OS para la población global: 

```{r `Final results on OS of the P3`, eval = T, echo = F}

knitr::include_graphics("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Graficos/Lartruvo_STS_P3_OS.png")

```

Tras salir estos resultados en Enero del 2019, Eli Lilly pidió revocar esta autorización para retirar el producto del mercado. El 26/04/2019 la EMA le denegó la autorización condicional, mientras que la FDA lo retiró el 25/02/2020.

## Frequentist simulation

Como se ha especificado anteriormente, vamos a considerar los parámetros que ha usado la compañía en el SAP para el diseño de este estudio. Cabe destacar que todo el ejercicio de simulación tanto para la generación de datos simulados como para el análisis bayesiano se va a usar la distribución Weibull.

1) Shape parameter = 1. La Cy usa una exponencial para el diseño por lo que el parámetro es =1. Una Weibull con shape = 1 es una exponencial.
2) Sample size = 460.
3) ratio = 1:1.
4) rand_type = CR. Esto es la manera que se asigna grupo a cada paciente, esto es complete randomization.
5) Tmax = 38. No lo especifican, pero viendo el KM del Fase 3, pongo el máximo de lo que han acumulado (había puesto 32 previamente). 
6) Censor = 0.15/0.15. Texto: "Assuming 30% censoring, a total sample size of 460 randomized patients
is required." 
7) Alpha = 0.02. # La PEP es dual y hacen el split de alfa a 2% en el ITT y 0.5% en LMS.
8) test.type = 1. Esto es que el test es de una cola.
9) Power = 80% 
10) HR esperado: 0.723
11) seed = 24. A lo largo de este estudio en casi todas las simulaciones se usará esta semilla.



La mediana para el experimental coincide si considero un HR de 0.73 y no 0.723

HR: 0.723: [1] 20.74689
HR: 0.73:  [1] 20.54795

Finalmente opto por la versión final del SAP que sería un HR de 0.723 con efectos de 15 para el brazo control y 20.75 para el brazo experimental

Los resultados comprenden 10.000 simulaciones para este escenario.

```{r `Simulation of the trial with the parameters from the protocol`, eval = T, echo = T}

shape_parameter <- 1
sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- matrix(c(15,20.75), nrow = 1, ncol = 2, byrow = TRUE) # HR=0.723

censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 

alpha <- 0.02
test.type <- 1
HR_1 <- FALSE
IA <- NULL
n_exp_events <- 322
Plot_Power = FALSE
plot_pvalues = FALSE
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE
analysis = "freq" 
n_sim <- 10000
seed <- 24

# Efectivamente, en el real hicieron el IA1, no pararon y salió significativo en el IA2.

sim1 <- sim_trials(n_sim = n_sim,
                   analysis = "freq",
                   sample_size = sample_size,
                   ratio = ratio,
                   rand_type = rand_type,
                   Tmax = Tmax,
                   shape_parameter = shape_parameter,
                   scenarios_eff = scenarios_eff,
                   censor = censor,
                   alpha = alpha,
                   test.type = test.type,
                   IA = IA,
                   method_IA = method_IA,
                   n_exp_events = n_exp_events,
                   HR_1 = HR_1,
                   seed=seed,
                   Plot_Power = Plot_Power,
                   plot_pvalues = plot_pvalues)

sim11 <- as.data.frame(sim1)
sim11 <- sim11 %>%
  select(-scenario) %>%
  mutate(sample_size = as.integer(sample_size))

table1 <- gt(sim11) %>%
  tab_header(title = "Frequentist simulation considering the parameters from the SAP") %>%
  fmt_number(columns = names(sim11)[-which(names(sim11) %in% c("sample_size", "count_significant", "count_not_significant"))], decimals = 4) %>%
  tab_style(style = cell_text(weight = "bold", align = "center"),locations = cells_column_labels(columns = everything())) %>%
  tab_options( column_labels.background.color = "lightsteelblue1",
    table.font.size = px(12),
    table.background.color = "white") %>%
  tab_style(style = cell_fill(color = "white"),
    locations = cells_body(rows = everything(), columns = everything())) %>%
  tab_style( style = cell_text(align = "center"),
    locations = cells_body(columns = everything()))

table1

```
Como podemos ver, la media del HR es 0.7280 (0.5749, 0.9218) y el % de estudios que han salido significativos es el 73.73% y donde el 95.13% de las veces el valor verdadero del HR (0.723) está contenido en los intervalos de confianza (Coverage Probability). 

# Evaluación del tamaño muestral con un efecto dado

Lo siguiente es evaluar mediante simulaciones como varían las características operantes en función de diferentes tamaños muestrales (así también se confirmará que 460 pacientes tiene un poder aprox del 72% usando un 2% 1-sided).

Para asegurar qué es más apropiado para evaluar el poder, vamos a hacer las siguientes simulaciones frecuentistas tanto en la situación en la que no hay IAs, como en la situación donde sí hay IAs.

Nota: A partir de ahora cuando debajo de la función principal para simular aparezcan unas líneas para leer un Excel es porque las simulaciones de mucha carga computacional se han hecho con el ordenador de multicore. 

Aquí están las simulaciones de poder sin IAs 

```{r `Simulation of the trial with the parameters from the SAP w different sample size`, eval = T, echo = T}

shape_parameter <- 1
sample_size <- seq(from = 50, to= 1000, by = 5)
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- matrix(c(15,20.75), nrow = 1, ncol = 2, byrow = TRUE) # HR=0.723
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- TRUE
IA <- NULL
n_exp_events <- 322
Plot_Power = TRUE
plot_pvalues = FALSE
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE
analysis = "freq" 
n_sim <- 10000
seed <- 24

# sim1 <- sim_trials(n_sim = n_sim,
#                    analysis = "freq",
#                    sample_size = sample_size,
#                    ratio = ratio,
#                    rand_type = rand_type,
#                    Tmax = Tmax,
#                    shape_parameter = shape_parameter,
#                    scenarios_eff = scenarios_eff,
#                    censor = censor,
#                    alpha = alpha,
#                    test.type = test.type,
#                    IA = IA,
#                    method_IA = method_IA,
#                    n_exp_events = n_exp_events,
#                    HR_1 = HR_1,
#                    seed=seed,
#                    Plot_Power = Plot_Power,
#                    plot_pvalues = plot_pvalues,
#                    Plot_Power_scenarios = Plot_Power_scenarios,
#                    Plot_Control_Scenarios = Plot_Control_Scenarios)
# 
# sim1_power <- sim1[[2]]
# sim1_power_T1E <- sim1[[3]]
# sim1_power_MSE <- sim1[[4]]
# sim1_results <- sim1[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim1)
# saveWorkbook(wd, "Freq_dif_sample_sizes_power_10K.xlsx", overwrite = TRUE)

# Aquí los datos se leen del Excel con los datos simulados en el ordenador de multicore.

sheet_names <- c("Sheet1")
sim2_results <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Freq_dif_sample_sizes_power_10K.xlsx", sheet = sheet_names[1])

# Dibujamos el plot del poder:

data <- subset(sim2_results, scenario == 1)

desired_powers <- c(0.75, 0.8, 0.85, 0.9, 0.95)
sample_sizes_for_desired_powers <- sapply(desired_powers, function(x) 
  min(data$sample_size[data$prop_significant >= x]))

data <- subset(sim2_results, scenario == 1)

desired_powers <- c(0.75, 0.8, 0.85, 0.9, 0.95)
sample_sizes_for_desired_powers <- sapply(desired_powers, function(x) 
  min(data$sample_size[data$prop_significant >= x]))

p1 <- ggplot(data, aes(x = sample_size, y = prop_significant)) +
  geom_point(size = 3, color = "steelblue") +  
  geom_line(linewidth = 1, color = "steelblue") +  
  geom_hline(yintercept = desired_powers, linetype = "dotted", color = c("purple", "orange", "green", "turquoise", "gold"), size = 0.65) + 
  scale_x_continuous(breaks = seq(min(data$sample_size)-5, max(data$sample_size)+5, 25)) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs. Sample Size",
       x = "Sample Size",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

colors_for_annotation <- c("purple", "orange", "green", "turquoise", "gold")  
leftmost_x <- min(data$sample_size) + 5  
for (i in 1:length(desired_powers)) {
  p1 <- p1 + annotate("text", 
                      x = leftmost_x, 
                      y = desired_powers[i], 
                      label = sprintf("%.0f%% power", desired_powers[i] * 100), 
                      hjust = 0,  
                      vjust = -0.3,  
                      size = 3.5, 
                      color = colors_for_annotation[i])
}

p1

```

Como se puede ver, efectivamente con 300 pacientes se tiene un poder por encima del 98% (98.87%). También se identifica que con estos datos simulados, 130 pacientes son suficientes para obtener un poder del 80%, 150 pacientes para tener un poder del 85% y 175 pacientes para un poder del 90%.

La compañía fue bastante conservadora a la hora de diseñar este estudio aunque el tamaño muestral se puede deducir que se diseñó para OS por lo que necesitan más eventos que para PFS.

Ahora miramos los datos para el poder junto al Error de Tipo I:
  
```{r `Simulation of sample sizes: T1E`, eval = T, echo = T}

df <- data.frame(
  sample_size = sim2_results$sample_size,
  HR = sim2_results$scenario,
  power_and_T1E = sim2_results$prop_significant)

# Nueva columna para indicar a partir de cuándo se hace el zoom para el ET1

df$zoom <- ifelse(df$power_and_T1E <= 0.1, "Zoomed", "Regular")

suppressWarnings(
  p2 <- ggplot(data = df) +
    geom_line(aes(x = sample_size, y = power_and_T1E, color = as.factor(HR))) +
    labs(title = "Power and Type I Error by Sample Size",
         x = "Sample Size",
         y = "%",
         color = "scenarios") +
    scale_color_discrete(labels = c("HR = 0.55", "HR = 1")) +
    scale_linetype_discrete(labels = c("Power", "Type I Error")) +
    theme_minimal() +
    facet_grid(zoom ~ ., scales = "free_y") + 
    geom_hline(yintercept = 0.1, linetype = "dashed", color = "black", data = subset(df, zoom == "Zoomed"))
)
p2


```

Así mismo podemos ver que el error de tipo I se mantiene por debajo del valor especificado (5%, 2-sided) por lo que se demuestra que no se infla el ET1 (como era de esperar en un estudio frecuentista si está bien hecho).

Por último, se puede ver valores del MSE como varían en valores más pequeños según se aumenta el tamaño muestral.

```{r `Simulation sample sizes: MSE`, eval = T, echo = T}


p4 <-  ggplot(data, aes(x = sample_size, y = MSE)) +
  geom_point(size = 3, color = "steelblue") +  
  geom_line(linewidth = 1, color = "steelblue") +
  labs(title = "MSE vs. Sample Size",
                      x = "Sample Size",
                      y = "MSE") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))
p4

```

# Evaluación de diferentes de efecto 

Ahora vamos a evaluar cómo afectan los compartamientos del efecto para cada uno de los brazos a un tamaño muestral dado. No sólo se va a hacer con los 400 pacientes planeados, si no también para X, Y y Z.

Ya que el tamaño del efecto depende de dos variables diferentes, efecto del brazo control y efecto del brazo experimental, tenemos que ver los dos escenarios de interés en base a un factor común, el HR.

Para ello, primero vamos a evaluar los diferentes escenarios en términos de medianas. Vamos a dejar fijo primero la mediana para el brazo control como está definido en el SAP (15) y luego dejamos fija la mediana para el brazo experimental (20.75). De esto modo, vemos cómo evoluciona todo el rango de Hazard Ratios relevantes a lo largo de los diferentes escenarios.

```{r `Medians w control arm at 15` , eval = T, echo = T}

shape_parameter <- 1

desired_HRs <- c(1.5, 1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(15, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

medians_control_fijo <- as.data.frame(medians_control_fijo)

medians_control_fijo <- medians_control_fijo %>%
  mutate(HR = desired_HRs) %>%
  select(HR, everything())

# Creamos la tabla
table2 <- gt(medians_control_fijo) %>%
  tab_header(title = "Effect Based on Different HR w Median control = 4.2") %>%
  fmt_number(
    columns = c("Control"),  # Aquí detallamos cuántos decimales queremos para los valores de estas columnas
    decimals = 1
  ) %>%
  fmt_number(
    columns = names(medians_control_fijo)[!names(medians_control_fijo) %in% c("Control")],  # Aplicamos estos decimales para los valores de todas las columnas menos las especificadas aquí
    decimals = 3
  ) %>%
  tab_style(
    style = cell_text(weight = "bold", align = "center"),
    locations = cells_column_labels(columns = everything())
  ) %>%
  tab_options(
    column_labels.background.color = "lightsteelblue1",
    table.font.size = px(12),
    table.background.color = "white"
  ) %>%
  tab_style(
    style = cell_fill(color = "lightsteelblue1"),
    locations = cells_body(rows = seq(2, nrow(medians_control_fijo), by = 2), columns = everything())
  ) %>%
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(1, nrow(medians_control_fijo), by = 2), columns = everything())
  ) %>% # En las tres sentencias anteriores es cómo hacemos que se alternen los colores
  tab_style(
    style = cell_fill(color = "lightsteelblue1"),  # Esto es para aplicar el color azul para valores de la primera columna
    locations = cells_body(columns = "HR")
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),  
    locations = cells_body(columns = "HR")
  ) %>%
  tab_style(
    style = cell_text(align = "center"),
    locations = cells_body(columns = everything())
  ) %>%
  tab_style(
    style = cell_borders(sides = "bottom", color = "black", weight = px(2)),
    locations = cells_column_labels(columns = names(medians_control_fijo)[names(medians_control_fijo) != "HR"])
  ) %>%
  tab_style(
    style = cell_borders(sides = "right", color = "black", weight = px(2)),
    locations = cells_body(columns = "HR")
  )

table2

```

Ahora dejando fija la mediana del brazo experimental a 20.75.

```{r `Medians w experimental arm at 20.75` , eval = T, echo = T}

shape_parameter <- 1

desired_HRs <- c(1.5, 1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
medians_experimiental_fijo <- rep(20.75, times = length(desired_HRs))

treatment_medians <- matrix(NA, nrow = length(desired_HRs), ncol = 2, 
                            dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  treatment_medians[i, "Control"] <- medians_experimiental_fijo[i] * (desired_HRs[i]^(1/shape_parameter))
  treatment_medians[i, "Treatment"] <- medians_experimiental_fijo[i]
}

treatment_medians <- as.data.frame(treatment_medians)

treatment_medians <- treatment_medians %>%
  mutate(HR = desired_HRs) %>%
  select(HR, everything())

# Creamos la tabla
table3 <- gt(treatment_medians) %>%
  tab_header(title = "Effect Based on Different HR w Median Treatment = 6") %>%
  fmt_number(
    columns = c("Treatment"),  # Aquí detallamos cuántos decimales queremos para los valores de estas columnas
    decimals = 0
  ) %>%
  fmt_number(
    columns = names(treatment_medians)[!names(treatment_medians) %in% c("Treatment")],  # Aplicamos estos decimales para los valores de todas las columnas menos las especificadas aquí
    decimals = 3
  ) %>%
  tab_style(
    style = cell_text(weight = "bold", align = "center"),
    locations = cells_column_labels(columns = everything())
  ) %>%
  tab_options(
    column_labels.background.color = "lightsteelblue1",
    table.font.size = px(12),
    table.background.color = "white"
  ) %>%
  tab_style(
    style = cell_fill(color = "lightsteelblue1"),
    locations = cells_body(rows = seq(2, nrow(treatment_medians), by = 2), columns = everything())
  ) %>%
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(1, nrow(treatment_medians), by = 2), columns = everything())
  ) %>% # En las tres sentencias anteriores es cómo hacemos que se alternen los colores
  tab_style(
    style = cell_fill(color = "lightsteelblue1"),  # Esto es para aplicar el color azul para valores de la primera columna
    locations = cells_body(columns = "HR")
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),  
    locations = cells_body(columns = "HR")
  ) %>%
  tab_style(
    style = cell_text(align = "center"),
    locations = cells_body(columns = everything())
  ) %>%
  tab_style(
    style = cell_borders(sides = "bottom", color = "black", weight = px(2)),
    locations = cells_column_labels(columns = names(treatment_medians)[names(treatment_medians) != "HR"])
  ) %>%
  tab_style(
    style = cell_borders(sides = "right", color = "black", weight = px(2)),
    locations = cells_body(columns = "HR")
  )

table3

```

Estos son los valores del tamaño muestral necesario para alcanzar los poderes de interés para las dos simulaciones: 1) Sin tener en cuenta los IAs y 2) Teniendo en cuenta los IAs.


1) Poder: 95% -> 850
2) Poder: 90% -> 695
3) Poder: 85% -> 605
4) Poder: 80% -> 535
5) Poder: 75% -> 475


# Evaluación de diferentes de efecto 

Ahora vamos a evaluar cómo afectan los compartamientos del efecto para cada uno de los brazos a un tamaño muestral dado. No sólo se va a hacer con los 460 pacientes planeados, si no también para X, Y y Z.

Ya que el tamaño del efecto depende de dos variables diferentes, efecto del brazo control y efecto del brazo experimental, tenemos que ver los dos escenarios de interés en base a un factor común, el HR.

Para ello, primero vamos a evaluar los diferentes escenarios en términos de medianas. Vamos a dejar fijo primero la mediana para el brazo control como está definido en el SAP (15) y luego dejamos fija la mediana para el brazo experimental (20.75). De esto modo, vemos cómo evoluciona todo el rango de Hazard Ratios relevantes a lo largo de los diferentes escenarios.



## 1) Tamaño muestral original del SAP: 460 Pacientes

```{r `Simulation scenarios w fixed control`, eval = T, echo = T}

#0) Tamaño muestral del SAP para los HRs

shape_parameter <- 1

desired_HRs <- c(1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(15, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = length(medians_control_fijo), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
IA <- NULL # No IAs de momento
method_IA <- "OF"
n_exp_events <- 322
HR_1 <- FALSE
Plot_Power = FALSE
plot_pvalues = FALSE
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE
analysis = "freq"
n_sim <- 10000
seed <- 24

# sim13 <- sim_trials(n_sim = n_sim,
#                     analysis = "freq",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     shape_parameter = shape_parameter,
#                     scenarios_eff = scenarios_eff,
#                     censor = censor,
#                     alpha = alpha,
#                     test.type = test.type,
#                     IA = IA,
#                     method_IA = method_IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     seed=seed,
#                     Plot_Power = Plot_Power,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim13)
# saveWorkbook(wd, "power_esc_10K_SAP_460.xlsx", overwrite = TRUE)

sheet_names <- c("Sheet1")
sim3_results <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/power_esc_10K_SAP_460.xlsx", sheet = sheet_names[1])

data <- sim3_results %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))

data$desired_HRs <- desired_HRs[1:nrow(data)]

p3 <- ggplot(data, aes(x = desired_HRs, y = prop_significant)) +
  geom_point(size = 3, color = "steelblue") +
  geom_line(linewidth = 1, color = "steelblue") +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(desired_HRs), max(desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(desired_HRs), max(desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

p3 <- p3 +
  annotate("text", x = max(desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green")

for (i in 1:nrow(data)) {
  p3 <- p3 + annotate("text", x = data$desired_HRs[i], y = data$prop_significant[i],
                      label = sprintf("%.2f", data$mean_HR[i]), hjust = 1, vjust = 1.5, size = 4, color = "black")
}

p3

```

Recordemos que los tamaño muestrales requeridos para alcanzar los poder objetivos de acuerdo con mis simulaciones son:

1) Poder: 95% -> 850
2) Poder: 90% -> 695
3) Poder: 85% -> 605
4) Poder: 80% -> 535
5) Poder: 75% -> 475


### 2) Tamaño muestral con poder del 95%: 850 Pacientes


```{r `Simulation scenarios w fixed experimental_620_no_IA`, eval = T, echo = T}

sheet_names <- c("Sheet1")
sim3_results <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/power_esc_10K_850.xlsx", sheet = sheet_names[1])

data <- sim3_results %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))

data$desired_HRs <- desired_HRs[1:nrow(data)]

p4 <- ggplot(data, aes(x = desired_HRs, y = prop_significant)) +
  geom_point(size = 3, color = "steelblue") +
  geom_line(linewidth = 1, color = "steelblue") +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(desired_HRs), max(desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(desired_HRs), max(desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

p4 <- p4 +
  annotate("text", x = max(desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green")

for (i in 1:nrow(data)) {
  p4 <- p4 + annotate("text", x = data$desired_HRs[i], y = data$prop_significant[i],
                      label = sprintf("%.3f", data$mean_HR[i]), hjust = 1, vjust = 1.5, size = 3, color = "black")
}
p4
```


### 3) Tamaño muestral con poder del 90%: 695 Pacientes


```{r `Simulation scenarios w fixed experimental_500_no_IA`, eval = T, echo = T}

sheet_names <- c("Sheet1")
sim3_results <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/power_esc_10K_695.xlsx", sheet = sheet_names[1])

data <- sim3_results %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))

data$desired_HRs <- desired_HRs[1:nrow(data)]

p4 <- ggplot(data, aes(x = desired_HRs, y = prop_significant)) +
  geom_point(size = 3, color = "steelblue") +
  geom_line(linewidth = 1, color = "steelblue") +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(desired_HRs), max(desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(desired_HRs), max(desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

p4 <- p4 +
  annotate("text", x = max(desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green")

for (i in 1:nrow(data)) {
  p4 <- p4 + annotate("text", x = data$desired_HRs[i], y = data$prop_significant[i],
                      label = sprintf("%.3f", data$mean_HR[i]), hjust = 1, vjust = 1.5, size = 3, color = "black")
}
p4
```

### 4) Tamaño muestral con poder del 85%: 605 Pacientes


```{r `Simulation scenarios w fixed experimental_605`, eval = T, echo = T}

sheet_names <- c("Sheet1")
sim3_results <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/power_esc_10K_605.xlsx", sheet = sheet_names[1])

data <- sim3_results %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))

data$desired_HRs <- desired_HRs[1:nrow(data)]

p4 <- ggplot(data, aes(x = desired_HRs, y = prop_significant)) +
  geom_point(size = 3, color = "steelblue") +
  geom_line(linewidth = 1, color = "steelblue") +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(desired_HRs), max(desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(desired_HRs), max(desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

p4 <- p4 +
  annotate("text", x = max(desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green")

for (i in 1:nrow(data)) {
  p4 <- p4 + annotate("text", x = data$desired_HRs[i], y = data$prop_significant[i],
                      label = sprintf("%.3f", data$mean_HR[i]), hjust = 1, vjust = 1.5, size = 3, color = "black")
}
p4
```

### 5) Tamaño muestral con poder del 80%: 535 Pacientes


```{r `Simulation scenarios w fixed experimental_535`, eval = T, echo = T}

sheet_names <- c("Sheet1")
sim3_results <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/power_esc_10K_535.xlsx", sheet = sheet_names[1])

data <- sim3_results %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))

data$desired_HRs <- desired_HRs[1:nrow(data)]

p4 <- ggplot(data, aes(x = desired_HRs, y = prop_significant)) +
  geom_point(size = 3, color = "steelblue") +
  geom_line(linewidth = 1, color = "steelblue") +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(desired_HRs), max(desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(desired_HRs), max(desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

p4 <- p4 +
  annotate("text", x = max(desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green")

for (i in 1:nrow(data)) {
  p4 <- p4 + annotate("text", x = data$desired_HRs[i], y = data$prop_significant[i],
                      label = sprintf("%.3f", data$mean_HR[i]), hjust = 1, vjust = 1.5, size = 3, color = "black")
}
p4
```

### 6) Tamaño muestral con poder del 75%: 475 Pacientes


```{r `Simulation scenarios w fixed experimental_475`, eval = T, echo = T}

sheet_names <- c("Sheet1")
sim3_results <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/power_esc_10K_475.xlsx", sheet = sheet_names[1])

data <- sim3_results %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))

data$desired_HRs <- desired_HRs[1:nrow(data)]

p4 <- ggplot(data, aes(x = desired_HRs, y = prop_significant)) +
  geom_point(size = 3, color = "steelblue") +
  geom_line(linewidth = 1, color = "steelblue") +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(desired_HRs), max(desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(desired_HRs), max(desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

p4 <- p4 +
  annotate("text", x = max(desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green")

for (i in 1:nrow(data)) {
  p4 <- p4 + annotate("text", x = data$desired_HRs[i], y = data$prop_significant[i],
                      label = sprintf("%.3f", data$mean_HR[i]), hjust = 1, vjust = 1.5, size = 3, color = "black")
}
p4
```

Es de interés evaluar diferentes escenarios en medianas tanta de un brazo como de otro en función de HR dado. Esto será útil más adelante cuando evaluemos los Bayesianos ya que en principio, se van a comparar los modelos para diferentes tamaños muestrales y en función de diferentes escenarios de HR.

Esto se va a calcular para evaluar las características operantes de Poder, Error de Tipo I y MSE. Los que cumplan buenas condiciones para los escenarios plausibles se calculará el tamaño muestral y el poder para seleccionar el ahorro que ofrecen con respecto al frecuentista.

```{r `Medians W HR fixed`, eval = T, echo = T}

shape_parameter <- 1 

control_medians <- c(12,12.25,12.5,12.75,13,13.25,13.5,13.75,14,14.25,14.5,14.75,15,15.25,15.5,15.75,16,16.25,16.5,16.75,17,17.25,17.5,17.75,18,18.25,18.5,18.75,19,19.25,19.5,19.75,20,20.25,20.5,20.75,21)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "HR")))

desired_HR = 0.723

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "HR"] = (lambda_control / lambda_treatment)^shape_parameter
}

medians <- as.data.frame(medians)

medians <- medians %>%
  select(HR, everything())

medians_part1 <- medians[1:19, ]
medians_part2 <- medians[20:nrow(medians), ]

# Creamos la tabla
# Create the table for medians_part1
table4a <- gt(medians_part1) %>%
  fmt_number(
    columns = c("HR", "Control"),
    decimals = 3
  ) %>%
  fmt_number(
    columns = names(medians_part1)[!names(medians_part1) %in% c("HR", "Control")],
    decimals = 3
  ) %>%
  tab_style(
    style = cell_text(weight = "bold", align = "center"),
    locations = cells_column_labels(columns = everything())
  ) %>%
  tab_options(
    column_labels.background.color = "lightsteelblue1",
    table.font.size = px(12),
    table.background.color = "white"
  ) %>%
  tab_style(
    style = cell_fill(color = "lightsteelblue1"),
    locations = cells_body(rows = seq(2, nrow(medians_part1), by = 2), columns = everything())
  ) %>%
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(1, nrow(medians_part1), by = 2), columns = everything())
  ) %>%
  tab_style(
    style = cell_fill(color = "lightsteelblue1"),
    locations = cells_body(columns = "HR")
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(columns = "HR")
  ) %>%
  tab_style(
    style = cell_text(align = "center"),
    locations = cells_body(columns = everything())
  ) %>%
  tab_style(
    style = cell_borders(sides = "bottom", color = "black", weight = px(2)),
    locations = cells_column_labels(columns = names(medians_part1)[names(medians_part1) != "HR"])
  ) %>%
  tab_style(
    style = cell_borders(sides = "right", color = "black", weight = px(2)),
    locations = cells_body(columns = "HR")
  )

# Render table
table4a


# Creamos la tabla
table4b <- gt(medians_part2) %>%
  #tab_header(title = "Effect in medians when HR = 0.55") %>%
  fmt_number(
    columns = c("HR", "Control"),  # Aquí detallamos cuántos decimales queremos para los valores de estas columnas
    decimals = 3
  ) %>%
  fmt_number(
    columns = names(medians_part2)[!names(medians_part2) %in% c("HR", "Control")],  # Aplicamos estos decimales para los valores de todas las columnas menos las especificadas aquí
    decimals = 3
  ) %>%
  tab_style(
    style = cell_text(weight = "bold", align = "center"),
    locations = cells_column_labels(columns = everything())
  ) %>%
  tab_options(
    column_labels.background.color = "lightsteelblue1",
    table.font.size = px(12),
    table.background.color = "white"
  ) %>%
  tab_style(
    style = cell_fill(color = "lightsteelblue1"),
    locations = cells_body(rows = seq(2, nrow(medians_part2), by = 2), columns = everything())
  ) %>%
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(1, nrow(medians_part2), by = 2), columns = everything())
  ) %>% # En las tres sentencias anteriores es cómo hacemos que se alternen los colores
  tab_style(
    style = cell_fill(color = "lightsteelblue1"),  # Esto es para aplicar el color azul para valores de la primera columna
    locations = cells_body(columns = "HR")
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),  
    locations = cells_body(columns = "HR")
  ) %>%
  tab_style(
    style = cell_text(align = "center"),
    locations = cells_body(columns = everything())
  ) %>%
  tab_style(
    style = cell_borders(sides = "bottom", color = "black", weight = px(2)),
    locations = cells_column_labels(columns = names(medians_part2)[names(medians_part2) != "HR"])
  ) %>%
  tab_style(
    style = cell_borders(sides = "right", color = "black", weight = px(2)),
    locations = cells_body(columns = "HR")
  )

table4b



```

# Análisis Bayesiano

Se usa para la variable de tipo-hasta un modelo Weibull para la inferencia de los datos.
He cambiado el código para, en vez de calcular el tamaño efecto como variable aleatoria, ahora calculo cada uno de los brazos por separado como variables aleatorias; de este modo podré especificar prior distributions para cada uno de los brazos por separado ya que vamos a asumir siempre que el brazo tratamiento va a tener una prior no informativa.

A continuación muestro el modelo STAN que tenía como efectos conjuntos (variable aleatorio diferencia de tratamiento en función de la pendiente de la regresión):
  
```{r `stan-file slope`, eval = T}

cat(readLines("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/weibull_model_def.stan"), sep = "\n")

```

Y este es el modelo que uso a partir de ahora evaluando el efecto de cada uno de los brazos por separado.

Tenemos tres parámetros a estimar en el modelo.

1. Shape: Este es común para los dos brazos.
2. Scale_control: Es es el parámetro escala para el brazo control en el modelo Weibull.
3. Scale_treatment: Es es el parámetro escala para el brazo tratamiento en el modelo Weibull.

Para las 3 variables se usa una distribución a prior gamma. 

Estas distribuciones a priori son proper ya que integra 1 en todo su dominio, ya que todas las prior son gamma y están parametrizadas para tener sólo valores positivos en los parámetros. Esto tiene sentido ya que en análisis de supervivencia sólo valores positivos son considerados. Una ejemplo común de improper es escoger una dist. uniforme sobre todos los valores reales por una media ya que esta dist. tiene valores infinitos bajo la curva.

Así mismo, estas prior son commensurate. Esto se refiere a que si puede tomar valores lógicos con el análisis de supervivencia. Estos priors lo son porque sólo pueden tomar valores positivos respetando la naturaleza y la escala de los datos. 

```{r `stan-file separate arms`, eval = T, echo = T}

cat(readLines("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/weibull_gamma_def.stan"), sep = "\n")

```

Antes de continuar, merece la pena evaluar la comparabilidad entre un ensayo frecuentista y Bayesiano en términos no sólo de decision (rechazar o no rechazar una hipótesis) si no que la fuerza con que se rechaza tiene que ser similar para el mismo dataset de cada uno de los ensayos simulados. Hay que tener en cuenta que los resultados no son para nada lo mismo (el concepto de p-valores y 1-Posterior son totalmente diferentes) y que para el frecuentista se hace una regresión de Cox y en el Bayesiano una regresión de Weibull.

Cabe destacar que aunque aquí no lo muestro, también he comparado los resultados de una regresión Weibull con mi frecuentista con el Weibull de Bayesiano y arroja resultados similares.

ESTO SE HARÁ DEFINITVAMENTE PARA 
1) VALORES SAP DEL ENSAYO
3) VALORES REALES DEL ENSAYO

DEBERÍA HACERLO COMPARANDO COX Y WEIBULL FRECUENTISTA CON EL BAYESIANO

A continuación muestro los parámetros que he usado para el ensayo frecuentista para obtener los p-valores:
  
## 1) Pvalues

```{r `pvalues freq_NO_IAs`, eval = T, echo = T}

shape_parameter <- 1
sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- matrix(c(15,20.75), nrow = 1, ncol = 2, byrow = TRUE) # HR=0.7
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 
alpha <- 0.02
test.type <- 1
IA <- NULL 
method_IA <- "OF"
n_exp_events <- 322
HR_1 <- FALSE
Plot_Power = FALSE
plot_pvalues = TRUE
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE
analysis = "freq"
n_sim <- 10000
seed <- 24
# 
#  sim6 <- sim_trials(n_sim = n_sim,
#                     analysis = "freq",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     shape_parameter = shape_parameter,
#                     scenarios_eff = scenarios_eff,
#                     censor = censor,
#                     alpha = alpha,
#                     test.type = test.type,
#                     IA = IA,
#                     method_IA = method_IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     seed=seed,
#                     Plot_Power = Plot_Power,
#                     plot_pvalues = plot_pvalues)
# 
# pvalues6 <- sim6[[2]]
# sim66 <- sim6[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", pvalues6)
# saveWorkbook(wd, "freq_pvalues_weibull.xlsx", overwrite = TRUE)

sheet_names <- c("Sheet1")
pvalues1 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/freq_pvalues_cox.xlsx", sheet = sheet_names[1])

# sheet_names <- c("Sheet1")
# pvalues1 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Blincyto_PhALL/Blincyto_PhALL_Tablas/freq_pvalues_CON_IAs.xlsx", sheet = sheet_names[1])

```


Ahora muestro el código que se ha usado para el Bayesiano con una distribución a priori no informativa para los dos brazos:
  
```{r `1-Posterior values_NO_IAs`, eval = T}

# P-VALUES BAYESIANO NO INFORMATIVO SIN IAs

shape_parameter <- 1
sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- matrix(c(15,20.75), nrow = 1, ncol = 2, byrow = TRUE) # HR=0.7
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
IA <- NULL 
method_IA <- "OF"
n_exp_events <- 322
HR_1 <- FALSE
Plot_Power = FALSE
plot_pvalues = TRUE
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE
analysis = "bayes"
n_sim <- 10000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(1, 1/19, 1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

 # sim5 <- sim_trials(n_sim = n_sim,
 #                    analysis = "bayes",
 #                    sample_size = sample_size,
 #                    ratio = ratio,
 #                    rand_type = rand_type,
 #                    Tmax = Tmax,
 #                    scenarios_eff = scenarios_eff,
 #                    shape_parameter = shape_parameter,
 #                    censor = censor,
 #                    test.type = test.type,
 #                    alpha = alpha,
 #                    method_IA = method_IA,
 #                    IA = IA,
 #                    n_exp_events = n_exp_events,
 #                    HR_1 = HR_1,
 #                    Plot_Power = Plot_Power,
 #                    #prior = prior,
 #                    modelo = modelo,
 #                    modelo_bayes_test = modelo_bayes_test,
 #                    prior_type = prior_type,
 #                    P_HR_data_Boundary = P_HR_data_Boundary,
 #                    prior_gamma = prior_gamma,
 #                    Plot_Power_scenarios = Plot_Power_scenarios,
 #                    desired_HRs = desired_HRs,
 #                    plot_pvalues = plot_pvalues,
 #                    Plot_Control_Scenarios = Plot_Control_Scenarios,
 #                    seed = seed)
 # 
 # pvalues5 <- sim5[[2]]
 # sim55 <- sim5[[1]]
 # 
 # wd <- createWorkbook()
 # addWorksheet(wd, "Sheet1")
 # writeData(wd, "Sheet1", pvalues5)
 # saveWorkbook(wd, "bayes_pvalues_Lartruvo.xlsx", overwrite = TRUE)


# Leemos los datos porque la simulación es de muchas horas:

sheet_names <- c("Sheet1")
pvalues4 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/bayes_pvalues_Lartruvo.xlsx", sheet = sheet_names[1])


```

Las características del Bayes que se ha hecho anteriormente:

f <- rstan::sampling(object  = modelo,
                     data    = lista_prior,
                     chains  = 3,       
                     thin    = 3,
                     iter    = 3400,   
                     warmup  = 1900,
                     refresh = 0,
                     seed = 12)
                     
Juntamos los datos de los dos enfoques para los mismos resultados de cada uno de los ensayos clínicos y se enseña un plot donde se muestra que todos los puntos están alineados alrededor de la línea identidad, mostrando una gran consistencia entre el enfoque frecuentista y el Bayesiano con priors no informativas.

# 1.1 Usando pvalores de Cox

```{r `Comparison pvalues vs 1-Posterior 1`, eval = T}

pvalues1$p_value <- pvalues1$p_value
plot_pvalues <- inner_join(pvalues1, pvalues4, by = "n_sim") # Si solo cogemos los de Cox y Bayes

# Cuando el "p-valor" Bayesiano supera el 50% hay que hacer 1-(1-post_prob) para hacer este valor
# comparable con el p-valor frecuentista
plot_pvalues <- plot_pvalues %>%
  mutate(`1-post_prob` = ifelse(`1-post_prob` > 0.5, (1 - `1-post_prob`), `1-post_prob`))

ggplot(plot_pvalues, aes(x = p_value, y = `1-post_prob`)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  xlab("P-value") +
  ylab("1 - Posterior Probability") +
  ggtitle("Frequentist P-value vs Bayesian 1 - Posterior Probability") +
  theme_minimal()

```

Además, en la siguiente figura (donde se ha hecho zoom en la zona entre 0 y 0.05) se muestra que en la mayoría de los casos simulados, el frecuentista y Bayesiano llegan a la misma conclusión con respecto a la significancia de los datos.

```{r `Comparison pvalues vs 1-Posterior 2`, eval = T}

plot_pvalues_table <- plot_pvalues %>%
  mutate(
    p_value_significant = ifelse(p_value < 0.025, 1, 0),
    post_prob_significant = ifelse(`1-post_prob` < 0.025, 1, 0)
  )

agreement_proportion <- mean(plot_pvalues_table$p_value_significant == plot_pvalues_table$post_prob_significant)

disagree_trials <- plot_pvalues_table %>%
  filter(p_value_significant != post_prob_significant)

n_disagree <- nrow(disagree_trials)

frequentist_only <- disagree_trials %>%
  filter(p_value_significant == 1 & post_prob_significant == 0)

bayesian_only <- disagree_trials %>%
  filter(p_value_significant == 0 & post_prob_significant == 1)

freq_only_prop <- nrow(frequentist_only) / n_disagree
bayes_only_prop <- nrow(bayesian_only) / n_disagree

p1 <- ggplot(plot_pvalues, aes(x = p_value, y = `1-post_prob`)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "black") +
  geom_vline(xintercept = 0.025, linetype = "dashed", color = "red") +
  geom_hline(yintercept = 0.025, linetype = "dashed", color = "red") +
  annotate("text", x = 0.0375, y = 0.0125, label = "Only Bayesian test is significant") +
  annotate("text", x = 0.0125, y = 0.0375, label = "Only frequentist test is significant") +
  xlim(c(0, 0.05)) +
  ylim(c(0, 0.05)) +
  xlab("P-value") +
  ylab("1- Posterior Probability") +
  ggtitle("P-value vs 1-Posterior Probability") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

p1
```
Por ultimo, aquí muestro la comparación entre los dos enfoques con el porcentaje total de "fallos", lo que se ve que es muy residual.

```{r `Comparison pvalues vs 1-Posterior 3`, eval = T}

freq_only_prop <- nrow(frequentist_only) / nrow(plot_pvalues_table)
bayes_only_prop <- nrow(bayesian_only) / nrow(plot_pvalues_table)

p2 <- ggplot(plot_pvalues_table, aes(x = p_value, y = `1-post_prob`)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  geom_vline(xintercept = 0.05, linetype = "dashed", color = "red") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "red") +
  annotate("text", x = 0.3, y = 0.02, label = paste("Only Bayesian test is significant (", round(bayes_only_prop*100, 2), "%)", sep = "")) +
  annotate("text", x = 0.15, y = 0.4, label = paste("Only frequentist test is significant (", round(freq_only_prop*100, 2), "%)", sep = "")) +
  xlim(c(0, 0.55)) +
  ylim(c(0, 0.55)) +
  xlab("P-value") +
  ylab("1-Posterior Probability") +
  ggtitle("P-value vs 1-Posterior Probability") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

p2
```


```{r T1E_combined_all1, eval = T}

library(cowplot)

p11 <- p1 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p22 <- p2 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))

plots_row <- plot_grid(p22, p11, ncol = 2, labels = c("A", "B"), label_size = 14)

combined_plot <- plot_grid(
  plots_row,
  #legend,
  ncol = 1,                 
  rel_heights = c(3, 0.4)   
)

print(combined_plot)

```


# 1.2 Usando pvalores de Weibull

```{r `Comparison pvalues vs 1-Posterior 1`, eval = T}

sheet_names <- c("Sheet1")
pvalues1 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/freq_pvalues_weibull.xlsx", sheet = sheet_names[1])

sheet_names <- c("Sheet1")
pvalues4 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/bayes_pvalues_Lartruvo.xlsx", sheet = sheet_names[1])

pvalues1$p_value <- pvalues1$p_value/2
plot_pvalues <- inner_join(pvalues1, pvalues4, by = "n_sim") # Si solo cogemos los de Cox y Bayes

# Cuando el "p-valor" Bayesiano supera el 50% hay que hacer 1-(1-post_prob) para hacer este valor
# comparable con el p-valor frecuentista
plot_pvalues <- plot_pvalues %>%
  mutate(`1-post_prob` = ifelse(`1-post_prob` > 0.5, (1 - `1-post_prob`), `1-post_prob`))

ggplot(plot_pvalues, aes(x = p_value, y = `1-post_prob`)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  xlab("P-value") +
  ylab("1 - Posterior Probability") +
  ggtitle("Frequentist P-value vs Bayesian 1 - Posterior Probability") +
  theme_minimal()

```

Además, en la siguiente figura (donde se ha hecho zoom en la zona entre 0 y 0.05) se muestra que en la mayoría de los casos simulados, el frecuentista y Bayesiano llegan a la misma conclusión con respecto a la significancia de los datos.

```{r `Comparison pvalues vs 1-Posterior 2`, eval = T}

plot_pvalues_table <- plot_pvalues %>%
  mutate(
    p_value_significant = ifelse(p_value < 0.025, 1, 0),
    post_prob_significant = ifelse(`1-post_prob` < 0.025, 1, 0)
  )

agreement_proportion <- mean(plot_pvalues_table$p_value_significant == plot_pvalues_table$post_prob_significant)

disagree_trials <- plot_pvalues_table %>%
  filter(p_value_significant != post_prob_significant)

n_disagree <- nrow(disagree_trials)

frequentist_only <- disagree_trials %>%
  filter(p_value_significant == 1 & post_prob_significant == 0)

bayesian_only <- disagree_trials %>%
  filter(p_value_significant == 0 & post_prob_significant == 1)

freq_only_prop <- nrow(frequentist_only) / n_disagree
bayes_only_prop <- nrow(bayesian_only) / n_disagree

p1 <- ggplot(plot_pvalues, aes(x = p_value, y = `1-post_prob`)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "black") +
  geom_vline(xintercept = 0.025, linetype = "dashed", color = "red") +
  geom_hline(yintercept = 0.025, linetype = "dashed", color = "red") +
  annotate("text", x = 0.0375, y = 0.0125, label = "Only Bayesian test is significant") +
  annotate("text", x = 0.0125, y = 0.0375, label = "Only frequentist test is significant") +
  xlim(c(0, 0.05)) +
  ylim(c(0, 0.05)) +
  xlab("P-value") +
  ylab("1- Posterior Probability") +
  ggtitle("P-value vs 1-Posterior Probability") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

p1
```
Por ultimo, aquí muestro la comparación entre los dos enfoques con el porcentaje total de "fallos", lo que se ve que es muy residual.

```{r `Comparison pvalues vs 1-Posterior 3`, eval = T}

freq_only_prop <- nrow(frequentist_only) / nrow(plot_pvalues_table)
bayes_only_prop <- nrow(bayesian_only) / nrow(plot_pvalues_table)

p2 <- ggplot(plot_pvalues_table, aes(x = p_value, y = `1-post_prob`)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  geom_vline(xintercept = 0.05, linetype = "dashed", color = "red") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "red") +
  annotate("text", x = 0.3, y = 0.02, label = paste("Only Bayesian test is significant (", round(bayes_only_prop*100, 2), "%)", sep = "")) +
  annotate("text", x = 0.15, y = 0.4, label = paste("Only frequentist test is significant (", round(freq_only_prop*100, 2), "%)", sep = "")) +
  xlim(c(0, 0.55)) +
  ylim(c(0, 0.55)) +
  xlab("P-value") +
  ylab("1-Posterior Probability") +
  ggtitle("P-value vs 1-Posterior Probability") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

p2
```
```{r T1E_combined_all2, eval = T}

library(cowplot)

p11 <- p1 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p22 <- p2 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))

plots_row <- plot_grid(p22, p11, ncol = 2, labels = c("A", "B"), label_size = 14)

combined_plot <- plot_grid(
  plots_row,
  #legend,
  ncol = 1,                 
  rel_heights = c(3, 0.4)   
)

print(combined_plot)

```


De manera preliminar (y sin detallar nada a modo de resumen). Se van a considerar 3 modelos Bayesianos diferentes donde se van a considerar 3 prior distribution diferentes para el brazo control considerando siempre una prior no informativa para el brazo tratamiento.
Esta información a priori sigue una distribución beta y aunque los resultados a partir de ahora se muestran en términos de medianas, las prior se van a especificar en términos de la escala weibull para este tratamiento.

La fórmula para transformar las medianas en parámetro escala para la Weibull es la siguiente (asumiendo un shape=1 que es exponencial):
  
parameters <- medians / (log(2)^(1/shape_parameter))

Por ejemplo, si estos análisis consideramos como hasta ahora con un tamaño muestral de 460 que la mediana para el brazo control es 15 y 20.75 para el brazo experimental considerando un HR de 0.723, la transformación en términos de escala Weibull es:

Control: Mediana de 15 es 21.64043
Experimental: Mediana de 20.75 es 29.93592

A continuación muestro los 3 plots de densidad para la prior del brazo control.

1) Prior no informativa: 

Equivalente al frecuentista porque "dejamos que los datos hablen por sí mismos" a través de la función de verosimilitud. Anteriormente, los parámetros para la distribución Gamma había sido una súper no informativa (1/1000, 1/1000), el problema con estos valores tan cercanos al 0 provoca que sea una distribución impropia, además de que tarda muchísimo más (muchos más puntos que evaluar) ya que esta distribución coincide con los ejes de las coordenadas.

Por eso, voy a considerar una distribución no informativa pero acotada al tiempo máximo del estudio de 38 meses. Para ello el 1º parámetro es 1, y el 2º va a ser 1/19 para que esta distribución tenga como media la mitad de la duración total (media=1/(1/19)=19), esto es muy conservador porque el valor que coincide con la mediana del control es 15.

2) Prior débil: 

Damos una idea de en qué rangos debería estar el parámetro para la mediana pero es información muy vaga y que no aporta gran conocimiento. La media de una gamma es 21.64043/1 por lo que está bien (aunque parezca que no está muy centrado)

3) Prior muy informativa: 

Tenemos una certeza absoluta de qué valor es el valor real de la mediana y dejamos poco espacio a la incertidumbre (muy poca variabilidad). Por eso ajustamos los dos parámetros para obtener valores de 2164.043 (2164.043/100) y 100.


```{r `Gamma priors`, eval = T}


parameters_gamma <- data.frame(shape = c(1, 21.64043, 2164.043),
                               rate = c( 1/19, 1, 100),
                               label = c("Non-informative: Gamma(1, 1/19)", "Weak: Gamma(21.64043, 1)", "Informative: Gamma(2164.043, 100)"))


gamma_density <- function(shape, rate, label) {
  data.frame(x = seq(0, 38, length.out = 100)) %>%
    mutate(density = dgamma(x, shape = shape, rate = rate),
           parameter = label)
}

densities <- purrr::pmap_df(parameters_gamma, gamma_density)

ggplot(densities, aes(x, density, color = parameter)) +
  geom_line() +
  labs(title = "Gamma Prior Distributions",
       x = "Median control in months",
       y = "Density") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

```

e ahora en adelante, todas las simulaciones hechas con Bayesiano va a tener las siguientes características (a continuación el link de Stan dode te puede cuál está por defecto)

f <- rstan::sampling(object  = modelo,
                     data    = lista_prior,
                     chains  = 3,       
                     thin    = 1,
                     iter    = 10000,   
                     warmup  = 5000,
                     refresh = 0,
                     seed = 24)

A continuación, pasamos a evaluar las características operantes para los 3 modelos Bayesianos propuestos. Se va a estudiar como anteriormente, el Poder, el Error de Tipo I y el MSE.

Se va a evaluar un escenario donde el HR = 0.723 y se asume una exponencial. Se van a considerar los parámetros del SAP como si no tuviéramos conocimiento de los resultados finales.

A continuación está el Bayesiano con prior no informativa:

```{r `Bayes: Non informative prior distribution`, eval = T}

shape_parameter <- 1 

control_medians <- c(12,12.25,12.5,12.75,13,13.25,13.5,13.75,14,14.25,14.5,14.75,15,15.25,15.5,15.75,16,16.25,16.5,16.75,17,17.25,17.5,17.75,18,18.25,18.5,18.75,19,19.25,19.5,19.75,20,20.25,20.5,20.75,21)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "HR")))

desired_HR = 0.723

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "HR"] = (lambda_control / lambda_treatment)^shape_parameter
}
medians <- medians[,1:2]

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- TRUE
IA <- NULL
n_exp_events <- 322
method_IA <- "bayes"
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE
analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(1, 1/19, 1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non Informative prior ~ Ga(1, 1/19)") 
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = NULL

# sim14 <-  sim_trials(n_sim = n_sim,
#                      analysis = "bayes",
#                      sample_size = sample_size,
#                      ratio = ratio,
#                      rand_type = rand_type,
#                      Tmax = Tmax,
#                      scenarios_eff = scenarios_eff,
#                      shape_parameter = shape_parameter,
#                      censor = censor,
#                      test.type = test.type,
#                      alpha = alpha,
#                      method_IA = method_IA,
#                      IA = IA,
#                      n_exp_events = n_exp_events,
#                      HR_1 = HR_1,
#                      Plot_Power = Plot_Power,
#                      modelo = modelo,
#                      modelo_bayes_test = modelo_bayes_test,
#                      prior_type = prior_type,
#                      P_HR_data_Boundary = P_HR_data_Boundary,
#                      prior_gamma = prior_gamma,
#                      Plot_Power_scenarios = Plot_Power_scenarios,
#                      desired_HRs = desired_HRs,
#                      plot_pvalues = plot_pvalues,
#                      Plot_Control_Scenarios = Plot_Control_Scenarios,
#                      seed = seed)
# sim14[[2]]
# sim14[[3]]
# sim14[[4]]
# 
# sim1414 <- sim14[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim1414)
# saveWorkbook(wd, "Bayes_Non_Informative_460_3000.xlsx", overwrite = TRUE)

sheet_names <- c("Sheet1")
non_informative_gamma <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Bayes_Non_Informative_460_3000.xlsx", sheet = sheet_names[1])


```


El Bayesiano con una prior muy poco informativa (débil):
  
```{r `Bayes: Weak informative prior distribution`, eval = T}

shape_parameter <- 1 

control_medians <- c(12,12.25,12.5,12.75,13,13.25,13.5,13.75,14,14.25,14.5,14.75,15,15.25,15.5,15.75,16,16.25,16.5,16.75,17,17.25,17.5,17.75,18,18.25,18.5,18.75,19,19.25,19.5,19.75,20,20.25,20.5,20.75,21)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "HR")))

desired_HR = 0.723

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "HR"] = (lambda_control / lambda_treatment)^shape_parameter
}
medians <- medians[,1:2]

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- TRUE
IA <- NULL
n_exp_events <- 322
method_IA <- "bayes"
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE
analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(21.64043, 1, 1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Weak Informative prior ~ Ga(21.64043, 1)") 
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = NULL

# sim14 <-  sim_trials(n_sim = n_sim,
#                      analysis = "bayes",
#                      sample_size = sample_size,
#                      ratio = ratio,
#                      rand_type = rand_type,
#                      Tmax = Tmax,
#                      scenarios_eff = scenarios_eff,
#                      shape_parameter = shape_parameter,
#                      censor = censor,
#                      test.type = test.type,
#                      alpha = alpha,
#                      method_IA = method_IA,
#                      IA = IA,
#                      n_exp_events = n_exp_events,
#                      HR_1 = HR_1,
#                      Plot_Power = Plot_Power,
#                      modelo = modelo,
#                      modelo_bayes_test = modelo_bayes_test,
#                      prior_type = prior_type,
#                      P_HR_data_Boundary = P_HR_data_Boundary,
#                      prior_gamma = prior_gamma,
#                      Plot_Power_scenarios = Plot_Power_scenarios,
#                      desired_HRs = desired_HRs,
#                      plot_pvalues = plot_pvalues,
#                      Plot_Control_Scenarios = Plot_Control_Scenarios,
#                      seed = seed)
# sim14[[2]]
# sim14[[3]]
# sim14[[4]]
# 
# sim1414 <- sim14[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim1414)
# saveWorkbook(wd, "Bayes_Weak_Informative_460_3000.xlsx", overwrite = TRUE)

sheet_names <- c("Sheet1")
weak_informative_gamma <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Bayes_Weak_Informative_460_3000.xlsx", sheet = sheet_names[1])


```


Y el Bayesiano con prior muy informativa:
  
```{r `Bayes: Strong informative prior distribution`, eval = T}

shape_parameter <- 1 

control_medians <- c(12,12.25,12.5,12.75,13,13.25,13.5,13.75,14,14.25,14.5,14.75,15,15.25,15.5,15.75,16,16.25,16.5,16.75,17,17.25,17.5,17.75,18,18.25,18.5,18.75,19,19.25,19.5,19.75,20,20.25,20.5,20.75,21)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "HR")))

desired_HR = 0.723

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "HR"] = (lambda_control / lambda_treatment)^shape_parameter
}
medians <- medians[,1:2]

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- TRUE
IA <- NULL
n_exp_events <- 322
method_IA <- "bayes"
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE
analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(2164.043, 100, 1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Strong Informative prior ~ Ga(2164.043, 100)") 
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = NULL

# sim14 <-  sim_trials(n_sim = n_sim,
#                      analysis = "bayes",
#                      sample_size = sample_size,
#                      ratio = ratio,
#                      rand_type = rand_type,
#                      Tmax = Tmax,
#                      scenarios_eff = scenarios_eff,
#                      shape_parameter = shape_parameter,
#                      censor = censor,
#                      test.type = test.type,
#                      alpha = alpha,
#                      method_IA = method_IA,
#                      IA = IA,
#                      n_exp_events = n_exp_events,
#                      HR_1 = HR_1,
#                      Plot_Power = Plot_Power,
#                      modelo = modelo,
#                      modelo_bayes_test = modelo_bayes_test,
#                      prior_type = prior_type,
#                      P_HR_data_Boundary = P_HR_data_Boundary,
#                      prior_gamma = prior_gamma,
#                      Plot_Power_scenarios = Plot_Power_scenarios,
#                      desired_HRs = desired_HRs,
#                      plot_pvalues = plot_pvalues,
#                      Plot_Control_Scenarios = Plot_Control_Scenarios,
#                      seed = seed)
# sim14[[2]]
# sim14[[3]]
# sim14[[4]]
# 
# sim1414 <- sim14[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim1414)
# saveWorkbook(wd, "Bayes_Strong_Informative_460_3000.xlsx", overwrite = TRUE)

sheet_names <- c("Sheet1")
strong_informative_gamma <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Bayes_Strong_Informative_460_3000.xlsx", sheet = sheet_names[1])


```


Por último, leemos los datos del frecuentista para HR=0.723:
  
```{r `Freq`, eval = T}

shape_parameter <- 1 

control_medians <- c(12,12.25,12.5,12.75,13,13.25,13.5,13.75,14,14.25,14.5,14.75,15,15.25,15.5,15.75,16,16.25,16.5,16.75,17,17.25,17.5,17.75,18,18.25,18.5,18.75,19,19.25,19.5,19.75,20,20.25,20.5,20.75,21)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "HR")))

desired_HR = 0.723

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "HR"] = (lambda_control / lambda_treatment)^shape_parameter
}
medians <- medians[,1:2]

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- TRUE
IA <- NULL
n_exp_events <- 322
method_IA <- "OF"
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
#Plot_Control_Scenarios = TRUE
Plot_Control_Scenarios = FALSE
analysis = "freq"
n_sim <- 3000
seed <- 24

# sim14 <-  sim_trials(n_sim = n_sim,
#                      analysis = analysis,
#                      sample_size = sample_size,
#                      ratio = ratio,
#                      rand_type = rand_type,
#                      Tmax = Tmax,
#                      scenarios_eff = scenarios_eff,
#                      shape_parameter = shape_parameter,
#                      censor = censor,
#                      test.type = test.type,
#                      alpha = alpha,
#                      method_IA = method_IA,
#                      IA = IA,
#                      n_exp_events = n_exp_events,
#                      HR_1 = HR_1,
#                      Plot_Power = Plot_Power,
#                      modelo = modelo,
#                      modelo_bayes_test = modelo_bayes_test,
#                      prior_type = prior_type,
#                      P_HR_data_Boundary = P_HR_data_Boundary,
#                      prior_gamma = prior_gamma,
#                      Plot_Power_scenarios = Plot_Power_scenarios,
#                      desired_HRs = desired_HRs,
#                      plot_pvalues = plot_pvalues,
#                      Plot_Control_Scenarios = Plot_Control_Scenarios,
#                      seed = seed)
# # sim14[[2]]
# # sim14[[3]]
# # sim14[[4]]
# # 
# # sim1414 <- sim14[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim14)
# saveWorkbook(wd, "Frequentist_460_3000.xlsx", overwrite = TRUE)

sheet_names <- c("Sheet1")
frequentist <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Frequentist_460_3000.xlsx", sheet = sheet_names[1])


```


Una vez que hemos recogido los resultados de la simulación vamos a comparar en los mismos gráficos el rendimiento de cada uno de los modelos. También se incorpora el frecuentista para tenerlo como referencia.

A continuación recogemos los datos para poder dibujarlos

```{r `Data gathering `, eval = T}

if(HR_1 == TRUE){hr_1 <- replicate(n = ncol(scenarios_eff), scenarios_eff[, 1])
scenarios_eff <- rbind(scenarios_eff, hr_1)}

scenarios_eff_df <- as.data.frame(scenarios_eff)
colnames(scenarios_eff_df) <- c("median_control", "median_treatment")

scenarios_eff_df$scenario <- seq_len(nrow(scenarios_eff_df))

# Frequentist
data_frequentist <- frequentist %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Frequentist")

data_filtered_diff_not_zero_frequentist <- data_frequentist %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_frequentist <- data_frequentist %>%
  filter(median_control == median_treatment)

# Non-informative
data_non_informative <- non_informative_gamma %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Non-informative Prior")

data_filtered_diff_not_zero_non_informative <- data_non_informative %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_non_informative <- data_non_informative %>%
  filter(median_control == median_treatment)

# Very informative
data_informative <- strong_informative_gamma %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Strongly Informative")

data_filtered_diff_not_zero_informative <- data_informative %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_informative <- data_informative %>%
  filter(median_control == median_treatment)

# Mid informative
data_mid_informative <- weak_informative_gamma %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Weakly informative")

data_filtered_diff_not_zero_mid_informative <- data_mid_informative %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_mid_informative <- data_mid_informative %>%
  filter(median_control == median_treatment)

median_control_center <- median(data_filtered_diff_not_zero_informative$median_control)
median_control_center_zero_diff <- median(data_filtered_diff_zero_informative$median_control)

combined_data_diff_not_zero <- bind_rows(data_filtered_diff_not_zero_frequentist,
                                         data_filtered_diff_not_zero_non_informative, 
                                         data_filtered_diff_not_zero_informative, 
                                         data_filtered_diff_not_zero_mid_informative)

combined_data_diff_zero <- bind_rows(data_filtered_diff_zero_frequentist,
                                     data_filtered_diff_zero_non_informative, 
                                     data_filtered_diff_zero_informative, 
                                     data_filtered_diff_zero_mid_informative)

```

Primero observamos cómo de bien se ajustan las inferencias a los datos verdaderos (poblacionales dentro de la simulación). 

Los resultados de MSE son prácticamente iguales para el modelo frecuentista como el bayesiano con las priors no informativas. Con respecto a la Weak, 

Por último la informativa tiene un comportamiento espectacular en escenarios donde la mediana real para el control son muy cercanos a 5.5 pero muy malos según se van alejando del valores central de esta prior.

```{r MSE, eval = T}

p6 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = mean_MSE, color = Prior)) + 
  geom_line(size = 1.2) +
  geom_vline(xintercept = median_control_center, linetype = "solid") +
  labs(
    title = "Mean Square Error (MSE)",
    x = "Median Control Arm (in months)",
    y = "Mean MSE",
    color = "Bayesian Prior Distributions"
  ) +
  theme_minimal() + 
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 10, face = "bold"),
    axis.text = element_text(size = 7),
    axis.title.x = element_text(size = 11),
    axis.title.y = element_text(size = 11),
    legend.margin = margin(9, 9, 9, 9),
    legend.text = element_text(size = 9),
    legend.title = element_text(size = 10, face = "bold")
  ) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.075))

p6

```

Con respecto al poder...

```{r Power, eval = T}

p7 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line(size = 1.2) + 
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "Power", x = "Median Control Arm (in months)", y = "Proportion of Significant Trials", color = "Bayesian Prior Distributions") +
  theme_minimal() + 
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 7),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.margin = margin(12, 12, 12, 12),
        legend.text = element_text(size = 11),
        legend.title = element_text(size = 12, face = "bold")) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_linetype_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0.5, 1))
p7

```

Por último, el error de Tipo I vemos que se controla para el bayesiano no informativo y el weak. Sin embargo, en el bayesiano informativo para valores mayores de 5.5 el error de tipo I se infla de una manera exponencial. Esto demuestra que este modelo no debería usarse a no ser que exista un consenso de que estos valores son "seguros".

```{r T1E, eval = T}

p8 <- ggplot(combined_data_diff_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line(size = 1.2) + 
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  geom_hline(yintercept = 0.02, linetype = "dashed", color = "green") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(title = "Type I Error (T1E)", x = "Median Control (in months)", y = "Proportion of Significant Trials", color = "Bayesian Prior Distributions") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 7),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.margin = margin(12, 12, 12, 12),
        legend.text = element_text(size = 11),
        legend.title = element_text(size = 12, face = "bold")) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_linetype_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_zero$median_control), max(combined_data_diff_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.2))
p8

```


```{r T1E_combined_all, eval = T}

library(cowplot)

p66 <- p6 + scale_color_discrete(
  name = "Bayesian Prior Distributions:", 
  labels = c(
    "Frequentist",
    "Non-informative Prior",
    "Informative Prior\ncentered at 15 months",
    "Weak Informative Prior\ncentered at 15 months"
  )
) +
  guides(color = guide_legend(ncol = 4))  

legend <- get_legend(
  p66 +
    theme(
      # legend.box = "horizontal", 
      legend.justification = c(0, 1),
      # legend.margin = margin(8, 8, 8, 8),
      legend.text = element_text(size = 10), 
      legend.title = element_text(size = 11, face = "bold")
    )
)

# Remove individual legends from the plots
p66 <- p66 + theme(legend.position = "none")
p77 <- p7 + theme(legend.position = "none")
p88 <- p8 + theme(legend.position = "none")

p66 <- p66 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p77 <- p77 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p88 <- p88 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))

plots_row <- plot_grid(p66, p77, p88, ncol = 3, labels = c("A", "B", "C"), label_size = 14)

combined_plot <- plot_grid(
  plots_row,
  legend,
  ncol = 1,                 
  rel_heights = c(3, 0.4)   
)

print(combined_plot)

```


También resulta valioso cómo afectan diferentes asunciones para la prior informativa en el brazo control si vamos moviendo el HR. Esto nos va a permitir ver cómo asunciones demasiado pesimistas para el brazo control favorece que sea más fácil ver diferencias entre brazos y por lo tanto, considerar el estudio como significativo. Por otro lado, es relativamente común ver cuando un estudio sale fallido achacar la culpa a que la mediana del brazo control ha sido más buena de lo esperado y por lo tanto, es más difícil demostrar eficacia.

Ahora vamos a usar el gráfico que ya se presentó anteriormente sólo para el frecuentista con priors muy informativas y comparar cómo afectan estas asunciones informativas a la facilidad o dificultad para declarar eficacia. Esto también se puede ver traducido a la importancia de elegir un threshold para demostrar eficacia ya que esto es totalmente comparable a usar un prior muy muy informativa a un valor determinado.

Recordar que siempre se usa un prior no informativa para el brazo experimental.

Se va a hacer sólo para los parámetros que la compañía usó en el SAP, i.e., distribución exponencial (shape parameter = 1), tamaño muestral de 460 y mediana de control de 15


  
```{r `Bayes: freq HR=0.723 Power changing HRs`, eval = T}

shape_parameter <- 1

desired_HRs <- c(1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(15, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- FALSE
IA <- NULL
method_IA = "OF"
n_exp_events <- 322
Plot_Power = FALSE
plot_pvalues = FALSE
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE
analysis = "freq" 
n_sim <- 3000
seed <- 24

 # sim3 <- sim_trials(n_sim = n_sim,
 #                    analysis = "freq",
 #                    sample_size = sample_size,
 #                    ratio = ratio,
 #                    rand_type = rand_type,
 #                    Tmax = Tmax,
 #                    shape_parameter = shape_parameter,
 #                    scenarios_eff = scenarios_eff,
 #                    censor = censor,
 #                    alpha = alpha,
 #                    test.type = test.type,
 #                    IA = IA,
 #                    method_IA = method_IA,
 #                    n_exp_events = n_exp_events,
 #                    HR_1 = HR_1,
 #                    seed=seed,
 #                    Plot_Power = Plot_Power,
 #                    plot_pvalues = plot_pvalues)
 # 
 # 
 # wd <- createWorkbook()
 # addWorksheet(wd, "Sheet1")
 # writeData(wd, "Sheet1", sim3)
 # saveWorkbook(wd, "freq_3000_HR_changing.xlsx", overwrite = TRUE)

sheet_names <- c("Sheet1")
sim3_results <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/freq_3000_HR_changing.xlsx", sheet = sheet_names[1])

# 

```

Vamos a ver la densidad para cada una de las distribuciones informativas del control forzando valores de 10, 15 y 20 (el real salio 19.75).

```{r `Gamma priors informatives`, eval = T}

# Vamos a presentarlo en términos de medianas en vez de escala Weibull (aunque luego sea esto lo que se meta como input en el model STAN)

# En términos del parámetro escala
# parameters <- medians / (log(2)^(1/shape_parameter))
# En términos de mediana
# medians <- parameters * (log(2))^(1/shape_parameter)

parameters_gamma <- data.frame(shape = c(1442.695, 2164.043, 2885.39),
                               rate = c( 100, 100, 100),
                               label = c("Informative: Gamma(1442.695, 100)", "Informative: Gamma(2164.043, 100)", "Informative: Gamma(2885.39, 100)"))

gamma_density <- function(shape, rate, label) {
  data.frame(x = seq(0, 50, length.out = 100)) %>%
    mutate(density = dgamma(x, shape = shape, rate = rate),
           parameter = label)
}

densities <- purrr::pmap_df(parameters_gamma, gamma_density)

ggplot(densities, aes(x, density, color = parameter)) +
  geom_line() +
  labs(title = "Gamma Prior Distributions",
       x = "Median control in months",
       y = "Density") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

```

Simulamos los resultados forzando a la prior informativa del control a ser de 10

```{r `Bayes: Bayes infor med: 10 HR=0.723 Power changing HRs`, eval = T}

# MEDIAN = 10

shape_parameter <- 1

desired_HRs <- c(1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(15, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- FALSE
IA <- NULL
method_IA = "OF"
n_exp_events <- 322
Plot_Power = FALSE
plot_pvalues = FALSE
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE

analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(1442.695, 100,  1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = NULL

# sim5 <-  sim_trials(n_sim = n_sim,
#                     analysis = "bayes",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim5)
# saveWorkbook(wd, "informative_gamma10_460_3000_HR_changing.xlsx", overwrite = TRUE)
# 


sheet_names <- c("Sheet1")
informative_gamma_10 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/informative_gamma10_460_3000_HR_changing.xlsx", sheet = sheet_names[1])

```

Simulamos los resultados forzando a la prior informativa del control a ser de 15

```{r `Bayes: Bayes infor med: 15 HR=0.723 Power changing HRs`, eval = T}

# MEDIAN CONTROL = 15


shape_parameter <- 1

desired_HRs <- c(1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(15, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- FALSE
IA <- NULL
method_IA = "OF"
n_exp_events <- 322
Plot_Power = FALSE
plot_pvalues = FALSE
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE

analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(2164.043, 100,  1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = NULL

# sim6 <-  sim_trials(n_sim = n_sim,
#                     analysis = "bayes",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim6)
# saveWorkbook(wd, "informative_gamma15_460_3000_HR_changing.xlsx", overwrite = TRUE)
# 


sheet_names <- c("Sheet1")
informative_gamma_15 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/informative_gamma15_460_3000_HR_changing.xlsx", sheet = sheet_names[1])

```

Simulamos los resultados forzando a la prior informativa del control a ser de 20 

```{r `Bayes: Bayes infor med: 20 HR=0.7 Power changing HRs`, eval = T}

# MEDIAN CONTROL = 20


shape_parameter <- 1

desired_HRs <- c(1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(15, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- FALSE
IA <- NULL
method_IA = "OF"
n_exp_events <- 322
Plot_Power = FALSE
plot_pvalues = FALSE
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE

analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(2885.39, 100,  1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = NULL

# sim7 <-  sim_trials(n_sim = n_sim,
#                     analysis = "bayes",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim7)
# saveWorkbook(wd, "informative_gamma20_460_3000_HR_changing.xlsx", overwrite = TRUE)
# 


sheet_names <- c("Sheet1")
informative_gamma_20 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/informative_gamma20_460_3000_HR_changing.xlsx", sheet = sheet_names[1])

```


Aquí juntamos los datos:
  
```{r `Simulation scenarios w fixed experimental & bayes w data`, eval = T, echo = T}


data_sim3_results <- sim3_results %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
  mutate(Prior = "Frequentist")

data_informative_gamma_10 <- informative_gamma_10 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  mutate(Prior = "Informative: Median control at 10")

data_informative_gamma_15 <- informative_gamma_15 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  mutate(Prior = "Informative:  Median control at 15")

data_informative_gamma_20 <- informative_gamma_20 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  mutate(Prior = "Informative:  Median control at 20")  


combined_data <- bind_rows(data_sim3_results,
                           data_informative_gamma_10, 
                           data_informative_gamma_15
                           ,data_informative_gamma_20)


combined_data <- combined_data %>%
  group_by(Prior) %>%
  mutate(desired_HRs = (desired_HRs))

# La diferencia la he considerado en valor absoluto (se podría también al cuadrado para que salgan siempre positivos)
combined_data <- combined_data %>%
  mutate(diff_HR = abs(mean_HR - desired_HRs))

```

Ahora los dibujamos juntos: 
  
```{r `Simulation scenarios w fixed experimental & bayes w plot`, eval = T, echo = T}


p4 <- ggplot(combined_data, aes(x = desired_HRs, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.7, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.7, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(combined_data$desired_HRs), max(combined_data$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data$desired_HRs), max(combined_data$desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 10),
        legend.position = "right")


p4 <- p4 +
  annotate("text", x = max(combined_data$desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(combined_data$desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green") +
  geom_label(aes(x = 1, y = Inf, label = "H0"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.55, y = Inf, label = "H1"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.5, y = Inf, label = "Real HR"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold")

p4

p5 <- ggplot(combined_data, aes(x = desired_HRs, y = diff_HR, color = Prior)) +
  geom_point(size = 3) +
  geom_line(aes(group = Prior), linewidth = 1) +
  scale_x_continuous(breaks = seq(min(combined_data$desired_HRs), max(combined_data$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data$desired_HRs), max(combined_data$desired_HRs), 0.1))) +
  labs(title = "Diff btw Simulated HR and Theoretical HRs",
       x = "Theoretical Hazard Ratio",
       y = "Difference") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 8, face = "bold"),
        axis.text = element_text(size = 8),
        legend.position = "right")

p5
```


También es interesante en vez de dibujar todas las prior muy informativas con diferentes asunciones para la mediana del control (e.g., 10, 15 y 20) es ver cómo se comportan diferentes priors (i.e., no informativa, weak informative y strong informative) para la misma asunción del efecto de mediana 10, 15 y 20.

Como ya tenemos estos valores para las strong informative para cada uno de los escenarios, vamos a poner ahora los parámetros para la no informativa (son los mismos datos para todos los escenarios) y la weak.


Ahora dibujamos las densidades para las weak prior que estarán centradas en los valores de 2.2, 4.2 y 6.2:
  
```{r `Gamma priors weak informatives`, eval = T}

# Vamos a presentarlo en términos de medianas en vez de escala Weibull (aunque luego sea esto lo que se meta como input en el model STAN)

# En términos del parámetro escala
# parameters <- medians / (log(2)^(1/shape_parameter))
# En términos de mediana
# medians <- parameters * (log(2))^(1/shape_parameter)

# parameters_gamma <- data.frame(shape = c(3.50, 5.50, 7.50),
#                          rate = c( 1, 1, 1),
#                          label = c("Informative: Gamma(5.049433, 1)", "Informative: Gamma(7.934823, 1)", "Informative: Gamma(10.82021, 1)"))

parameters_gamma <- data.frame(shape = c(14.42695, 21.64043, 28.8539),
                               rate = c( 1, 1, 1),
                               label = c("Weak: Gamma(14.42695, 1)", "Weak: Gamma(21.64043, 1)", "Weak: Gamma(28.8539, 1)"))

gamma_density <- function(shape, rate, label) {
  data.frame(x = seq(0, 50, length.out = 100)) %>%
    mutate(density = dgamma(x, shape = shape, rate = rate),
           parameter = label)
}

densities <- purrr::pmap_df(parameters_gamma, gamma_density)

ggplot(densities, aes(x, density, color = parameter)) +
  geom_line() +
  labs(title = "Gamma Prior Distributions",
       x = "Median control in months",
       y = "Density") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

```

Parámetros weak para mediana del 10

```{r `Bayes: Bayes weak infor med: 10 HR=0.723 Power changing HRs`, eval = T}

# MEDIAN CONTROL = 10

shape_parameter <- 1

desired_HRs <- c(1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(15, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- FALSE
IA <- NULL
method_IA = "OF"
n_exp_events <- 322
Plot_Power = FALSE
plot_pvalues = FALSE
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE

analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(14.42695, 1,  1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = NULL

# sim8 <-  sim_trials(n_sim = n_sim,
#                     analysis = "bayes",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim8)
# saveWorkbook(wd, "weak_informative_gamma10_460_3000_HR_changing.xlsx", overwrite = TRUE)
# 


sheet_names <- c("Sheet1")
weak_informative_gamma_10 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/weak_informative_gamma10_460_3000_HR_changing.xlsx", sheet = sheet_names[1])


```

Parámetros weak informative de 15

```{r `Bayes: Bayes weak infor med: 15 HR=0.723 Power changing HRs`, eval = T}

# MEDIAN CONTROL ARM 15


shape_parameter <- 1

desired_HRs <- c(1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(15, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- FALSE
IA <- NULL
method_IA = "OF"
n_exp_events <- 322
Plot_Power = FALSE
plot_pvalues = FALSE
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE

analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(21.64043, 1,  1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = NULL

# sim9 <-  sim_trials(n_sim = n_sim,
#                     analysis = "bayes",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim9)
# saveWorkbook(wd, "weak_informative_gamma15_460_3000_HR_changing.xlsx", overwrite = TRUE)
# 


sheet_names <- c("Sheet1")
weak_informative_gamma_15 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/weak_informative_gamma15_460_3000_HR_changing.xlsx", sheet = sheet_names[1])

```


Parámetros weak informative de 20

```{r `Bayes: Bayes weak infor med: 20 HR=0.723 Power changing HRs`, eval = T}

# MEDIAN CONTROL ARM 20


shape_parameter <- 1

desired_HRs <- c(1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(15, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- FALSE
IA <- NULL
method_IA = "OF"
n_exp_events <- 322
Plot_Power = FALSE
plot_pvalues = FALSE
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE
analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(28.8539, 1,  1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = NULL

# sim10 <-  sim_trials(n_sim = n_sim,
#                     analysis = "bayes",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim10)
# saveWorkbook(wd, "weak_informative_gamma20_460_3000_HR_changing.xlsx", overwrite = TRUE)
# 


sheet_names <- c("Sheet1")
weak_informative_gamma_20 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/weak_informative_gamma20_460_3000_HR_changing.xlsx", sheet = sheet_names[1])
```


Ahora juntamos los datos para dibujar cada uno de los escenarios.

```{r `Simulation scenarios w fixed experimental & bayes w data 2`, eval = T, echo = T}


data_sim3_results <- sim3_results %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
  mutate(Prior = "Frequentist")

# data_non_informative <- non_informative_gamma %>%
#   group_by(scenario) %>%
#   summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
#             mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
#   mutate(Prior = "Non informative")

data_weak_informative_gamma_10 <- weak_informative_gamma_10 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
  mutate(Prior = "Weak Informative: Median control at 10")


data_informative_gamma_10 <- informative_gamma_10 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  mutate(Prior = "Informative: Median control at 10")

data_weak_informative_gamma_15 <- weak_informative_gamma_15 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
  mutate(Prior = "Weak Informative: Median control at 15")

data_informative_gamma_15 <- informative_gamma_15 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  mutate(Prior = "Informative:  Median control at 15")

data_weak_informative_gamma_20 <- weak_informative_gamma_20 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
  mutate(Prior = "Weak Informative: Median control at 20")      

data_informative_gamma_20 <- informative_gamma_20 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  mutate(Prior = "Informative:  Median control at 20")  

# 10
combined_data_10 <- bind_rows(data_sim3_results,
                               # data_non_informative, 
                               data_weak_informative_gamma_10, 
                               data_informative_gamma_10)

combined_data_10 <- combined_data_10 %>%
  group_by(Prior) %>%
  mutate(desired_HRs = (desired_HRs))

combined_data_10 <- combined_data_10 %>%
  mutate(diff_HR = abs(mean_HR - desired_HRs))

# 15
combined_data_15 <- bind_rows(data_sim3_results,
                               #data_non_informative, 
                               data_weak_informative_gamma_15, 
                               data_informative_gamma_15)


combined_data_15 <- combined_data_15 %>%
  group_by(Prior) %>%
  mutate(desired_HRs = (desired_HRs))

combined_data_15 <- combined_data_15 %>%
  mutate(diff_HR = abs(mean_HR - desired_HRs))

# 20

combined_data_20 <- bind_rows(data_sim3_results,
                               #data_non_informative, 
                               data_weak_informative_gamma_20, 
                               data_informative_gamma_20)

combined_data_20 <- combined_data_20 %>%
  group_by(Prior) %>%
  mutate(desired_HRs = (desired_HRs))

combined_data_20 <- combined_data_20 %>%
  mutate(diff_HR = abs(mean_HR - desired_HRs))

```

Dibujamos los de la mediana de 10

```{r `Simulation scenarios w fixed experimental & bayes w plot 2.2`, eval = T, echo = T}

p6 <- ggplot(combined_data_10, aes(x = desired_HRs, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.723, linetype = "dashed", color = "red", size = 1) +
  #geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(data$desired_HRs), max(data$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(data$desired_HRs), max(data$desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power Varying Hazard Ratios with Prior Median Control at 10 Months",
       x = "Theoretical Hazard Ratios",
       y = "Power",
    color = "Scale Prior Distributions") +
  theme_minimal() +
  theme(plot.title = element_text(size = 9, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 9, face = "bold"),
    axis.text = element_text(size = 8),
    axis.title.x = element_text(size = 9),
    axis.title.y = element_text(size = 9),
    legend.margin = margin(12, 12, 12, 12),
    legend.text = element_text(size = 11),
    legend.title = element_text(size = 9, face = "bold"),
        legend.position = "right")

p6 <- p6 +
  annotate("text", x = max(data$desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(data$desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green") +
  geom_label(aes(x = 1, y = Inf, label = "H0"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.7, y = Inf, label = "H1"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") 
# +
#   geom_label(aes(x = 0.5, y = Inf, label = "Real HR"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold")

# Graficamos las diferencias

p7 <- ggplot(combined_data_10, aes(x = desired_HRs, y = diff_HR, color = Prior)) +
  geom_point(size = 3) +
  geom_line(aes(group = Prior), linewidth = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_10$desired_HRs), max(combined_data_10$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_10$desired_HRs), max(combined_data_10$desired_HRs), 0.1))) +
  labs(title = "Diff btw Simulated HR and Theoretical HRs w prior median control = 10",
       x = "Theoretical Hazard Ratio",
       y = "Difference") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 8, face = "bold"),
        axis.text = element_text(size = 8),
        legend.position = "right")

p7
```

Dibujamos los de la mediana de 15

```{r `Simulation scenarios w fixed experimental & bayes w plot 15`, eval = T, echo = T}

p8 <- ggplot(combined_data_15, aes(x = desired_HRs, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.7, linetype = "dashed", color = "red", size = 1) +
  #geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(data$desired_HRs), max(data$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(data$desired_HRs), max(data$desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power Varying Hazard Ratios with Prior Median Control at 15 Months",
       x = "Theoretical Hazard Ratios",
       y = "Power",
    color = "Scale Prior Distributions") +
  theme_minimal() +
  theme(plot.title = element_text(size = 9, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 9, face = "bold"),
    axis.text = element_text(size = 8),
    axis.title.x = element_text(size = 9),
    axis.title.y = element_text(size = 9),
    legend.margin = margin(12, 12, 12, 12),
    legend.text = element_text(size = 11),
    legend.title = element_text(size = 9, face = "bold"),
        legend.position = "right")

p8 <- p8 +
  annotate("text", x = max(data$desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(data$desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green") +
  geom_label(aes(x = 1, y = Inf, label = "H0"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.723, y = Inf, label = "H1"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") 
# +
#   geom_label(aes(x = 0.5, y = Inf, label = "Real HR"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold")

# Graficamos las diferencias

p9 <- ggplot(combined_data_15, aes(x = desired_HRs, y = diff_HR, color = Prior)) +
  geom_point(size = 3) +
  geom_line(aes(group = Prior), linewidth = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_15$desired_HRs), max(combined_data_15$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_15$desired_HRs), max(combined_data_15$desired_HRs), 0.1))) +
  scale_y_continuous(limits = c(NA, 0.4)) + # Setting the y-axis max limit here
  labs(title = "Diff btw Simulated HR and Theoretical HRs w prior median control = 15",
       x = "Theoretical Hazard Ratio",
       y = "Difference") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 10),
        legend.position = "right")

p9
```

Dibujamos los de la mediana de 20

```{r `Simulation scenarios w fixed experimental & bayes w plot 20`, eval = T, echo = T}

p10 <- ggplot(combined_data_20, aes(x = desired_HRs, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.723, linetype = "dashed", color = "red", size = 1) +
  #geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(data$desired_HRs), max(data$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(data$desired_HRs), max(data$desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power Varying Hazard Ratios with Prior Median Control at 20 Months",
       x = "Theoretical Hazard Ratios",
       y = "Power",
    color = "Scale Prior Distributions") +
  theme_minimal() +
  theme(plot.title = element_text(size = 9, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 9, face = "bold"),
    axis.text = element_text(size = 8),
    axis.title.x = element_text(size = 9),
    axis.title.y = element_text(size = 9),
    legend.margin = margin(12, 12, 12, 12),
    legend.text = element_text(size = 11),
    legend.title = element_text(size = 9, face = "bold"),
        legend.position = "right")

p10 <- p10 +
  annotate("text", x = max(data$desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(data$desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green") +
  geom_label(aes(x = 1, y = Inf, label = "H0"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.7, y = Inf, label = "H1"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") 
# +
#   geom_label(aes(x = 0.5, y = Inf, label = "Real HR"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold")

p10

# Graficamos las diferencias

p11 <- ggplot(combined_data_20, aes(x = desired_HRs, y = diff_HR, color = Prior)) +
  geom_point(size = 3) +
  geom_line(aes(group = Prior), linewidth = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_20$desired_HRs), max(combined_data_20$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_20$desired_HRs), max(combined_data_20$desired_HRs), 0.1))) +
  labs(title = "Diff btw Simulated HR and Theoretical HRs w prior median control = 20",
       x = "Theoretical Hazard Ratio",
       y = "Difference") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 10),
        legend.position = "right")

p11
```

```{r Changing_HRs_combined_all, eval = T}

library(cowplot)

p66 <- p6 + scale_color_discrete(
  name = "Bayesian Prior Distributions:", 
  labels = c(
    "Frequentist",
    "Informative Prior",
    "Weak Informative Prior"
  )
) +
  guides(color = guide_legend(ncol = 4))  

legend <- get_legend(
  p66 +
    theme(
      # legend.box = "horizontal", 
      legend.justification = c(0, 1),
      # legend.margin = margin(8, 8, 8, 8),
      legend.text = element_text(size = 10), 
      legend.title = element_text(size = 11, face = "bold")
    )
)

# Remove individual legends from the plots
p66 <- p66 + theme(legend.position = "none")
p88 <- p8 + theme(legend.position = "none")
p100 <- p10 + theme(legend.position = "none")

p66 <- p66 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p88 <- p88 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p100 <- p100 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))

plots_row <- plot_grid(p66, p88, p100, ncol = 3, labels = c("A", "B", "C"), label_size = 13)

combined_plot <- plot_grid(
  plots_row,
  legend,
  ncol = 1,                 
  rel_heights = c(3, 0.4)   
)

print(combined_plot)


```

# Incorporación de información externa para el brazo control de otros estudios.

Según la descripción hecha para elegir el brazo control hecho por la compañía, se van a tener en cuenta 2 publicaciones de ensayos:

1) Phase 2 study ANNOUNCE - P1b/2 control arm. La mediana del brazo control para OS fue de OS: 14.7 (9.2, 17.1). 

2) Judson (2014) - Mediana del brazo control es de 12.8 meses (10.5–14.3)

3) Seddon (2017) - Mediana del brazo control es de 19 meses (15, 22.82)

4) D'Adamo (2005) - Mediana del brazo control es de 16 (muy pocos pacientes ya que son 17 y sobrevivieron 13)

Para el MAP vamos a tener en cuenta la 4 publicaciones. Para el Power Prior donde hay que elegir sólo un ensayo para incorporarlo en la función de verosimilitud, se va a elegir el P1b/2 de este ensayo al ser el más cercano a lo que asumió la Cy en el SAP y el más reciente.  

```{r `Phase 1b/2 study ANNOUNCE Trial` , eval = T}

# Aquí hago esto con los dos brazos del Fase 2

experimental_p2 <- read.csv("IPD_Lartruvo_survivlal_EXPERIMENTAL_P2.csv")
control_p2 <- read.csv("IPD_Lartruvo_survival_CONTROL_P2.csv")

experimental_p2$arm <- 0 
control_p2$arm <- 1       

data_hist1 <- rbind(experimental_p2, control_p2)

fit_hist1 <- coxph(Surv(Survival.time, Status) ~ arm, data = data_hist1)
Estimate <- c(1/exp(confint(fit_hist1))[2], 1/summary(fit_hist1)$coefficients[2], 1/exp(confint(fit_hist1))[1])

fit_experimental_p2 <- survfit(Surv(Survival.time, Status) ~ 1, data = experimental_p2)
fit_control_p2 <- survfit(Surv(Survival.time, Status) ~ 1, data = control_p2)

# Regresión de Weibull para sacar los parametros de interes
weibull_experimental_p2 <- survreg(Surv(Survival.time, Status) ~ 1, dist = "weibull", data = experimental_p2)
weibull_control_p2 <- survreg(Surv(Survival.time, Status) ~ 1, dist = "weibull", data = control_p2)

time_seq <- seq(min(data_hist1$Survival.time), max(data_hist1$Survival.time), length.out = 100)

# P de supervivencia usando el modelo Weibull
weibull_experimental_p2_surv <- 1 - pweibull(time_seq, shape = 1/weibull_experimental_p2$scale, scale = exp(weibull_experimental_p2$coefficients))
weibull_control_p2_surv <- 1 - pweibull(time_seq, shape = 1/weibull_control_p2$scale, scale = exp(weibull_control_p2$coefficients))

# Comparamos
plot(fit_experimental_p2, col = "blue", main = "Kaplan-Meier Plot with Weibull Fits", xlab = "Time", ylab = "Survival Probability", lty = 1, conf.int = FALSE)
lines(fit_control_p2, col = "red", lty = 1, conf.int = FALSE)
lines(time_seq, weibull_experimental_p2_surv, col = "green", lty = 2)
lines(time_seq, weibull_control_p2_surv, col = "black", lty = 2)

# Vemos que modelos se ajustan mejor
weibull_mod <- flexsurvreg(Surv(Survival.time, Status) ~ arm, data = data_hist1, dist = "weibull")
exp_mod <- flexsurvreg(Surv(Survival.time, Status) ~ arm, data = data_hist1, dist = "exp")
gompertz_mod <- flexsurvreg(Surv(Survival.time, Status) ~ arm, data = data_hist1, dist = "gompertz")
lognormal_mod <- flexsurvreg(Surv(Survival.time, Status) ~ arm, data = data_hist1, dist = "lnorm")

# Compare models
model_list <- list(Weibull = weibull_mod, Exponential = exp_mod, Gompertz = gompertz_mod, LogNormal = lognormal_mod)
#sapply(model_list, AIC)

```

2) Judson (2014) - Mediana del brazo control es de 12.8 meses (10.5–14.3)


```{r `Judson (2014) - Mediana del brazo control es de 12.8 meses (10.5–14.3)`, eval = T}

data_hist2 <- read.csv("IPD_Lartruvo_survival_Externo_1_Judson.csv")

fit_hist2 <- coxph(Surv(Survival.time, Status) ~ 1, data = data_hist2)

fit_judson <- survfit(Surv(Survival.time, Status) ~ 1, data = data_hist2)

# Regresión de Weibull para sacar los parametros de interes
weibull_judson <- survreg(Surv(Survival.time, Status) ~ 1, dist = "weibull", data = data_hist2)

time_seq <- seq(min(data_hist2$Survival.time), max(data_hist2$Survival.time), length.out = 100)

# P de supervivencia usando el modelo Weibull
weibull_judson_surv <- 1 - pweibull(time_seq, shape = 1/weibull_judson$scale, scale = exp(weibull_judson$coefficients))

# Comparamos
plot(fit_judson, col = "blue", main = "Kaplan-Meier Plot with Weibull Fits", xlab = "Time", ylab = "Survival Probability", lty = 1, conf.int = FALSE)
lines(time_seq, weibull_judson_surv, col = "green", lty = 2)

# Vemos que modelos se ajustan mejor
weibull_mod <- flexsurvreg(Surv(Survival.time, Status) ~ 1, data = data_hist2, dist = "weibull")
exp_mod <- flexsurvreg(Surv(Survival.time, Status) ~ 1, data = data_hist2, dist = "exp")
gompertz_mod <- flexsurvreg(Surv(Survival.time, Status) ~ 1, data = data_hist2, dist = "gompertz")
lognormal_mod <- flexsurvreg(Surv(Survival.time, Status) ~ 1, data = data_hist2, dist = "lnorm")

# Compare models
model_list <- list(Weibull = weibull_mod, Exponential = exp_mod, Gompertz = gompertz_mod, LogNormal = lognormal_mod)
#sapply(model_list, AIC)

```

3) Seddon (2017) - Mediana del brazo control es de 19 meses (15, 22.82)
 

```{r `3) Seddon (2017) - Mediana del brazo control es de 19 meses (15, 22.82)` , eval = T}


data_hist3 <- read.csv("IPD_Lartruvo_survival_Externo_2_Seddon_2017.csv")

fit_hist3 <- coxph(Surv(Survival.time, Status) ~ 1, data = data_hist3)

fit_Seddon <- survfit(Surv(Survival.time, Status) ~ 1, data = data_hist3)

# Regresión de Weibull para sacar los parametros de interes
weibull_Seddon <- survreg(Surv(Survival.time, Status) ~ 1, dist = "weibull", data = data_hist3)

time_seq <- seq(min(data_hist3$Survival.time), max(data_hist3$Survival.time), length.out = 100)

# P de supervivencia usando el modelo Weibull
weibull_Seddon <- 1 - pweibull(time_seq, shape = 1/weibull_Seddon$scale, scale = exp(weibull_Seddon$coefficients))

# Comparamos
plot(fit_Seddon, col = "blue", main = "Kaplan-Meier Plot with Weibull Fits", xlab = "Time", ylab = "Survival Probability", lty = 1, conf.int = FALSE)
lines(time_seq, weibull_Seddon, col = "green", lty = 2)

# Vemos que modelos se ajustan mejor
weibull_mod <- flexsurvreg(Surv(Survival.time, Status) ~ 1, data = data_hist3, dist = "weibull")
exp_mod <- flexsurvreg(Surv(Survival.time, Status) ~ 1, data = data_hist3, dist = "exp")
gompertz_mod <- flexsurvreg(Surv(Survival.time, Status) ~ 1, data = data_hist3, dist = "gompertz")
lognormal_mod <- flexsurvreg(Surv(Survival.time, Status) ~ 1, data = data_hist3, dist = "lnorm")

# Compare models
model_list <- list(Weibull = weibull_mod, Exponential = exp_mod, Gompertz = gompertz_mod, LogNormal = lognormal_mod)
#sapply(model_list, AIC)

```

4) D'Adamo (2015) - Mediana del brazo control es de 16 

```{r `4) D'Adamo (2015) - Mediana del brazo control es de 16 ` , eval = T}


data_hist4 <- read.csv("IPD_Lartruvo_survival_Externo_3_Dadamo.csv")

fit_hist4 <- coxph(Surv(Survival.time, Status) ~ 1, data = data_hist4)

fit_Dadamo <- survfit(Surv(Survival.time, Status) ~ 1, data = data_hist4)

# Regresión de Weibull para sacar los parametros de interes
weibull_Dadamo <- survreg(Surv(Survival.time, Status) ~ 1, dist = "weibull", data = data_hist4)

time_seq <- seq(min(data_hist4$Survival.time), max(data_hist4$Survival.time), length.out = 100)

# P de supervivencia usando el modelo Weibull
weibull_Dadamo <- 1 - pweibull(time_seq, shape = 1/weibull_Dadamo$scale, scale = exp(weibull_Dadamo$coefficients))

# Comparamos
plot(fit_Dadamo, col = "blue", main = "Kaplan-Meier Plot with Weibull Fits", xlab = "Time", ylab = "Survival Probability", lty = 1, conf.int = FALSE)
lines(time_seq, weibull_Dadamo, col = "green", lty = 2)

# Vemos que modelos se ajustan mejor
weibull_mod <- flexsurvreg(Surv(Survival.time, Status) ~ 1, data = data_hist4, dist = "weibull")
exp_mod <- flexsurvreg(Surv(Survival.time, Status) ~ 1, data = data_hist4, dist = "exp")
gompertz_mod <- flexsurvreg(Surv(Survival.time, Status) ~ 1, data = data_hist4, dist = "gompertz")
lognormal_mod <- flexsurvreg(Surv(Survival.time, Status) ~ 1, data = data_hist4, dist = "lnorm")

# Compare models
model_list <- list(Weibull = weibull_mod, Exponential = exp_mod, Gompertz = gompertz_mod, LogNormal = lognormal_mod)
#sapply(model_list, AIC)

```

# Análisis Bayesiano incorporando información externa

El primer método que se va a considerar es el MAP donde incorpora en la Prior información top-level externa para el brazo control.

Primer paper de referencia es "Robust Meta-Analytic-Predictive Priors in Clinical Trials with
Historical Control Information". Aquí discuten el uso de una mezcla entre MAP y no informativo o weak en caso de que haya un conflicto entre los datos del control con el histórico. 

Al tener varios estudios históricos se van a hacer los siguientes modelos:
  
1) MAP -> Varios ensayos históricos
2) Conjugate MAP w weak prior info -> Varios ensayos históricos. (Pesos desde 0.1 hasta 0.5 para la parte informativa)
3) Conjugate MAP w weak prior info -> Varios ensayos históricos. (Peso como variable aleatoria) 

A continuación se va a seguir un procedimiento para la incorporación de información externa a través de la pre-especificación de una prior con parámetros sacados de un meta-análisis (MAP).

Hay muchas maneras de hacer esto pero vamos a centrarnos en la siguiente metodología:
  
1- Obtener la información externa para el brazo control: Me he centrado principalmente en coger los ensayos especificados por el promotor para justificar la asunción para el tamaño muestral del brazo control.

2- Obtener los parámetros para la dist exponencial/weibull de los ensayos históricos: Al ser datos de supervivencia (con binario no existe este problema), es necesario tener los datos paciente a paciente para poder sacar los valores de la distribución paramétrica que consideremos, ya sea exponencial/weibull. Al no ser posible, se digitalizan las curvas para poder hacer esta regresión paramétrica.

3- Una vez tengamos estos valores de la regresión paramétrica, lambda para la exponencial, y lambda y shape para la weibull, en todos los ensayos históricos, vamos a realizar el meta-análisis.

4- El meta-análisis aquí se va a considerar de dos maneras, frecuentista y Bayesiano. Para el enfoque frecuentista se va a utilizar el paquete ampliamente usado "metafor" y para el Bayesiano el "brms". Se van a comparar los dos en este informe y ya vemos si usamos uno, otro o los dos.

5- El MAP se va a hacer para la propuesta de una prior usando sólo 1 ensayo clínico y varios.

6- Una vez tengamos la media y SD del MAP, se transformarán estos valores en términos de la distribución Gamma para el modelo STAN.

# Meta-Analytic Prior


1) MAP -> Varios ensayos históricos

A continuación se muestra el resultado del meta-análisis desde un punto de vista frecuentista utilizando los resultados globales de los estudios anteriormente descritos. 


```{r MAP_Multiple_Historical_Data , eval = T, fig.width=12, fig.height=8}

# Ensayo Externo 1

soc_externo_1 <- survreg(Surv(Survival.time, Status) ~ 1, dist = "exponential", data = control_p2)

n_soc_externo_1 <- dim(control_p2)[1]
scale_soc_externo_1 <- exp(soc_externo_1$coefficients[1])
se_soc_externo_1 <- sqrt(vcov(soc_externo_1)[1])


# Ensayo Externo 2 

soc_externo_2 <- survreg(Surv(Survival.time, Status) ~ 1, dist = "exponential", data = data_hist2)

n_soc_externo_2 <- dim(data_hist2)[1]
scale_soc_externo_2 <- exp(soc_externo_2$coefficients[1])
se_soc_externo_2 <- sqrt(vcov(soc_externo_2)[1])

# Ensayo Externo 3

soc_externo_3 <- survreg(Surv(Survival.time, Status) ~ 1, dist = "exponential", data = data_hist3)

n_soc_externo_3 <- dim(data_hist3)[1]
scale_soc_externo_3 <- exp(soc_externo_3$coefficients[1])
se_soc_externo_3 <- sqrt(vcov(soc_externo_3)[1])


# Ensayo Externo 4

soc_externo_4 <- survreg(Surv(Survival.time, Status) ~ 1, dist = "exponential", data = data_hist4)

n_soc_externo_4 <- dim(data_hist4)[1]
scale_soc_externo_4 <- exp(soc_externo_4$coefficients[1])
se_soc_externo_4 <- sqrt(vcov(soc_externo_4)[1])


# Juntamos los datos

df <- data.frame(
  study = c('P1b/2-ANNOUNCE Trial', 'Judson et al. (2014)', 'Seddon et al. (2017)', 'D´Adamo (2015)'),
  year = c(2016, 2014, 2017, 2015),
  ni = c(n_soc_externo_1, n_soc_externo_2, n_soc_externo_3, n_soc_externo_4),
  yi = c(scale_soc_externo_1, scale_soc_externo_2, scale_soc_externo_3, scale_soc_externo_4),
  vi = c(se_soc_externo_1^2, se_soc_externo_2^2, se_soc_externo_3^2, se_soc_externo_4^2),
  sei = c(se_soc_externo_1, se_soc_externo_2, se_soc_externo_3, se_soc_externo_4)
)



map_meta <- metagen(TE = df$yi, seTE = df$sei, studlab = df$study, data = df, prediction = TRUE, method.tau = "PM",test = "knha")           


```

A continuación se muestra el resultado del meta-análisis desde un punto de vista Bayesiano utilizando los resultados globales de los estudios anteriormente descritos. 

```{r MAP_Multiple_Historical_Data_Bayes , eval = T, echo = FALSE}



#https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/bayesian-ma.html

# Estas priors son muy comunes para los meta-análisis

priors <- c(prior(normal(0,1), class = Intercept),
            prior(cauchy(0,0.5), class = sd))

brm_out <- brm( yi | se(sei) ~ 1 + (1 | study),
                data = df,
                prior = priors,
                iter = 4000)

out_f <- spread_draws(brm_out, b_Intercept) %>% 
  mutate(study = "Average")

out_r <- spread_draws(brm_out, r_study[study,term], b_Intercept) %>% 
  mutate(b_Intercept = r_study + b_Intercept)

avg_effects <- out_r %>% group_by(.iteration) %>% summarise(avg_r_study = mean(r_study, na.rm = TRUE))

out_f <- out_f %>%
  left_join(avg_effects, by = ".iteration") %>%
  mutate(b_Intercept = b_Intercept + avg_r_study)

out_all <- bind_rows(out_r, out_f) %>% 
  ungroup() %>%
  mutate(study = fct_relevel(study, "Average"),
         study = str_replace_all(study, "\\.", " "))

out_all_sum <- group_by(out_all, study) %>% 
  mean_qi(b_Intercept)

out_all %>%   
  ggplot(aes(b_Intercept, study)) +
  geom_vline(xintercept = 0, size = .25, lty = 2) +
  stat_halfeye(.width = c(.8, .95), fill = "dodgerblue") +
  geom_text(
    data = mutate_if(out_all_sum, is.numeric, round, 2),
    aes(label = str_glue("{b_Intercept} [{.lower}, {.upper}]"), x = 4.1),  
    hjust = "inward"
  ) +
  geom_point(
    data = df %>% mutate(study = str_replace_all(study, "\\.", " ")), 
    aes(x=yi), position = position_nudge(y = -.05), shape = 1
  ) +
  coord_cartesian(xlim = c(0, 35)) +
  scale_y_discrete(name = "External Control Studies") +
  theme(
    panel.background = element_rect(fill = "white")
)

```
Ahora vamos a extraer los parámetros para especificar nuestras prior informativas basado en los MAPs.

```{r MAP_Multiple_Historical_Data_Bayes_Freq , eval = T}

# MAP Frecuentista

# Esto cuanta con toda la variabilidad, intra y externa


yi <- map_meta$TE 
vi <- map_meta$seTE^2 
tau2 <- map_meta$tau2  
mu <- map_meta$TE.random  

# Los efectos estimados para cada estudio
#Borenstein, M., Hedges, L. V., Higgins, J. P. T., & Rothstein, H. R. (2009). Introduction to Meta-Analysis. John Wiley & Sons.
theta_hat <- (1/vi + 1/tau2)^(-1) * (yi/vi + mu/tau2)

# Cuantificamos la variabilidad:
frequentist_sd <- sd(theta_hat)

# Esto calcula solo tau^2 que calcula la heterogeneida entre estudios pero no cuenta con la intra
# mean_estimate <- map_meta$TE.random
# CI_lower <- map_meta$lower.random
# CI_upper <- map_meta$upper.random
# 
# SE <- (CI_upper - CI_lower) / (2 * 1.96)
#SE <- SE*3

# Gamma parameters
alpha_map_freq <- (mu/frequentist_sd)^2
beta_map_freq  <- mu/frequentist_sd^2

## MAP Bayesiano ##

post_samples <- brms::posterior_samples(brm_out)

fixed_effects <- post_samples$b_Intercept

#colnames(post_samples)[grep("r_study", colnames(post_samples))]

random_effects <- post_samples[, grep("r_study", colnames(post_samples))]

combined_effects <- sweep(as.matrix(random_effects), 2, fixed_effects, `+`)

true_avg_effect <- apply(combined_effects, 1, mean)

combined_mean <- mean(true_avg_effect)
combined_sd <- sd(true_avg_effect)

# Gamma

alpha_map_bayes <- (combined_mean/combined_sd)^2
beta_map_bayes <- combined_mean / combined_sd^2

parameters_gamma <- data.frame(shape = c(1, 21.64043, 2164.043, alpha_map_freq, alpha_map_bayes),
                               rate = c( 1/19, 1, 100, beta_map_freq, beta_map_bayes),
                               label = c("Non-informative: Gamma(1, 1/19)", "Weak: Gamma(21.64043, 1)", "Informative: Gamma(2164.043, 100)", "MAP freq", "MAP bayes"))

gamma_density <- function(shape, rate, label) {
  data.frame(x = seq(0, 50, length.out = 100)) %>%
    mutate(density = dgamma(x, shape = shape, rate = rate),
           parameter = label)
}

densities <- purrr::pmap_df(parameters_gamma, gamma_density)

ggplot(densities, aes(x, density, color = parameter)) +
  geom_line() +
  labs(title = "Gamma Prior Distributions",
       x = "Median control in months",
       y = "Density") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

```

```{r `Gamma priors Tesis`, eval = T}

# parameters_gamma <- data.frame(shape = c(1, 21.64043, 2164.043),
#                                rate = c( 1/19, 1, 100),
#                                label = c("Non-informative: Gamma(1, 1/19)", "Weak: Gamma(21.64043, 1)", "Informative: Gamma(2164.043, 100)"))
# 
# parameters_gamma <- data.frame(shape = c(14.42695, 21.64043, 28.8539),
#                                rate = c( 1, 1, 1),
#                                label = c("Weak: Gamma(14.42695, 1)", "Weak: Gamma(21.64043, 1)", "Weak: Gamma(28.8539, 1)"))
# 
# parameters_gamma <- data.frame(shape = c(1442.695, 2164.043, 2885.39),
#                                rate = c( 100, 100, 100),
#                                label = c("Informative: Gamma(1442.695, 100)", "Informative: Gamma(2164.043, 100)", "Informative: Gamma(2885.39, 100)"))


parameters_gamma <- data.frame(
  shape = c(1, 21.64043, 2164.043, 1442.695, 2885.39, 14.42695, 28.8539, alpha_map_freq),
  rate = c(1/19, 1, 100, 100, 100, 1, 1, beta_map_freq),
  label = factor(
    c(
      "Non-informative Prior", 
      "Weak Informative Prior centered at 15 months", 
      "Informative Prior centered at 15 months", 
      "Informative Prior centered at 10 months", 
      "Informative Prior centered at 20 months", 
      "Weak Informative Prior centered at 10 months", 
      "Weak Informative Prior centered at 20 months", 
      "Frequentist Meta-Analysis"
    ),
    levels = c(
      "Non-informative Prior", 
      "Weak Informative Prior centered at 15 months", 
      "Informative Prior centered at 15 months", 
      "Informative Prior centered at 10 months", 
      "Informative Prior centered at 20 months", 
      "Weak Informative Prior centered at 10 months", 
      "Weak Informative Prior centered at 20 months", 
      "Frequentist Meta-Analysis"
    )
  )
)

gamma_density <- function(shape, rate, label) {
  data.frame(x = seq(0, 38, length.out = 1000)) %>%
    mutate(density = dgamma(x, shape = shape, rate = rate),
           parameter = label)
}

densities <- purrr::pmap_df(parameters_gamma, gamma_density)

ggplot(densities, aes(x, density, color = parameter, linetype = parameter)) +
  geom_line(size = 1.2) +
  scale_color_manual(
    values = c(
      "Non-informative Prior" = "blue",
      "Weak Informative Prior centered at 15 months" = "orange",
      "Informative Prior centered at 15 months" = "green",
      "Informative Prior centered at 10 months" = "purple",
      "Informative Prior centered at 20 months" = "red",
      "Weak Informative Prior centered at 10 months" = "brown",
      "Weak Informative Prior centered at 20 months" = "deeppink2",
      "Frequentist Meta-Analysis" = "black"
    )
  ) +
  scale_linetype_manual(
    values = c(
      "Non-informative Prior" = "solid",
      "Weak Informative Prior centered at 15 months" = "solid",
      "Informative Prior centered at 15 months" = "solid",
      "Informative Prior centered at 10 months" = "dotted",
      "Informative Prior centered at 20 months" = "dotted",
      "Weak Informative Prior centered at 10 months" = "dotdash",
      "Weak Informative Prior centered at 20 months" = "dotdash",
      "Frequentist Meta-Analysis" = "dashed"
    )
  ) +
  labs(
    title = "Gamma Prior Distributions",
    x = "Scale Parameter for the Control Arm",
    y = "Density",
    color = "Scale Prior Distributions",
    linetype = "Scale Prior Distributions"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 10, face = "bold"),
    axis.text = element_text(size = 12),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12),
    legend.margin = margin(12, 12, 12, 12),
    legend.text = element_text(size = 11),
    legend.title = element_text(size = 12, face = "bold")
  )


```




A continuación, vamos a evaluar las características operantes cogiendo los parámetros obtenido por el MAP tanto frecuentista como Bayesiano.

Características Operantes del MAP Frecuentista:
  
```{r `Bayes: Frequentist MAP`, eval = T}

shape_parameter <- 1 

control_medians <- c(12,12.25,12.5,12.75,13,13.25,13.5,13.75,14,14.25,14.5,14.75,15,15.25,15.5,15.75,16,16.25,16.5,16.75,17,17.25,17.5,17.75,18,18.25,18.5,18.75,19,19.25,19.5,19.75,20,20.25,20.5,20.75,21)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "HR")))

desired_HR = 0.723

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "HR"] = (lambda_control / lambda_treatment)^shape_parameter
}
medians <- medians[,1:2]

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- TRUE
IA <- NULL
n_exp_events <- 322
method_IA <- "bayes"
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE
Dynamic_Borrowing_PP = FALSE

analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_freq, beta_map_freq, 1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("MAP") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = NULL

# sim10 <-  sim_trials(n_sim = n_sim,
#                     analysis = "bayes",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     Dynamic_Borrowing_PP = Dynamic_Borrowing_PP,
#                     w = w,
#                     a0 = a0,
#                     seed = seed)
# sim10[[2]]
# sim10[[3]]
# sim10[[4]]
# 
# sim100 <- sim10[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim100)
# saveWorkbook(wd, "MAP_Freq_460_3000_HR_0723.xlsx", overwrite = TRUE)


sheet_names <- c("Sheet1")
MAP <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/MAP_Freq_460_3000_HR_0723.xlsx", sheet = sheet_names[1])

```



```{r `Data gathering2 `, eval = T}

# MAP
data_MAP <- MAP %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP")

data_filtered_diff_not_zero_MAP <- data_MAP %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_MAP <- data_MAP %>%
  filter(median_control == median_treatment)


combined_data_diff_not_zero <- bind_rows(data_filtered_diff_not_zero_frequentist,
                                         data_filtered_diff_not_zero_non_informative, 
                                         data_filtered_diff_not_zero_informative, 
                                         data_filtered_diff_not_zero_mid_informative,
                                         data_filtered_diff_not_zero_MAP)

combined_data_diff_not_zero$Prior <- factor(
  combined_data_diff_not_zero$Prior,
  levels = c(
    "Frequentist",
     "Strongly Informative",
     "Non-informative Prior",
    "Weakly informative",
    "MAP"
  )
)

combined_data_diff_zero <- bind_rows(data_filtered_diff_zero_frequentist,
                                     data_filtered_diff_zero_non_informative, 
                                     data_filtered_diff_zero_informative, 
                                     data_filtered_diff_zero_mid_informative,
                                     data_filtered_diff_zero_MAP)

combined_data_diff_zero$Prior <- factor(
  combined_data_diff_not_zero$Prior,
  levels = c(
    "Frequentist",
     "Strongly Informative",
     "Non-informative Prior",
    "Weakly informative",
    "MAP"
  )
)

```


En el gráfico de MSE vemos que el MAP frecuentista funciona muy bien en escenarios donde la mediana control está cerca de 4.5 pero el rendimiento va empeorando según se aleja, tiene un rendimiento parecido a la muy informativa (no tan bueno en valores centrales de su densidad porque no es tan informativa). Por otro lado, el MAP bayesiano tiene un mejor rendimiento a lo largo de todos los valores de la mediana, estando por debajo incluso de la weak, frecuentista, etc.

```{r MSE_w_MAP, eval = T}

p6 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = mean_MSE, color = Prior)) + 
  geom_line(size = 1.2) +
  geom_vline(xintercept = median_control_center, linetype = "solid") +
  labs(
    title = "Mean Square Error (MSE)",
    x = "Median Control Arm (in months)",
    y = "Mean MSE",
    color = "Bayesian Prior Distributions"
  ) +
  theme_minimal() + 
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 10, face = "bold"),
    axis.text = element_text(size = 7),
    axis.title.x = element_text(size = 11),
    axis.title.y = element_text(size = 11),
    legend.margin = margin(9, 9, 9, 9),
    legend.text = element_text(size = 9),
    legend.title = element_text(size = 10, face = "bold")
  ) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.075))

p6

```

Con respecto al poder, al ser un tamaño muestral muy alto para lo que se necesita es difícil distinguirlo. Aún así, se ven que los dos métodos tienen bastante poder.

```{r Power_w_MAP, eval = T}

p7 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line(size = 1.2) + 
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "Power", x = "Median Control Arm (in months)", y = "Proportion of Significant Trials", color = "Bayesian Prior Distributions") +
  theme_minimal() + 
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 7),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.margin = margin(12, 12, 12, 12),
        legend.text = element_text(size = 11),
        legend.title = element_text(size = 12, face = "bold")) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_linetype_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0.5, 1))
p7

```

Por último, el error de Tipo I: El MAP frecuentista se dispara según se aleja de 4.5 (media aproximada), podemos ver que tiene un peor comportamiento que la muy informativa. Coger alguna de las dos es como la lotería, si aciertas te llevas el premio gordo pero si no, el ET1 se dispara. Por eso es más importante ser conservador.

Por otro lado, el MAP Bayes infla el ET1 en valores superiores a 6. Aún así, no se dispara mucho y con un poco de ajuste podemos controlar este error mientras que guardamos el buen rendimiento de este modelo.

```{r T1E_w_MAP, eval = T}

p8 <- ggplot(combined_data_diff_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line(size = 1.2) + 
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  geom_hline(yintercept = 0.02, linetype = "dashed", color = "green") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(title = "Type I Error (T1E)", x = "Median Control (in months)", y = "Proportion of Significant Trials", color = "Bayesian Prior Distributions") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 7),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.margin = margin(12, 12, 12, 12),
        legend.text = element_text(size = 11),
        legend.title = element_text(size = 12, face = "bold")) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_linetype_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_zero$median_control), max(combined_data_diff_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.2))
p8

```

```{r T1E_combined_all, eval = T}

library(cowplot)

p66 <- p6 + scale_color_discrete(
  name = "Bayesian Prior Distributions", 
  labels = c(
        "Frequentist",
         "Informative Prior\ncentered at 15 months",
    "Non-informative Prior",
    "Weak Informative Prior\ncentered at 15 months",
        "Frequentist\nMeta-Analysis Prior"

  )
) +
  guides(color = guide_legend(ncol = 6))  

legend <- get_legend(
  p66 +
    theme(
      # legend.box = "horizontal", 
      legend.justification = c(0, 1),
      # legend.margin = margin(8, 8, 8, 8),
      legend.text = element_text(size = 10), 
      legend.title = element_text(size = 11, face = "bold")
    )
)

# Remove individual legends from the plots
p66 <- p66 + theme(legend.position = "none")
p77 <- p7 + theme(legend.position = "none")
p88 <- p8 + theme(legend.position = "none")

p66 <- p66 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p77 <- p77 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p88 <- p88 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))

plots_row <- plot_grid(p66, p77, p88, ncol = 3, labels = c("A", "B", "C"), label_size = 14)

combined_plot <- plot_grid(
  plots_row,
  legend,
  ncol = 1,                 
  rel_heights = c(3, 0.4)   
)

print(combined_plot)

```



Ahora evaluamos los diferentes HRs para el MAP frecuentista

```{r ` MAP Freq Power changing HRs`, eval = T}

shape_parameter <- 1

desired_HRs <- c(1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(15, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- FALSE
IA <- NULL
method_IA = "OF"
n_exp_events <- 322
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE
Dynamic_Borrowing_PP = FALSE

analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_freq, beta_map_freq, 1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("MAP") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = NULL

# sim12 <-  sim_trials(n_sim = n_sim,
#                     analysis = "bayes",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     Dynamic_Borrowing_PP = Dynamic_Borrowing_PP,
#                     w = w,
#                     a0 = a0,
#                     seed = seed)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim12)
# saveWorkbook(wd, "MAP_freq_460_3000_HR_changing.xlsx", overwrite = TRUE)

sheet_names <- c("Sheet1")
MAP_freq_HRs <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/MAP_freq_460_3000_HR_changing.xlsx", sheet = sheet_names[1])

```


```{r `Data combined MAPs`, eval = T}

data_sim3_results <- sim3_results %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
  mutate(Prior = "Frequentist")


data_weak_informative_gamma_10 <- weak_informative_gamma_10 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
  mutate(Prior = "Weak Informative: Median control at 10")


data_informative_gamma_10 <- informative_gamma_10 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  mutate(Prior = "Informative: Median control at 10")

data_weak_informative_gamma_15 <- weak_informative_gamma_15 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
  mutate(Prior = "Weak Informative: Median control at 15")

data_informative_gamma_15 <- informative_gamma_15 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  mutate(Prior = "Informative:  Median control at 15")

data_weak_informative_gamma_20 <- weak_informative_gamma_20 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
  mutate(Prior = "Weak Informative: Median control at 20")      

data_informative_gamma_20 <- informative_gamma_20 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  mutate(Prior = "Informative:  Median control at 20")  

# MAP 

data_map_freq_HRs <- MAP_freq_HRs %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) +                                          sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  mutate(Prior = "MAP") 


# 10
combined_data_10 <- bind_rows(data_sim3_results,
                               data_weak_informative_gamma_10, 
                               data_informative_gamma_10,
                               data_map_freq_HRs)

combined_data_10 <- combined_data_10 %>%
  group_by(Prior) %>%
  mutate(desired_HRs = (desired_HRs))

combined_data_10 <- combined_data_10 %>%
  mutate(diff_HR = abs(mean_HR - desired_HRs))

# 15
combined_data_15 <- bind_rows(data_sim3_results,
                               data_weak_informative_gamma_15, 
                               data_informative_gamma_15,
                               data_map_freq_HRs)


combined_data_15 <- combined_data_15 %>%
  group_by(Prior) %>%
  mutate(desired_HRs = (desired_HRs))

combined_data_15 <- combined_data_15 %>%
  mutate(diff_HR = abs(mean_HR - desired_HRs))

# 20

combined_data_20 <- bind_rows(data_sim3_results,
                               data_weak_informative_gamma_20, 
                               data_informative_gamma_20,
                               data_map_freq_HRs)

combined_data_20 <- combined_data_20 %>%
  group_by(Prior) %>%
  mutate(desired_HRs = (desired_HRs))

combined_data_20 <- combined_data_20 %>%
  mutate(diff_HR = abs(mean_HR - desired_HRs))

```

Dibujamos los de la mediana de 10

```{r `Simulation scenarios w fixed experimental & bayes w plot 10_2`, eval = T, echo = T}


p6 <- ggplot(combined_data_10, aes(x = desired_HRs, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.55, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_10$desired_HRs), max(combined_data_10$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_10$desired_HRs), max(combined_data_10$desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio w prior median control at 10",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 10),
        legend.position = "right")

p6 <- p6 +
  annotate("text", x = max(combined_data_10$desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(combined_data_10$desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green") +
  geom_label(aes(x = 1, y = Inf, label = "H0"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.55, y = Inf, label = "H1"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.5, y = Inf, label = "Real HR"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold")

p6

# Graficamos las diferencias

p7 <- ggplot(combined_data_10, aes(x = desired_HRs, y = diff_HR, color = Prior)) +
  geom_point(size = 3) +
  geom_line(aes(group = Prior), linewidth = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_10$desired_HRs), max(combined_data_10$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_10$desired_HRs), max(combined_data_10$desired_HRs), 0.1))) +
  labs(title = "Diff btw Simulated HR and Theoretical HRs w prior median control = 10",
       x = "Theoretical Hazard Ratio",
       y = "Difference") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 8, face = "bold"),
        axis.text = element_text(size = 8),
        legend.position = "right")

p7
```

Dibujamos los de la mediana de 15

```{r `Simulation scenarios w fixed experimental & bayes w plot 15_2`, eval = T, echo = T}


p8 <- ggplot(combined_data_15, aes(x = desired_HRs, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.55, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_15$desired_HRs), max(combined_data_15$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_15$desired_HRs), max(combined_data_15$desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio w prior median control at 15",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 10),
        legend.position = "right")

p8 <- p8 +
  annotate("text", x = max(combined_data_15$desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(combined_data_15$desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green") +
  geom_label(aes(x = 1, y = Inf, label = "H0"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.55, y = Inf, label = "H1"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.5, y = Inf, label = "Real HR"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold")

p8

# Graficamos las diferencias

p9 <- ggplot(combined_data_15, aes(x = desired_HRs, y = diff_HR, color = Prior)) +
  geom_point(size = 3) +
  geom_line(aes(group = Prior), linewidth = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_15$desired_HRs), max(combined_data_15$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_15$desired_HRs), max(combined_data_15$desired_HRs), 0.1))) +
  scale_y_continuous(limits = c(NA, 0.4)) + # Setting the y-axis max limit here
  labs(title = "Diff btw Simulated HR and Theoretical HRs w prior median control = 15",
       x = "Theoretical Hazard Ratio",
       y = "Difference") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 8, face = "bold"),
        axis.text = element_text(size = 8),
        legend.position = "right")

p9

```

Dibujamos los de la mediana de 20

```{r `Simulation scenarios w fixed experimental & bayes w plot 20_2`, eval = T, echo = T}


p10 <- ggplot(combined_data_20, aes(x = desired_HRs, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.55, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_20$desired_HRs), max(combined_data_20$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_20$desired_HRs), max(combined_data_20$desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio w prior median control at 20",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 10),
        legend.position = "right")

p10 <- p10 +
  annotate("text", x = max(combined_data_20$desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(combined_data_20$desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green") +
  geom_label(aes(x = 1, y = Inf, label = "H0"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.55, y = Inf, label = "H1"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.5, y = Inf, label = "Real HR"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold")

p10

# Graficamos las diferencias

p11 <- ggplot(combined_data_20, aes(x = desired_HRs, y = diff_HR, color = Prior)) +
  geom_point(size = 3) +
  geom_line(aes(group = Prior), linewidth = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_20$desired_HRs), max(combined_data_20$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_20$desired_HRs), max(combined_data_20$desired_HRs), 0.1))) +
  labs(title = "Diff btw Simulated HR and Theoretical HRs w prior median control = 20",
       x = "Theoretical Hazard Ratio",
       y = "Difference") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 8, face = "bold"),
        axis.text = element_text(size = 8),
        legend.position = "right")

p11
```

# Mixture Meta-Analytic Prior with non-informative

Para hacer el mixture es apropiado considerar la 2 diferentes priors, informativa usando MAP y no informativa, de la misma distribución.

Por un lado tenemos:
  
1) Mixture Prior: Combinación de 2 priors

2) Robust Mixture Prior: Combinación de un MAP + Componente no informativo (the MAP prior is not available in analytical form. To allow for a concise description of the prior and tractable posterior analysis we approximate the MAP prior by a mixture of conjugate priors, with the Kullback–Leibler divergence as a measure of discrepancy.)

Ver también cuánto tamaño muestral nos ahorramos por cada calibración del peso puede ser interesante. Claro, pero aquí serían diferentes shape parameter para el componen.

Ahora vamos a ejecutar el código para obtener los resultados para diferentes mixture priors con diferentes w. 
W es el peso que se da a la parte informativa, en este caso obtenido en el MAP.
No tiene sentido proponer un diseño donde se de un peso de más del 50% a la parte informativa, por lo que este valor va a ser el cap.

Vamos a ir de 10 en 10 para el porcentaje de información para la parte informativa, i.e., 0.1, 0.20, 0.3, 0.4, 0.5.

Sólo se va a ensañar un código en el informe pero se ha realizado para todos los pesos y mostramos sus características.

- Se usa el MAP desde el punto de vista Frecuentista
  
- La prior no informativa va a ser una Gamma(1, 1/19)

A continuación se muestran las formas de las densidades:

```{r Densities_mixture_MW2, eval = T}

# Dibujamos las densidades del mixture

gamma_density_1 <- function(x, alpha, beta) {
  dgamma(x, shape=alpha, rate=beta)
}

gamma_density_2 <- function(x) {
  dgamma(x, shape=1, rate=1/19)
  
}

gamma_density_3 <- function(x, alpha, beta) {
  dgamma(x, shape=alpha, rate=beta)
}

mixture_density_1 <- function(x, w1, alpha1, beta1) {
  w1 * gamma_density_1(x, alpha1, beta1) + (1-w1) * gamma_density_2(x)
}


alpha1 = alpha_map_freq
beta1 = beta_map_freq
alpha2 = alpha_map_bayes
beta2 = beta_map_bayes

w1 = 0.2
w2 = 0.50

x_seq <- seq(0, 50, length.out = 400)
gamma1_densities <- gamma_density_1(x_seq, alpha1, beta1)
gamma2_densities <- gamma_density_2(x_seq)
gamma3_densities <- gamma_density_3(x_seq, alpha2, beta2)
mixture1_densities <- sapply(x_seq, function(x) mixture_density_1(x, w1, alpha1, beta1))
mixture2_densities <- sapply(x_seq, function(x) mixture_density_1(x, w2, alpha1, beta1))

df <- data.frame(x = x_seq, 
                 Prior_MAP_Frequentist = gamma1_densities, 
                 Prior_Non_informative = gamma2_densities,
                 Prior_MAP_Bayes = gamma3_densities,
                 Mixture_Freq_weight_0.2_informative_part = mixture1_densities,
                 Mixture_Freq_weight_0.5_informative_part = mixture2_densities)

ggplot(df, aes(x = x)) + 
  geom_line(aes(y = Prior_MAP_Frequentist, color = "Prior_MAP_Frequentist")) + 
  geom_line(aes(y = Prior_Non_informative, color = "Prior_Non_informative")) +
  geom_line(aes(y = Prior_MAP_Bayes, color = "Prior_MAP_Bayes")) +
  geom_line(aes(y = Mixture_Freq_weight_0.2_informative_part, color = "Mixture_Freq_weight_0.2_informative_part")) +
  geom_line(aes(y = Mixture_Freq_weight_0.5_informative_part, color = "Mixture_Freq_weight_0.5_informative_part")) +
  labs(title = "Gamma Densities and Their Mixtures", 
       x = "x", 
       y = "Density", 
       color = "Density Legend") + 
  theme_minimal()

# Dibujamos todas las densidades inclyendo las mixture

parameters_gamma <- data.frame(shape = c(0.00001, 1, 21.64043, 2164.043, alpha_map_freq, alpha_map_bayes),
                               rate = c(0.00001, 1/19, 1, 100, beta_map_freq, beta_map_bayes),
                               label = c("Non-informative: Gamma(0.00001, 0.00001)", "Non-informative: Gamma(1, 1/19)", "Weak: Gamma(21.64043, 1)", "Informative: Gamma(2164.043, 100)", "MAP freq", "MAP bayes"))

gamma_density <- function(shape, rate, label) {
  data.frame(x = seq(0, 50, length.out = 400)) %>%
    mutate(density = dgamma(x, shape = shape, rate = rate),
           parameter = label)
}

densities <- purrr::pmap_df(parameters_gamma, gamma_density)

alpha1 = alpha_map_freq  
beta1 = beta_map_freq  
w1 = 0.2
w2 = 0.50

gamma_density_1 <- function(x, alpha, beta) {
  dgamma(x, shape=alpha, rate=beta)
}

gamma_density_2 <- function(x) {
  dgamma(x, shape=1, rate=1/19)
}

gamma_density_3 <- function(x, alpha, beta) {
  dgamma(x, shape=alpha, rate=beta)
}

Mixture_weight_0.2_informative_part <- function(x, w1, alpha2, beta2) {
  w1 * gamma_density_1(x, alpha1, beta1) + (1-w1) * gamma_density_2(x)
}

Mixture_weight_0.5_informative_part <- function(x, w2, alpha2, beta2) {
  w2 * gamma_density_3(x, alpha1, beta1) + (1-w2) * gamma_density_2(x)
}

x_seq <- seq(0, 50, length.out = 400) # Make sure this covers the range of both plots

mixture_df <- data.frame(
  x = x_seq,
  density = c(
    sapply(x_seq, function(x) Mixture_weight_0.2_informative_part(x, w1, alpha1, beta1)),
    sapply(x_seq, function(x) Mixture_weight_0.5_informative_part(x, w2, alpha1, beta1))
  ),
  parameter = c(rep("Mixture_weight_0.2_informative_part", length(x_seq)), rep("Mixture_weight_0.5_informative_part", length(x_seq)))
)

final_df <- bind_rows(densities, mixture_df)

ggplot(mixture_df, aes(x = x, y = density, color = parameter)) +
  geom_line() +
  labs(title = "Gamma Prior Distributions and Their Mixtures", 
       x = "x", 
       y = "Density", 
       color = "Density Legend") +
  theme_minimal()


```

Las simulaciones con los mismos escenarios para los pesos w de 0.1 a 0.5 de 0.1 en 0.1

```{r MAP_Bayes_Mixture_Prior_w , eval = T}

alpha_map_freq <- 53.88923
beta_map_freq <- 2.247106

shape_parameter <- 1 

control_medians <- c(12,12.25,12.5,12.75,13,13.25,13.5,13.75,14,14.25,14.5,14.75,15,15.25,15.5,15.75,16,16.25,16.5,16.75,17,17.25,17.5,17.75,18,18.25,18.5,18.75,19,19.25,19.5,19.75,20,20.25,20.5,20.75,21)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "HR")))

desired_HR = 0.723

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "HR"] = (lambda_control / lambda_treatment)^shape_parameter
}
medians <- medians[,1:2]

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- TRUE
IA <- NULL
method_IA <- "OF"
n_exp_events <- 322
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE
Dynamic_Borrowing_PP = FALSE

analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_freq, beta_map_freq, 1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Robust Mixture MAP Prior w = 0.1") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_mixture_gamma_sh_uniform.stan" ,verbose = F) 
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = 0.1
a0 = NULL

# sim14 <-  sim_trials(n_sim = n_sim,
#                        analysis = "bayes",
#                        sample_size = sample_size,
#                        ratio = ratio,
#                        rand_type = rand_type,
#                        Tmax = Tmax,
#                        scenarios_eff = scenarios_eff,
#                        shape_parameter = shape_parameter,
#                        censor = censor,
#                        test.type = test.type,
#                        alpha = alpha,
#                        method_IA = method_IA,
#                        IA = IA,
#                        n_exp_events = n_exp_events,
#                        HR_1 = HR_1,
#                        Plot_Power = Plot_Power,
#                        modelo = modelo,
#                        modelo_bayes_test = modelo_bayes_test,
#                        prior_type = prior_type,
#                        P_HR_data_Boundary = P_HR_data_Boundary,
#                        prior_gamma = prior_gamma,
#                        Plot_Power_scenarios = Plot_Power_scenarios,
#                        desired_HRs = desired_HRs,
#                        plot_pvalues = plot_pvalues,
#                        Plot_Control_Scenarios = Plot_Control_Scenarios,
#                        Dynamic_Borrowing_PP = Dynamic_Borrowing_PP,
#                        w = w,
#                        a0 = a0,
#                        seed = seed)
# 
# sim14[[2]]
# sim14[[3]]
# sim14[[4]]
# 
# sim140 <- sim14[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim140)
# saveWorkbook(wd, "Mixture_MAP_w_0.1_460_3000.xlsx", overwrite = TRUE)

## Leemos los datos de las tablas obtenidas con el ordenador de multicore ##

sheet_names <- c("Sheet1")

Mixture_MAP_w_0.1 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Mixture_MAP_w_0.1_460_3000.xlsx", sheet = sheet_names[1])

Mixture_MAP_w_0.2 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Mixture_MAP_w_0.2_460_3000.xlsx", sheet = sheet_names[1])

Mixture_MAP_w_0.3 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Mixture_MAP_w_0.3_460_3000.xlsx", sheet = sheet_names[1])

Mixture_MAP_w_0.4 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Mixture_MAP_w_0.4_460_3000.xlsx", sheet = sheet_names[1])

Mixture_MAP_w_0.5 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Mixture_MAP_w_0.5_460_3000.xlsx", sheet = sheet_names[1])


## Ahora juntamos los datos para graficarlos ##

# MAP Bayes Mixture prior with weight 0.1

data_Mixture_MAP_w_0.1 <- Mixture_MAP_w_0.1 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.1")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.1 <- data_Mixture_MAP_w_0.1 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.1 <- data_Mixture_MAP_w_0.1 %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.2

data_Mixture_MAP_w_0.2 <- Mixture_MAP_w_0.2 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.2")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.2 <- data_Mixture_MAP_w_0.2 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.2 <- data_Mixture_MAP_w_0.2 %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.3

data_Mixture_MAP_w_0.3 <- Mixture_MAP_w_0.3 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.3")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.3 <- data_Mixture_MAP_w_0.3 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.3 <- data_Mixture_MAP_w_0.3 %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.4

data_Mixture_MAP_w_0.4 <- Mixture_MAP_w_0.4 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.4")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.4 <- data_Mixture_MAP_w_0.4 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.4 <- data_Mixture_MAP_w_0.4 %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.5

data_Mixture_MAP_w_0.5 <- Mixture_MAP_w_0.5 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.5")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.5 <- data_Mixture_MAP_w_0.5 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.5 <- data_Mixture_MAP_w_0.5 %>%
  filter(median_control == median_treatment)


combined_data_diff_not_zero <- bind_rows(data_filtered_diff_not_zero_frequentist,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.1,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.2,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.3,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.4,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.5)

combined_data_diff_zero <- bind_rows(data_filtered_diff_zero_frequentist,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.1,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.2,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.3,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.4,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.5)

## Ahora sacamos las gráficas de las características operantes usando como prior el MAP Bayesiano con pesos ##

# Este es el gráfico de las densidades de las prior que se han considerado 

# Este es el gráfico de las densidades de las prior que se han considerado 

# Parte informativa: MAP Bayes
gamma_density_1 <- function(x, alpha, beta) {
  dgamma(x, shape=alpha, rate=beta)
}

# Parte no informativa (robusta)
gamma_density_2 <- function(x) {
  dgamma(x, shape=1, rate=1/19)
}

# La unión de las dos prior con peso prespecificado
mixture_density <- function(x, w1, alpha, beta) {
  w1 * gamma_density_1(x, alpha, beta) + (1-w1) * gamma_density_2(x)
}

# Parámetros obtenidos del MAP frecuentista para la distribución Gamma
alpha = alpha_map_freq
beta = beta_map_freq

w1 = 0.1; w2 = 0.2; w3 = 0.3; w4 = 0.4; w5 = 0.5

x_seq <- seq(0, 50, length.out = 1000)
gamma1_densities <- gamma_density_1(x_seq, alpha, beta)
gamma2_densities <- gamma_density_2(x_seq)
mixture1_densities <- sapply(x_seq, function(x) mixture_density(x, w1, alpha, beta))
mixture2_densities <- sapply(x_seq, function(x) mixture_density(x, w2, alpha, beta))
mixture3_densities <- sapply(x_seq, function(x) mixture_density(x, w3, alpha, beta))
mixture4_densities <- sapply(x_seq, function(x) mixture_density(x, w4, alpha, beta))
mixture5_densities <- sapply(x_seq, function(x) mixture_density(x, w5, alpha, beta))

df <- data.frame(x = x_seq, 
                 MAP_Bayes = gamma1_densities, 
                 Non_Informative = gamma2_densities,
                 Mixture_weight_0.1 = mixture1_densities,
                 Mixture_weight_0.2 = mixture2_densities,
                 Mixture_weight_0.3 = mixture3_densities,
                 Mixture_weight_0.4 = mixture4_densities,
                 Mixture_weight_0.5 = mixture5_densities)

ggplot(df, aes(x = x), size = 1.2) + 
  geom_line(aes(y = MAP_Bayes, color = "Frequentist Meta-Analysis Prior"), size = 1.2) + 
  geom_line(aes(y = Non_Informative, color = "Non-Informative"), size = 1.2) +
  geom_line(aes(y = Mixture_weight_0.1, color = "Robust MAP Prior w=0.1"), size = 1.2) +
  geom_line(aes(y = Mixture_weight_0.2, color = "Robust MAP Prior w=0.2"), size = 1.2) +
  geom_line(aes(y = Mixture_weight_0.3, color = "Robust MAP Prior w=0.3"), size = 1.2) +
  geom_line(aes(y = Mixture_weight_0.4, color = "Robust MAP Prior w=0.4"), size = 1.2) +
  geom_line(aes(y = Mixture_weight_0.5, color = "Robust MAP Prior w=0.5"), size = 1.2) +
  labs(title = "Gamma Pior Distributions and Their Mixtures", 
       x = "Scale Parameter for the Control Arm", 
       y = "Density", 
       color = "Bayesian Prior Distributions") + 
  theme_minimal()


# MSE

p6 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = mean_MSE, color = Prior)) + 
  geom_line(size = 1.2) +
  geom_vline(xintercept = median_control_center, linetype = "solid") +
  labs(
    title = "Mean Square Error (MSE)",
    x = "Median Control Arm (in months)",
    y = "Mean MSE",
    color = "Bayesian Prior Distributions"
  ) +
  theme_minimal() + 
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 10, face = "bold"),
    axis.text = element_text(size = 7),
    axis.title.x = element_text(size = 11),
    axis.title.y = element_text(size = 11),
    legend.margin = margin(9, 9, 9, 9),
    legend.text = element_text(size = 9),
    legend.title = element_text(size = 10, face = "bold")
  ) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.075))

p6

# Power

p7 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line(size = 1.2) + 
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "Power", x = "Median Control Arm (in months)", y = "Proportion of Significant Trials", color = "Bayesian Prior Distributions") +
  theme_minimal() + 
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 7),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.margin = margin(12, 12, 12, 12),
        legend.text = element_text(size = 11),
        legend.title = element_text(size = 12, face = "bold")) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_linetype_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0.5, 1))
p7

# ET1

p8 <- ggplot(combined_data_diff_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line(size = 1.2) + 
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  geom_hline(yintercept = 0.02, linetype = "dashed", color = "green") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(title = "Type I Error (T1E)", x = "Median Control (in months)", y = "Proportion of Significant Trials", color = "Bayesian Prior Distributions") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 7),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.margin = margin(12, 12, 12, 12),
        legend.text = element_text(size = 11),
        legend.title = element_text(size = 12, face = "bold")) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_linetype_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_zero$median_control), max(combined_data_diff_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.2))
p8

```



```{r T1E_combined_RMAP, eval = T}

library(cowplot)

p66 <- p6 + scale_color_discrete(
  name = "Bayesian Prior Distributions", 
  labels = c(
        "Frequentist",
         "Robust MAP\nPrior w=0.1",
         "Robust MAP\nPrior w=0.2",
         "Robust MAP\nPrior w=0.3",
         "Robust MAP\nPrior w=0.4",
         "Robust MAP\nPrior w=0.5"
  )
) +
  guides(color = guide_legend(ncol = 6))  

legend <- get_legend(
  p66 +
    theme(
      # legend.box = "horizontal", 
      legend.justification = c(0, 1),
      # legend.margin = margin(8, 8, 8, 8),
      legend.text = element_text(size = 10), 
      legend.title = element_text(size = 11, face = "bold")
    )
)

# Remove individual legends from the plots
p66 <- p66 + theme(legend.position = "none")
p77 <- p7 + theme(legend.position = "none")
p88 <- p8 + theme(legend.position = "none")

p66 <- p66 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p77 <- p77 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p88 <- p88 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))

plots_row <- plot_grid(p66, p77, p88, ncol = 3, labels = c("A", "B", "C"), label_size = 14)

combined_plot <- plot_grid(
  plots_row,
  legend,
  ncol = 1,                 
  rel_heights = c(3, 0.4)   
)

print(combined_plot)

```


En el Robust MAP parece que ninguno cumple con el requisito de control del ET1 en todos los escenarios, de todos modos quizás aquí podría quedarmo con el del peso 0.1.

Nota: Quizás podría probar con peso de 0.05 pero... las ganancias merecen la pena. Sería sólo para este caso de estudio quizás a ver qué tal.

# Power prior: Incorporando IPD en el análisis

Como en el MAP, he hecho simulaciones para estudiar las características operantes con diferentes pesos para a0: 0.1, 0.2, 0.3, 0.4 y 0.5.
  
```{r Power_Prior , eval = T}

alpha_map_freq <- 53.88923
beta_map_freq <- 2.247106

shape_parameter <- 1 

control_medians <- c(12,12.25,12.5,12.75,13,13.25,13.5,13.75,14,14.25,14.5,14.75,15,15.25,15.5,15.75,16,16.25,16.5,16.75,17,17.25,17.5,17.75,18,18.25,18.5,18.75,19,19.25,19.5,19.75,20,20.25,20.5,20.75,21)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "HR")))

desired_HR = 0.723

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "HR"] = (lambda_control / lambda_treatment)^shape_parameter
}
medians <- medians[,1:2]

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- TRUE
IA <- NULL
method_IA <- "OF"
n_exp_events <- 322
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE
Dynamic_Borrowing_PP = FALSE

analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_freq, beta_map_freq, 1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Power Prior a0 = 0.1") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_power_prior.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = 0.1

# Datos externos
control_p2 <- read.csv("IPD_Lartruvo_survival_CONTROL_P2.csv")
control_p2$arm <- 1
external_data <- control_p2
external_data$dataset <- 'external'
names(external_data)[names(external_data) == "Survival.time"] <- "time"
names(external_data)[names(external_data) == "Status"] <- "status"
external_data$id <- seq(1, dim(external_data)[1], by = 1)
external_data$status <- ifelse(external_data$status==1, TRUE, FALSE)      

# sim15 <-  sim_trials(n_sim = n_sim,
#                        analysis = "bayes",
#                        sample_size = sample_size,
#                        ratio = ratio,
#                        rand_type = rand_type,
#                        Tmax = Tmax,
#                        scenarios_eff = scenarios_eff,
#                        shape_parameter = shape_parameter,
#                        censor = censor,
#                        test.type = test.type,
#                        alpha = alpha,
#                        method_IA = method_IA,
#                        IA = IA,
#                        n_exp_events = n_exp_events,
#                        HR_1 = HR_1,
#                        Plot_Power = Plot_Power,
#                        modelo = modelo,
#                        modelo_bayes_test = modelo_bayes_test,
#                        prior_type = prior_type,
#                        P_HR_data_Boundary = P_HR_data_Boundary,
#                        prior_gamma = prior_gamma,
#                        Plot_Power_scenarios = Plot_Power_scenarios,
#                        desired_HRs = desired_HRs,
#                        plot_pvalues = plot_pvalues,
#                        Plot_Control_Scenarios = Plot_Control_Scenarios,
#                        Dynamic_Borrowing_PP = Dynamic_Borrowing_PP,
#                        external_data = external_data,
#                        w = w,
#                        a0 = a0,
#                        seed = seed)
# 
# sim15[[2]]
# sim15[[3]]
# sim15[[4]]
# 
# sim150 <- sim15[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim150)
# saveWorkbook(wd, "Power_Prior_a0_0.1_460_3000.xlsx", overwrite = TRUE)

## Leemos los datos de las tablas obtenidas con el ordenador de multicore ##


sheet_names <- c("Sheet1")

Power_Prior_a0_0.1 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Power_Prior_a0_0.1_460_3000.xlsx", sheet = sheet_names[1])

Power_Prior_a0_0.2 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Power_Prior_a0_0.2_460_3000.xlsx", sheet = sheet_names[1])

Power_Prior_a0_0.3 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Power_Prior_a0_0.3_460_3000.xlsx", sheet = sheet_names[1])

Power_Prior_a0_0.4 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Power_Prior_a0_0.4_460_3000.xlsx", sheet = sheet_names[1])

Power_Prior_a0_0.5 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Power_Prior_a0_0.5_460_3000.xlsx", sheet = sheet_names[1])



# Bayes_Power_Prior_a0_0.1

data_Bayes_Power_Prior_a0_0.1 <- Power_Prior_a0_0.1 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Power Prior a0 = 0.1")

data_filtered_diff_not_zero_Bayes_Power_Prior_a0_0.1 <- data_Bayes_Power_Prior_a0_0.1 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_Bayes_Power_Prior_a0_0.1 <- data_Bayes_Power_Prior_a0_0.1 %>%
  filter(median_control == median_treatment)


# Bayes_Power_Prior_a0_0.2

data_Bayes_Power_Prior_a0_0.2 <- Power_Prior_a0_0.2 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Power Prior a0 = 0.2")

data_filtered_diff_not_zero_Bayes_Power_Prior_a0_0.2 <- data_Bayes_Power_Prior_a0_0.2 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_Bayes_Power_Prior_a0_0.2 <- data_Bayes_Power_Prior_a0_0.2 %>%
  filter(median_control == median_treatment)


# Bayes_Power_Prior_a0_0.3

data_Bayes_Power_Prior_a0_0.3 <- Power_Prior_a0_0.3 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Power Prior a0 = 0.3")

data_filtered_diff_not_zero_Bayes_Power_Prior_a0_0.3 <- data_Bayes_Power_Prior_a0_0.3 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_Bayes_Power_Prior_a0_0.3<- data_Bayes_Power_Prior_a0_0.3 %>%
  filter(median_control == median_treatment)

# Bayes_Power_Prior_a0_0.4

data_Bayes_Power_Prior_a0_0.4 <- Power_Prior_a0_0.4 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Power Prior a0 = 0.4")

data_filtered_diff_not_zero_Bayes_Power_Prior_a0_0.4 <- data_Bayes_Power_Prior_a0_0.4 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_Bayes_Power_Prior_a0_0.4 <- data_Bayes_Power_Prior_a0_0.4 %>%
  filter(median_control == median_treatment)

# Bayes_Power_Prior_a0_0.5

data_Bayes_Power_Prior_a0_0.5 <- Power_Prior_a0_0.5 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Power Prior a0 = 0.5")

data_filtered_diff_not_zero_Bayes_Power_Prior_a0_0.5 <- data_Bayes_Power_Prior_a0_0.5 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_Bayes_Power_Prior_a0_0.5 <- data_Bayes_Power_Prior_a0_0.5 %>%
  filter(median_control == median_treatment)


combined_data_diff_not_zero <- bind_rows(data_filtered_diff_not_zero_frequentist,
                                         data_filtered_diff_not_zero_Bayes_Power_Prior_a0_0.1,
                                         data_filtered_diff_not_zero_Bayes_Power_Prior_a0_0.2,
                                         data_filtered_diff_not_zero_Bayes_Power_Prior_a0_0.3,
                                         data_filtered_diff_not_zero_Bayes_Power_Prior_a0_0.4,
                                         data_filtered_diff_not_zero_Bayes_Power_Prior_a0_0.5)

combined_data_diff_zero <- bind_rows(data_filtered_diff_zero_frequentist,
                                     data_filtered_diff_zero_Bayes_Power_Prior_a0_0.1,
                                     data_filtered_diff_zero_Bayes_Power_Prior_a0_0.2,
                                     data_filtered_diff_zero_Bayes_Power_Prior_a0_0.3,
                                     data_filtered_diff_zero_Bayes_Power_Prior_a0_0.4,
                                     data_filtered_diff_zero_Bayes_Power_Prior_a0_0.5)

# combined_data_diff_zero <- bind_rows(data_filtered_diff_zero_Bayes_Power_Prior_a0_0.1,
#                                      data_filtered_diff_zero_Bayes_Power_Prior_Seddon_a0_0.1,
#                                      data_filtered_diff_zero_Bayes_Power_Prior_a0_0.2,
#                                      data_filtered_diff_zero_Bayes_Power_Prior_a0_0.3,
#                                      data_filtered_diff_zero_Bayes_Power_Prior_Seddon_a0_0.3,
#                                      data_filtered_diff_zero_Bayes_Power_Prior_a0_0.4,
#                                      data_filtered_diff_zero_Bayes_Power_Prior_Seddon_a0_0.4,
#                                      data_filtered_diff_zero_Bayes_Power_Prior_a0_0.5,
#                                      data_filtered_diff_zero_Bayes_Power_Prior_Seddon_a0_0.5)

## Ahora sacamos las gráficas de las características operantes usando como prior el MAP Bayesiano con pesos ##

# MSE

p6 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = mean_MSE, color = Prior)) + 
  geom_line(size = 1.2) +
  geom_vline(xintercept = median_control_center, linetype = "solid") +
  labs(
    title = "Mean Square Error (MSE)",
    x = "Median Control Arm (in months)",
    y = "Mean MSE",
    color = "Bayesian Prior Distributions"
  ) +
  theme_minimal() + 
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 10, face = "bold"),
    axis.text = element_text(size = 7),
    axis.title.x = element_text(size = 11),
    axis.title.y = element_text(size = 11),
    legend.margin = margin(9, 9, 9, 9),
    legend.text = element_text(size = 9),
    legend.title = element_text(size = 10, face = "bold")
  ) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.075))

p6

# Power

p7 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line(size = 1.2) + 
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "Power", x = "Median Control Arm (in months)", y = "Proportion of Significant Trials", color = "Bayesian Prior Distributions") +
  theme_minimal() + 
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 7),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.margin = margin(12, 12, 12, 12),
        legend.text = element_text(size = 11),
        legend.title = element_text(size = 12, face = "bold")) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_linetype_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0.5, 1))
p7

# ET1

p8 <- ggplot(combined_data_diff_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line(size = 1.2) + 
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  geom_hline(yintercept = 0.02, linetype = "dashed", color = "green") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(title = "Type I Error (T1E)", x = "Median Control (in months)", y = "Proportion of Significant Trials", color = "Bayesian Prior Distributions") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 7),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.margin = margin(12, 12, 12, 12),
        legend.text = element_text(size = 11),
        legend.title = element_text(size = 12, face = "bold")) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_linetype_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_zero$median_control), max(combined_data_diff_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.2))
p8
      
```


```{r T1E_combined_powerprior, eval = T}

library(cowplot)

p66 <- p6 + scale_color_discrete(
  name = "Bayesian Prior Distributions", 
  labels = c(
        "Frequentist",
         "Power Prior\na0=0.1",
         "Power Prior\na0=0.2",
         "Power Prior\na0=0.3",
         "Power Prior\na0=0.4",
         "Power Prior\na0=0.5")
) +
  guides(color = guide_legend(ncol = 6))  

legend <- get_legend(
  p66 +
    theme(
      # legend.box = "horizontal", 
      legend.justification = c(0, 1),
      # legend.margin = margin(8, 8, 8, 8),
      legend.text = element_text(size = 10), 
      legend.title = element_text(size = 11, face = "bold")
    )
)

# Remove individual legends from the plots
p66 <- p66 + theme(legend.position = "none")
p77 <- p7 + theme(legend.position = "none")
p88 <- p8 + theme(legend.position = "none")

p66 <- p66 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p77 <- p77 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p88 <- p88 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))

plots_row <- plot_grid(p66, p77, p88, ncol = 3, labels = c("A", "B", "C"), label_size = 14)

combined_plot <- plot_grid(
  plots_row,
  legend,
  ncol = 1,                 
  rel_heights = c(3, 0.4)   
)

print(combined_plot)

```



De aquí ninguno cumple los criterios a priori.

# Power prior: Incorporando IPD en el análisis usando SEDDON (MÁS PARECIDO AL REAL)

Como en el MAP, he hecho simulaciones para estudiar las características operantes con diferentes pesos para a0: 0.1, 0.2, 0.3, 0.4 y 0.5.
  
```{r Power_Prior , eval = T}

alpha_map_freq <- 53.88923
beta_map_freq <- 2.247106

shape_parameter <- 1 

control_medians <- c(12,12.25,12.5,12.75,13,13.25,13.5,13.75,14,14.25,14.5,14.75,15,15.25,15.5,15.75,16,16.25,16.5,16.75,17,17.25,17.5,17.75,18,18.25,18.5,18.75,19,19.25,19.5,19.75,20,20.25,20.5,20.75,21)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "HR")))

desired_HR = 0.723

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "HR"] = (lambda_control / lambda_treatment)^shape_parameter
}
medians <- medians[,1:2]

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- TRUE
IA <- NULL
method_IA <- "OF"
n_exp_events <- 322
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE
Dynamic_Borrowing_PP = FALSE

analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_freq, beta_map_freq, 1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Power Prior a0 = 0.1") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_power_prior.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = 0.1

# Datos externos
control_p2 <- read.csv("IPD_Lartruvo_survival_Externo_2_Seddon_2017.csv")
control_p2$arm <- 1
external_data <- control_p2
external_data$dataset <- 'external'
names(external_data)[names(external_data) == "Survival.time"] <- "time"
names(external_data)[names(external_data) == "Status"] <- "status"
external_data$id <- seq(1, dim(external_data)[1], by = 1)
external_data$status <- ifelse(external_data$status==1, TRUE, FALSE)      

# sim15 <-  sim_trials(n_sim = n_sim,
#                        analysis = "bayes",
#                        sample_size = sample_size,
#                        ratio = ratio,
#                        rand_type = rand_type,
#                        Tmax = Tmax,
#                        scenarios_eff = scenarios_eff,
#                        shape_parameter = shape_parameter,
#                        censor = censor,
#                        test.type = test.type,
#                        alpha = alpha,
#                        method_IA = method_IA,
#                        IA = IA,
#                        n_exp_events = n_exp_events,
#                        HR_1 = HR_1,
#                        Plot_Power = Plot_Power,
#                        modelo = modelo,
#                        modelo_bayes_test = modelo_bayes_test,
#                        prior_type = prior_type,
#                        P_HR_data_Boundary = P_HR_data_Boundary,
#                        prior_gamma = prior_gamma,
#                        Plot_Power_scenarios = Plot_Power_scenarios,
#                        desired_HRs = desired_HRs,
#                        plot_pvalues = plot_pvalues,
#                        Plot_Control_Scenarios = Plot_Control_Scenarios,
#                        Dynamic_Borrowing_PP = Dynamic_Borrowing_PP,
#                        external_data = external_data,
#                        w = w,
#                        a0 = a0,
#                        seed = seed)
# 
# sim15[[2]]
# sim15[[3]]
# sim15[[4]]
# 
# sim150 <- sim15[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim150)
# saveWorkbook(wd, "Power_Prior_Seddon_a0_0.1_460_3000.xlsx", overwrite = TRUE)


## Leemos los datos de las tablas obtenidas con el ordenador de multicore ##


sheet_names <- c("Sheet1")

Power_Prior_Seddon_a0_0.1 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Power_Prior_Seddon_a0_0.1_460_3000.xlsx", sheet = sheet_names[1])

Power_Prior_Seddon_a0_0.2 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Power_Prior_Seddon_a0_0.2_460_3000.xlsx", sheet = sheet_names[1])

Power_Prior_Seddon_a0_0.3 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Power_Prior_Seddon_a0_0.3_460_3000.xlsx", sheet = sheet_names[1])

Power_Prior_Seddon_a0_0.4 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Power_Prior_Seddon_a0_0.4_460_3000.xlsx", sheet = sheet_names[1])

Power_Prior_Seddon_a0_0.5 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Power_Prior_Seddon_a0_0.5_460_3000.xlsx", sheet = sheet_names[1])



# Bayes_Power_Prior_a0_0.1

data_Bayes_Power_Prior_Seddon_a0_0.1 <- Power_Prior_Seddon_a0_0.1 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Power Prior Seddon a0 = 0.1")

data_filtered_diff_not_zero_Bayes_Power_Prior_Seddon_a0_0.1 <- data_Bayes_Power_Prior_Seddon_a0_0.1 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_Bayes_Power_Prior_Seddon_a0_0.1 <- data_Bayes_Power_Prior_Seddon_a0_0.1 %>%
  filter(median_control == median_treatment)


# Bayes_Power_Prior_a0_0.2

data_Bayes_Power_Prior_Seddon_a0_0.2 <- Power_Prior_Seddon_a0_0.2 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Power Prior Seddon a0 = 0.2")

data_filtered_diff_not_zero_Bayes_Power_Prior_Seddon_a0_0.2 <- data_Bayes_Power_Prior_Seddon_a0_0.2 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_Bayes_Power_Prior_Seddon_a0_0.2 <- data_Bayes_Power_Prior_Seddon_a0_0.2 %>%
  filter(median_control == median_treatment)


# Bayes_Power_Prior_a0_0.3

data_Bayes_Power_Prior_Seddon_a0_0.3 <- Power_Prior_Seddon_a0_0.3 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Power Prior Seddon a0 = 0.3")

data_filtered_diff_not_zero_Bayes_Power_Prior_Seddon_a0_0.3 <- data_Bayes_Power_Prior_Seddon_a0_0.3 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_Bayes_Power_Prior_Seddon_a0_0.3<- data_Bayes_Power_Prior_Seddon_a0_0.3 %>%
  filter(median_control == median_treatment)

# Bayes_Power_Prior_a0_0.4

data_Bayes_Power_Prior_Seddon_a0_0.4 <- Power_Prior_Seddon_a0_0.4 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Power Prior Seddon a0 = 0.4")

data_filtered_diff_not_zero_Bayes_Power_Prior_Seddon_a0_0.4 <- data_Bayes_Power_Prior_Seddon_a0_0.4 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_Bayes_Power_Prior_Seddon_a0_0.4 <- data_Bayes_Power_Prior_Seddon_a0_0.4 %>%
  filter(median_control == median_treatment)

# Bayes_Power_Prior_a0_0.5

data_Bayes_Power_Prior_Seddon_a0_0.5 <- Power_Prior_Seddon_a0_0.5 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Power Prior Seddon a0 = 0.5")

data_filtered_diff_not_zero_Bayes_Power_Prior_Seddon_a0_0.5 <- data_Bayes_Power_Prior_Seddon_a0_0.5 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_Bayes_Power_Prior_Seddon_a0_0.5 <- data_Bayes_Power_Prior_Seddon_a0_0.5 %>%
  filter(median_control == median_treatment)


combined_data_diff_not_zero <- bind_rows(data_filtered_diff_not_zero_frequentist,
                                         data_filtered_diff_not_zero_Bayes_Power_Prior_Seddon_a0_0.1,
                                         data_filtered_diff_not_zero_Bayes_Power_Prior_Seddon_a0_0.2,
                                         data_filtered_diff_not_zero_Bayes_Power_Prior_Seddon_a0_0.3,
                                         data_filtered_diff_not_zero_Bayes_Power_Prior_Seddon_a0_0.4,
                                         data_filtered_diff_not_zero_Bayes_Power_Prior_Seddon_a0_0.5)

combined_data_diff_zero <- bind_rows(data_filtered_diff_zero_frequentist,
                                     data_filtered_diff_zero_Bayes_Power_Prior_Seddon_a0_0.1,
                                     data_filtered_diff_zero_Bayes_Power_Prior_Seddon_a0_0.2,
                                     data_filtered_diff_zero_Bayes_Power_Prior_Seddon_a0_0.3,
                                     data_filtered_diff_zero_Bayes_Power_Prior_Seddon_a0_0.4,
                                     data_filtered_diff_zero_Bayes_Power_Prior_Seddon_a0_0.5)

## Ahora sacamos las gráficas de las características operantes usando como prior el MAP Bayesiano con pesos ##

# MSE

p6 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = mean_MSE, color = Prior)) + 
  geom_line(size = 1.2) +
  geom_vline(xintercept = median_control_center, linetype = "solid") +
  labs(
    title = "Mean Square Error (MSE)",
    x = "Median Control Arm (in months)",
    y = "Mean MSE",
    color = "Bayesian Prior Distributions"
  ) +
  theme_minimal() + 
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 10, face = "bold"),
    axis.text = element_text(size = 7),
    axis.title.x = element_text(size = 11),
    axis.title.y = element_text(size = 11),
    legend.margin = margin(9, 9, 9, 9),
    legend.text = element_text(size = 9),
    legend.title = element_text(size = 10, face = "bold")
  ) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.075))

p6

# Power

p7 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line(size = 1.2) + 
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "Power", x = "Median Control Arm (in months)", y = "Proportion of Significant Trials", color = "Bayesian Prior Distributions") +
  theme_minimal() + 
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 7),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.margin = margin(12, 12, 12, 12),
        legend.text = element_text(size = 11),
        legend.title = element_text(size = 12, face = "bold")) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_linetype_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0.5, 1))
p7

# ET1

p8 <- ggplot(combined_data_diff_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line(size = 1.2) + 
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  geom_hline(yintercept = 0.02, linetype = "dashed", color = "green") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(title = "Type I Error (T1E)", x = "Median Control (in months)", y = "Proportion of Significant Trials", color = "Bayesian Prior Distributions") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 7),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.margin = margin(12, 12, 12, 12),
        legend.text = element_text(size = 11),
        legend.title = element_text(size = 12, face = "bold")) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_linetype_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_zero$median_control), max(combined_data_diff_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.2))
p8
      
```


```{r T1E_combined_powerprior, eval = T}

library(cowplot)

p66 <- p6 + scale_color_discrete(
  name = "Bayesian Prior Distributions", 
  labels = c(
        "Frequentist",
         "Power Prior a0=0.1 using\nPseudo-IPD from\nSeddon et al.(2017)",
         "Power Prior a0=0.2 using\nPseudo-IPD from\nSeddon et al.(2017)",
         "Power Prior a0=0.3 using\nPseudo-IPD from\nSeddon et al.(2017)",
         "Power Prior a0=0.4 using\nPseudo-IPD from\nSeddon et al.(2017)",
         "Power Prior a0=0.5 using\nPseudo-IPD from\nSeddon et al.(2017)")
) +
  guides(color = guide_legend(ncol = 8))  

legend <- get_legend(
  p66 +
    theme(
      # legend.box = "horizontal", 
      legend.justification = c(0, 1),
      # legend.margin = margin(8, 8, 8, 8),
      legend.text = element_text(size = 10), 
      legend.title = element_text(size = 11, face = "bold")
    )
)

# Remove individual legends from the plots
p66 <- p66 + theme(legend.position = "none")
p77 <- p7 + theme(legend.position = "none")
p88 <- p8 + theme(legend.position = "none")

p66 <- p66 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p77 <- p77 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p88 <- p88 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))

plots_row <- plot_grid(p66, p77, p88, ncol = 3, labels = c("A", "B", "C"), label_size = 14)

combined_plot <- plot_grid(
  plots_row,
  legend,
  ncol = 1,                 
  rel_heights = c(3, 0.4)   
)

print(combined_plot)

```


De aquí tampoco pero es verdad que se comporta mucho mejor que el anterior. Estos datos son lo que se parecen más a la realidad pero en el momento de hacer el diseño, no lo sabían obviamente.

Aquí llama la atención que, al poner más peso a la parte informativa, mejor controla el ET1. ¿?¿?¿?




# Dynamic Power Prior


A continuación se propone el último modelo que me parece de mucho interés. Este es parte de la familia de los Power Prior y la diferencia con el anterior es que, en vez de prespecificar pesos para el descuento del control externo a0, vamos a crear unas reglas hechas de antemano para la elección automática del a0.

Aquí tenemos el diagrama de barras para entenderlo mucho más fácilmente para los dos modelos que he hecho, 1) con pesos más altos con un máximo de 0.5 y 2) el mismo esquema, pero los pesos considerados están a la mitad, siendo el máximo de 0.25.

```{r Tabla_condiciones_Dynamic_Power_Prior1 , eval = T}

# Dibujamos los esquemas dinámicos en los que se va a prestar información en función de cómo se parezca el brazo control y el control externo.

# 1) Pesos más altos


breakpoints <- c(0.775, 0.825, 0.875, 0.925, 0.975, 1.025, 1.075, 1.125, 1.175, 1.225)
strengths <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.4, 0.3, 0.2, 0.1, 0)

df <- data.frame(
  HR = breakpoints[-length(breakpoints)], 
  HR_next = breakpoints[-1], 
  Strength = strengths[-length(strengths)] 
)

p1 <- ggplot(df, aes(xmin = HR, xmax = HR_next, ymin = 0, ymax = Strength)) +
  geom_rect(color = "black", fill = "lightsteelblue1") +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red") +
  scale_x_continuous(name = "Hazard Ratio", limits = c(0.7, 1.3), breaks = seq(0.6, 1.4, 0.05)) +
  scale_y_continuous(name = "Borrowing Strength", limits = c(0, 0.6), breaks = seq(0, 0.5, 0.05)) +
  theme_minimal() +
  theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) 

p1

# 2) Pesos ma la mitad

breakpoints <- c(0.775, 0.825, 0.875, 0.925, 0.975, 1.025, 1.075, 1.125, 1.175, 1.225)
strengths <- c(0.05, 0.1, 0.15, 0.2, 0.25, 0.2, 0.15, 0.1, 0.05, 0)

df <- data.frame(
  HR = breakpoints[-length(breakpoints)], 
  HR_next = breakpoints[-1], 
  Strength = strengths[-length(strengths)] 
)

p2 <- ggplot(df, aes(xmin = HR, xmax = HR_next, ymin = 0, ymax = Strength)) +
  geom_rect(color = "black", fill = "lightsteelblue1") +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red") +
  scale_x_continuous(name = "Hazard Ratio", limits = c(0.7, 1.3), breaks = seq(0.6, 1.4, 0.05)) +
  scale_y_continuous(name = "Borrowing Strength", limits = c(0, 0.3), breaks = seq(0, 0.5, 0.05)) +
  theme_minimal() +
  theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) 

p2
      
```

```{r T1E_combined_all, eval = T}

library(cowplot)

p11 <- p1 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p22 <- p2 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))

plots_row <- plot_grid(p11, p22, ncol = 2, labels = c("A", "B"), label_size = 14)

combined_plot <- plot_grid(
  plots_row,
  #legend,
  ncol = 1,                 
  rel_heights = c(3, 0.4)   
)

print(combined_plot)

```

## 1) Dynamic Power Prior - Información Fase 2


Como podemos observar, si los datos se parecen (i.e., HR=1), entonces se va a prestar más información y a0 tendrá el máximo valor que es 0.5, de lo contrario, si se va diferenciando progresivamente el peso para a0 va a ir disminuyendo. Si el HR entre el brazo control y el brazo control externo es menor de 0.7 o mayor de 1.3, entonces no se va a el power prior y se hará un análisis normal (a0 = 0).

Para estudiar esta similitud, he hecho un código STAN adhoc usando una regresión de Cox para compararlo, sin embargo, el tiempo de simulación tarda el doble. Es por eso, que para comparar estos 2 brazos control, lo he hecho con una regresión de Cox frecuentista. Simplemente para ahorrar tiempo ya que lo he comprobado y sale exactamente lo mismo al usar en el Bayesiano priors no informativas N(0,10).

La condición que se ve en el gráfico de barras lo he puesto directamente en la función madre para no complicarme, así que por eso no está como input. Pero ya con más tiempo se puede hacer especificarlo desde la llamada de la función. 

Esto es muy útil porque no es una caja negra como si usáramos el peso como otra variable aleatoria más, si no que sabemos de antemano las condiciones y esto podemos verlo desde un punto de vista regulatorio.

```{r Dynamic_Power_Prior , eval = T}

shape_parameter <- 1 

control_medians <- c(12,12.25,12.5,12.75,13,13.25,13.5,13.75,14,14.25,14.5,14.75,15,15.25,15.5,15.75,16,16.25,16.5,16.75,17,17.25,17.5,17.75,18,18.25,18.5,18.75,19,19.25,19.5,19.75,20,20.25,20.5,20.75,21)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "HR")))

desired_HR = 0.723

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "HR"] = (lambda_control / lambda_treatment)^shape_parameter
}
medians <- medians[,1:2]

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- TRUE
IA <- NULL
method_IA <- "OF"
n_exp_events <- 322
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE

analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_freq, beta_map_freq, 1, 1/19), nrow = 2, ncol = 2, byrow = TRUE)
prior_type <- c("Dynamic Power Prior HW") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_power_prior.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = NULL

# Datos externos
control_p2 <- read.csv("IPD_Lartruvo_survival_CONTROL_P2.csv")
#control_p2 <- read.csv("IPD_Lartruvo_survival_Externo_2_Seddon_2017.csv")
control_p2$arm <- 1
external_data <- control_p2
external_data$dataset <- 'external'
names(external_data)[names(external_data) == "Survival.time"] <- "time"
names(external_data)[names(external_data) == "Status"] <- "status"
external_data$id <- seq(1, dim(external_data)[1], by = 1)
external_data$status <- ifelse(external_data$status==1, TRUE, FALSE) 

# Lo que decide que haga el dynamic borrowing

Dynamic_Borrowing_PP = TRUE

# sim21 <-  sim_trials(n_sim = n_sim,
#                      analysis = "bayes",
#                      sample_size = sample_size,
#                      ratio = ratio,
#                      rand_type = rand_type,
#                      Tmax = Tmax,
#                      scenarios_eff = scenarios_eff,
#                      shape_parameter = shape_parameter,
#                      censor = censor,
#                      test.type = test.type,
#                      alpha = alpha,
#                      method_IA = method_IA,
#                      IA = IA,
#                      n_exp_events = n_exp_events,
#                      HR_1 = HR_1,
#                      Plot_Power = Plot_Power,
#                      modelo = modelo,
#                      modelo_bayes_test = modelo_bayes_test,
#                      prior_type = prior_type,
#                      P_HR_data_Boundary = P_HR_data_Boundary,
#                      prior_gamma = prior_gamma,
#                      Plot_Power_scenarios = Plot_Power_scenarios,
#                      desired_HRs = desired_HRs,
#                      plot_pvalues = plot_pvalues,
#                      Plot_Control_Scenarios = Plot_Control_Scenarios,
#                      Dynamic_Borrowing_PP = Dynamic_Borrowing_PP,
#                      external_data = external_data,
#                      w = w,
#                      a0 = a0,
#                      seed = seed)
# 
# sim21[[2]]
# sim21[[3]]
# sim21[[4]]
# 
# sim210 <- sim21[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim210)
# saveWorkbook(wd, "Dynamic_Power_Prior_HW_460_3000.xlsx", overwrite = TRUE)

## Leemos los datos de las tablas obtenidas con el ordenador de multicore ##

sheet_names <- c("Sheet1")

# 1) Dynamic Power Prior: Pesos altos

Bayes_Dynamic_Power_Prior_HW <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Dynamic_Power_Prior_HW_460_3000.xlsx", sheet = sheet_names[1])

# 1) Dynamic Power Prior: Pesos bajos

Bayes_Dynamic_Power_Prior_LW <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Dynamic_Power_Prior_LW_460_3000.xlsx", sheet = sheet_names[1])


# Bayes_Dynamic_Power_Prior_HW

data_Bayes_Dynamic_Power_Prior_HW <- Bayes_Dynamic_Power_Prior_HW %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Dynamic Power Prior HW")

data_filtered_diff_not_zero_Bayes_Dynamic_Power_Prior_HW <- data_Bayes_Dynamic_Power_Prior_HW %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_Bayes_Dynamic_Power_Prior_HW <- data_Bayes_Dynamic_Power_Prior_HW %>%
  filter(median_control == median_treatment)

# Bayes_Dynamic_Power_Prior_LW

data_Bayes_Dynamic_Power_Prior_LW <- Bayes_Dynamic_Power_Prior_LW %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Dynamic Power Prior LW")

data_filtered_diff_not_zero_Bayes_Dynamic_Power_Prior_LW <- data_Bayes_Dynamic_Power_Prior_LW %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_Bayes_Dynamic_Power_Prior_LW <- data_Bayes_Dynamic_Power_Prior_LW %>%
  filter(median_control == median_treatment)

sheet_names <- c("Sheet1")

# 1) Dynamic Power Prior: Pesos altos

Bayes_Dynamic_Power_Prior_Seddon_HW <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Dynamic_Power_Prior_Seddon_HW_460_3000.xlsx", sheet = sheet_names[1])

# 1) Dynamic Power Prior: Pesos bajos

Bayes_Dynamic_Power_Prior_Seddon_LW <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Dynamic_Power_Prior_Seddon_LW_460_3000.xlsx", sheet = sheet_names[1])


# Bayes_Dynamic_Power_Prior_Seddon_HW

data_Bayes_Dynamic_Power_Prior_Seddon_HW <- Bayes_Dynamic_Power_Prior_Seddon_HW %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Dynamic Power Prior Seddon HW")

data_filtered_diff_not_zero_Bayes_Dynamic_Power_Prior_Seddon_HW <- data_Bayes_Dynamic_Power_Prior_Seddon_HW %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_Bayes_Dynamic_Power_Prior_Seddon_HW <- data_Bayes_Dynamic_Power_Prior_Seddon_HW %>%
  filter(median_control == median_treatment)

# Bayes_Dynamic_Power_Prior_Seddon_LW

data_Bayes_Dynamic_Power_Prior_Seddon_LW <- Bayes_Dynamic_Power_Prior_Seddon_LW %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Dynamic Power Prior Seddon LW")

data_filtered_diff_not_zero_Bayes_Dynamic_Power_Prior_Seddon_LW <- data_Bayes_Dynamic_Power_Prior_Seddon_LW %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_Bayes_Dynamic_Power_Prior_Seddon_LW <- data_Bayes_Dynamic_Power_Prior_Seddon_LW %>%
  filter(median_control == median_treatment)


combined_data_diff_not_zero <- bind_rows(data_filtered_diff_not_zero_frequentist,
                                         data_filtered_diff_not_zero_Bayes_Dynamic_Power_Prior_HW,
                                         data_filtered_diff_not_zero_Bayes_Dynamic_Power_Prior_LW,
                                         data_filtered_diff_not_zero_Bayes_Dynamic_Power_Prior_Seddon_HW,
                                         data_filtered_diff_not_zero_Bayes_Dynamic_Power_Prior_Seddon_LW)

combined_data_diff_zero <- bind_rows(data_filtered_diff_zero_frequentist,
                                     data_filtered_diff_zero_Bayes_Dynamic_Power_Prior_HW,
                                     data_filtered_diff_zero_Bayes_Dynamic_Power_Prior_LW,
                                     data_filtered_diff_zero_Bayes_Dynamic_Power_Prior_Seddon_HW,
                                     data_filtered_diff_zero_Bayes_Dynamic_Power_Prior_Seddon_LW)

## Ahora sacamos las gráficas de las características operantes usando como prior el MAP Bayesiano con pesos ##

# MSE

p6 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = mean_MSE, color = Prior)) + 
  geom_line(size = 1.2) +
  geom_vline(xintercept = median_control_center, linetype = "solid") +
  labs(
    title = "Mean Square Error (MSE)",
    x = "Median Control Arm (in months)",
    y = "Mean MSE",
    color = "Bayesian Prior Distributions"
  ) +
  theme_minimal() + 
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 10, face = "bold"),
    axis.text = element_text(size = 7),
    axis.title.x = element_text(size = 11),
    axis.title.y = element_text(size = 11),
    legend.margin = margin(9, 9, 9, 9),
    legend.text = element_text(size = 9),
    legend.title = element_text(size = 10, face = "bold")
  ) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.075))

p6

# Power

p7 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line(size = 1.2) + 
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "Power", x = "Median Control Arm (in months)", y = "Proportion of Significant Trials", color = "Bayesian Prior Distributions") +
  theme_minimal() + 
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 7),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.margin = margin(12, 12, 12, 12),
        legend.text = element_text(size = 11),
        legend.title = element_text(size = 12, face = "bold")) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_linetype_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0.5, 1))
p7

# ET1

p8 <- ggplot(combined_data_diff_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line(size = 1.2) + 
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  geom_hline(yintercept = 0.02, linetype = "dashed", color = "green") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(title = "Type I Error (T1E)", x = "Median Control (in months)", y = "Proportion of Significant Trials", color = "Bayesian Prior Distributions") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 7),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.margin = margin(12, 12, 12, 12),
        legend.text = element_text(size = 11),
        legend.title = element_text(size = 12, face = "bold")) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_linetype_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_zero$median_control), max(combined_data_diff_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.2))
p8

      
```


```{r T1E_combined_powerprior2, eval = T}

library(cowplot)

p66 <- p6 + scale_color_discrete(
  name = "Bayesian Prior Distributions", 
  labels = c(
        "Frequentist",
        "Dynamic Power Prior with High Weights (0, 0.5)",
        "Dynamic Power Prior with Low Weights (0, 0.25)",
        "Dynamic Power Prior with High Weights (0, 0.5)\nusing Pseudo-IPD from Seddon et al.(2017)",
        "Dynamic Power Prior with Low Weights (0, 0.25)\nusing Pseudo-IPD from Seddon et al.(2017)"
  )
) +
  guides(color = guide_legend(ncol = 5))  

legend <- get_legend(
  p66 +
    theme(
      # legend.box = "horizontal", 
      legend.justification = c(0, 1),
      # legend.margin = margin(8, 8, 8, 8),
      legend.text = element_text(size = 10), 
      legend.title = element_text(size = 11, face = "bold")
    )
)

# Remove individual legends from the plots
p66 <- p66 + theme(legend.position = "none")
p77 <- p7 + theme(legend.position = "none")
p88 <- p8 + theme(legend.position = "none")

p66 <- p66 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p77 <- p77 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p88 <- p88 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))

plots_row <- plot_grid(p66, p77, p88, ncol = 3, labels = c("A", "B", "C"), label_size = 14)

combined_plot <- plot_grid(
  plots_row,
  legend,
  ncol = 1,                 
  rel_heights = c(3, 0.4)   
)

print(combined_plot)

```


## 2) Dynamic Power Prior - Información Seddon (parecida a lo que pasó después)


Como podemos observar, si los datos se parecen (i.e., HR=1), entonces se va a prestar más información y a0 tendrá el máximo valor que es 0.5, de lo contrario, si se va diferenciando progresivamente el peso para a0 va a ir disminuyendo. Si el HR entre el brazo control y el brazo control externo es menor de 0.7 o mayor de 1.3, entonces no se va a el power prior y se hará un análisis normal (a0 = 0).

Para estudiar esta similitud, he hecho un código STAN adhoc usando una regresión de Cox para compararlo, sin embargo, el tiempo de simulación tarda el doble. Es por eso, que para comparar estos 2 brazos control, lo he hecho con una regresión de Cox frecuentista. Simplemente para ahorrar tiempo ya que lo he comprobado y sale exactamente lo mismo al usar en el Bayesiano priors no informativas N(0,10).

La condición que se ve en el gráfico de barras lo he puesto directamente en la función madre para no complicarme, así que por eso no está como input. Pero ya con más tiempo se puede hacer especificarlo desde la llamada de la función. 

Esto es muy útil porque no es una caja negra como si usáramos el peso como otra variable aleatoria más, si no que sabemos de antemano las condiciones y esto podemos verlo desde un punto de vista regulatorio.

```{r Dynamic_Power_Prior_seddon , eval = T}

shape_parameter <- 1 

control_medians <- c(12,12.25,12.5,12.75,13,13.25,13.5,13.75,14,14.25,14.5,14.75,15,15.25,15.5,15.75,16,16.25,16.5,16.75,17,17.25,17.5,17.75,18,18.25,18.5,18.75,19,19.25,19.5,19.75,20,20.25,20.5,20.75,21)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "HR")))

desired_HR = 0.723

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "HR"] = (lambda_control / lambda_treatment)^shape_parameter
}
medians <- medians[,1:2]

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- TRUE
IA <- NULL
method_IA <- "OF"
n_exp_events <- 322
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE

analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_freq, beta_map_freq, 1, 1/19), nrow = 2, ncol = 2, byrow = TRUE)
prior_type <- c("Dynamic Power Prior HW") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_power_prior.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = NULL

# Datos externos
#control_p2 <- read.csv("IPD_Lartruvo_survival_CONTROL_P2.csv")
control_p2 <- read.csv("IPD_Lartruvo_survival_Externo_2_Seddon_2017.csv")
control_p2$arm <- 1
external_data <- control_p2
external_data$dataset <- 'external'
names(external_data)[names(external_data) == "Survival.time"] <- "time"
names(external_data)[names(external_data) == "Status"] <- "status"
external_data$id <- seq(1, dim(external_data)[1], by = 1)
external_data$status <- ifelse(external_data$status==1, TRUE, FALSE) 

# Lo que decide que haga el dynamic borrowing

Dynamic_Borrowing_PP = TRUE

# sim21 <-  sim_trials(n_sim = n_sim,
#                      analysis = "bayes",
#                      sample_size = sample_size,
#                      ratio = ratio,
#                      rand_type = rand_type,
#                      Tmax = Tmax,
#                      scenarios_eff = scenarios_eff,
#                      shape_parameter = shape_parameter,
#                      censor = censor,
#                      test.type = test.type,
#                      alpha = alpha,
#                      method_IA = method_IA,
#                      IA = IA,
#                      n_exp_events = n_exp_events,
#                      HR_1 = HR_1,
#                      Plot_Power = Plot_Power,
#                      modelo = modelo,
#                      modelo_bayes_test = modelo_bayes_test,
#                      prior_type = prior_type,
#                      P_HR_data_Boundary = P_HR_data_Boundary,
#                      prior_gamma = prior_gamma,
#                      Plot_Power_scenarios = Plot_Power_scenarios,
#                      desired_HRs = desired_HRs,
#                      plot_pvalues = plot_pvalues,
#                      Plot_Control_Scenarios = Plot_Control_Scenarios,
#                      Dynamic_Borrowing_PP = Dynamic_Borrowing_PP,
#                      external_data = external_data,
#                      w = w,
#                      a0 = a0,
#                      seed = seed)
# 
# sim21[[2]]
# sim21[[3]]
# sim21[[4]]
# 
# sim210 <- sim21[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim210)
# saveWorkbook(wd, "Dynamic_Power_Prior_HW_460_3000.xlsx", overwrite = TRUE)

## Leemos los datos de las tablas obtenidas con el ordenador de multicore ##

sheet_names <- c("Sheet1")

# 1) Dynamic Power Prior: Pesos altos

Bayes_Dynamic_Power_Prior_Seddon_HW <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Dynamic_Power_Prior_Seddon_HW_460_3000.xlsx", sheet = sheet_names[1])

# 1) Dynamic Power Prior: Pesos bajos

Bayes_Dynamic_Power_Prior_Seddon_LW <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Dynamic_Power_Prior_Seddon_LW_460_3000.xlsx", sheet = sheet_names[1])


# Bayes_Dynamic_Power_Prior_Seddon_HW

data_Bayes_Dynamic_Power_Prior_Seddon_HW <- Bayes_Dynamic_Power_Prior_Seddon_HW %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Dynamic Power Prior Seddon HW")

data_filtered_diff_not_zero_Bayes_Dynamic_Power_Prior_Seddon_HW <- data_Bayes_Dynamic_Power_Prior_Seddon_HW %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_Bayes_Dynamic_Power_Prior_Seddon_HW <- data_Bayes_Dynamic_Power_Prior_Seddon_HW %>%
  filter(median_control == median_treatment)

# Bayes_Dynamic_Power_Prior_Seddon_LW

data_Bayes_Dynamic_Power_Prior_Seddon_LW <- Bayes_Dynamic_Power_Prior_Seddon_LW %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Dynamic Power Prior Seddon LW")

data_filtered_diff_not_zero_Bayes_Dynamic_Power_Prior_Seddon_LW <- data_Bayes_Dynamic_Power_Prior_Seddon_LW %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_Bayes_Dynamic_Power_Prior_Seddon_LW <- data_Bayes_Dynamic_Power_Prior_Seddon_LW %>%
  filter(median_control == median_treatment)


combined_data_diff_not_zero <- bind_rows(data_filtered_diff_not_zero_frequentist,
                                         data_filtered_diff_not_zero_Bayes_Dynamic_Power_Prior_Seddon_HW,
                                         data_filtered_diff_not_zero_Bayes_Dynamic_Power_Prior_Seddon_LW)

combined_data_diff_zero <- bind_rows(data_filtered_diff_zero_frequentist,
                                     data_filtered_diff_zero_Bayes_Dynamic_Power_Prior_Seddon_HW,
                                     data_filtered_diff_zero_Bayes_Dynamic_Power_Prior_Seddon_LW)

## Ahora sacamos las gráficas de las características operantes usando como prior el MAP Bayesiano con pesos ##

# MSE

p6 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = mean_MSE, color = Prior, linetype = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "MSE", x = "Median Control", y = "Mean MSE") +
  theme_minimal()  + 
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.075))
p6

# Power

p7 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "Power", x = "Median Control", y = "Prop. Significant") +
  theme_minimal() + 
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 1))
p7

# ET1

p8 <- ggplot(combined_data_diff_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  geom_hline(yintercept = 0.02, linetype = "dashed", color = "green") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(title = "Type I Error", x = "Median Control", y = "Prop. Significant") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_zero$median_control), max(combined_data_diff_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.5))
p8

      
```


# BONUS: Weak Informative usando la asunción de Seddon de 19 meses (no hago el del F2 porque ya se hizo)

parameters <- medians / (log(2)^(1/shape_parameter))

Control: Mediana de 19 es 27.41121 

```{r `Bayes: Weak informative prior distribution Seddon`, eval = T}

shape_parameter <- 1 

control_medians <- c(12,12.25,12.5,12.75,13,13.25,13.5,13.75,14,14.25,14.5,14.75,15,15.25,15.5,15.75,16,16.25,16.5,16.75,17,17.25,17.5,17.75,18,18.25,18.5,18.75,19,19.25,19.5,19.75,20,20.25,20.5,20.75,21)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "HR")))

desired_HR = 0.723

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "HR"] = (lambda_control / lambda_treatment)^shape_parameter
}
medians <- medians[,1:2]

sample_size <- 460
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- TRUE
IA <- NULL
n_exp_events <- 322
method_IA <- "bayes"
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE
analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(27.41121, 1, 1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Weak Informative prior Seddon ~ Ga(27.41121, 1)") 
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = NULL

# sim23 <-  sim_trials(n_sim = n_sim,
#                      analysis = "bayes",
#                      sample_size = sample_size,
#                      ratio = ratio,
#                      rand_type = rand_type,
#                      Tmax = Tmax,
#                      scenarios_eff = scenarios_eff,
#                      shape_parameter = shape_parameter,
#                      censor = censor,
#                      test.type = test.type,
#                      alpha = alpha,
#                      method_IA = method_IA,
#                      IA = IA,
#                      n_exp_events = n_exp_events,
#                      HR_1 = HR_1,
#                      Plot_Power = Plot_Power,
#                      modelo = modelo,
#                      modelo_bayes_test = modelo_bayes_test,
#                      prior_type = prior_type,
#                      P_HR_data_Boundary = P_HR_data_Boundary,
#                      prior_gamma = prior_gamma,
#                      Plot_Power_scenarios = Plot_Power_scenarios,
#                      desired_HRs = desired_HRs,
#                      plot_pvalues = plot_pvalues,
#                      Plot_Control_Scenarios = Plot_Control_Scenarios,
#                      seed = seed)
# sim23[[2]]
# sim23[[3]]
# sim23[[4]]
# 
# sim230 <- sim23[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim230)
# saveWorkbook(wd, "Bayes_Weak_Informative_Seddon_460_3000.xlsx", overwrite = TRUE)

sheet_names <- c("Sheet1")
Bayes_Weak_Informative_Seddon <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/Bayes_Weak_Informative_Seddon_460_3000.xlsx", sheet = sheet_names[1])

data_Bayes_Weak_Informative_Seddon <- Bayes_Weak_Informative_Seddon %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Weak Informative Prior according to Seddon")

data_filtered_diff_not_zero_Bayes_Weak_Informative_Seddon <- data_Bayes_Weak_Informative_Seddon %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_Bayes_Weak_Informative_Seddon<- data_Bayes_Weak_Informative_Seddon %>%
  filter(median_control == median_treatment)


combined_data_diff_not_zero <- bind_rows(data_filtered_diff_not_zero_frequentist,
                                         data_filtered_diff_not_zero_mid_informative,
                                     data_filtered_diff_not_zero_Bayes_Weak_Informative_Seddon)

combined_data_diff_zero <- bind_rows(data_filtered_diff_zero_frequentist,
                                     data_filtered_diff_zero_mid_informative,
                                     data_filtered_diff_zero_Bayes_Weak_Informative_Seddon)

## Ahora sacamos las gráficas de las características operantes usando como prior el MAP Bayesiano con pesos ##

# MSE

p6 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = mean_MSE, color = Prior)) + 
  geom_line(size = 1.2) +
  geom_vline(xintercept = median_control_center, linetype = "solid") +
  labs(
    title = "Mean Square Error (MSE)",
    x = "Median Control Arm (in months)",
    y = "Mean MSE",
    color = "Bayesian Prior Distributions"
  ) +
  theme_minimal() + 
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 10, face = "bold"),
    axis.text = element_text(size = 7),
    axis.title.x = element_text(size = 11),
    axis.title.y = element_text(size = 11),
    legend.margin = margin(9, 9, 9, 9),
    legend.text = element_text(size = 9),
    legend.title = element_text(size = 10, face = "bold")
  ) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.075))

p6

# Power

p7 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line(size = 1.2) + 
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "Power", x = "Median Control Arm (in months)", y = "Proportion of Significant Trials", color = "Bayesian Prior Distributions") +
  theme_minimal() + 
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 7),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.margin = margin(12, 12, 12, 12),
        legend.text = element_text(size = 11),
        legend.title = element_text(size = 12, face = "bold")) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_linetype_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0.5, 1))
p7

# ET1

p8 <- ggplot(combined_data_diff_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line(size = 1.2) + 
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  geom_hline(yintercept = 0.02, linetype = "dashed", color = "green") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(title = "Type I Error (T1E)", x = "Median Control (in months)", y = "Proportion of Significant Trials", color = "Bayesian Prior Distributions") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 7),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.margin = margin(12, 12, 12, 12),
        legend.text = element_text(size = 11),
        legend.title = element_text(size = 12, face = "bold")) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_linetype_discrete(name = "Bayesian Prior Distributions") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_zero$median_control), max(combined_data_diff_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.2))
p8


```

```{r T1E_combined_powerprior2, eval = T}

library(cowplot)

p66 <- p6 + scale_color_discrete(
  name = "Bayesian Prior Distributions", 
  labels = c(
        "Frequentist",
        "Weak Informative Prior\nBased on Seddon et al.(2017)", # Esto lo cambié por el orden
        "Weak Informative Prior\ncentered at 15 months"
        
  )
) +
  guides(color = guide_legend(ncol = 3))  

legend <- get_legend(
  p66 +
    theme(
      # legend.box = "horizontal", 
      legend.justification = c(0, 1),
      # legend.margin = margin(8, 8, 8, 8),
      legend.text = element_text(size = 10), 
      legend.title = element_text(size = 11, face = "bold")
    )
)

p66 <- p66 + theme(legend.position = "none")
p77 <- p7 + theme(legend.position = "none")
p88 <- p8 + theme(legend.position = "none")

p66 <- p66 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p77 <- p77 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))
p88 <- p88 + theme(plot.background = element_rect(colour = "black", size = 0.5, fill = NA))

plots_row <- plot_grid(p66, p77, p88, ncol = 3, labels = c("A", "B", "C"), label_size = 14)

combined_plot <- plot_grid(
  plots_row,
  legend,
  ncol = 1,                 
  rel_heights = c(3, 0.4)   
)

print(combined_plot)

```



# Selección de Modelos para calcular el tamaño muestral con diferentes efectos de tratamiento.

Ahora vamos a elegir modelos tipos para ver cuántos pacientes nos ahorramos seleccionando algunos de los modelos anteriormente vistos. En todos los modelos se han hecho 5K simulaciones por cada tamaño muestral, mientras que en el frecuentista se ha hecho 100K (también hay otro de 5K pero ya que hecho el otro lo dejo):
  
  1) Frecuentista 
  2) Prior poco informativa centered at median 15 months (Infla un poco el ET1 a partir del mes 18)
  3) Robust Mixture Prior: 0.1 Prior MAP + 0.9 Prior no informativa (lo infla pero por poco)
  4) Robust Mixture Prior: 0.2 Prior MAP + 0.8 Prior no informativa (lo infla más)
Power prior: Todos lo inflan mucho, aunque el Seddon es el que menos.
Los Dynamic Power Prior están por ver.
  5) Prior poco informativa centered at median 19 months
Usando estos modelos, vamos a ver cuántos pacientes son necesarios para obtener un poder del 80%, 85% y 90% de poder. Como para evaluar estos modelos se han usado diferentes asunciones para el efecto del brazo control, vamos a asumir 2 tipos de efectos para cada uno de los modelos a la hora de generar los datos:
    
1) Efecto que se especificó en el SAP para el brazo experimental y control
2) Efecto obtenido al final del estudio para el brazo experimental y control. Para este, tengo que modificar el código para poder tener 2 shapes diferentes a la hora de general los datos para cada uno de los brazos. Primero hago una regresión weibull para cada uno de los brazos por separado y ahí, lo incorporo en el código.

Luego hay que crear una tabla resumen con cada uno de los modelos y los números de pacientes necesarios para cada uno de los 3 poderes para cada uno de los efectos diferentes. Así mismo, se especificarán los valores de MSE, poder y T1E obtenidos anteriormente.

Sin necesidad de poner los inputs para todos los modelos ya que se han hecho anteriormente, aquí sólo muestro como son los inputs para el modelo frecuentista y uno de los modelos Bayesianos.

Como esto es para los resultados finales, el nº de simulaciones por escenario ha aumentado de 3.000 a 5.000.

Como ya se han ejecutado, voy a leer los resultados directamente de todos los modelos moviendo todos los tamaños muestrales.

1) Frecuentista - SAP

```{r `Final models: Frequentist - SAP`, eval = T, echo = T}

shape_parameter <- 1
sample_size <- seq(from = 350, to= 600, by = 5)
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- matrix(c(15,20.75), nrow = 1, ncol = 2, byrow = TRUE) # HR=0.723
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- TRUE
IA <- NULL
n_exp_events <- 322
HR_1 <- FALSE
Plot_Power = TRUE
plot_pvalues = FALSE
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE
analysis = "freq" 
n_sim <- 3000
seed <- 24
# 
# sim2 <- sim_trials(n_sim = n_sim,
#                     analysis = "freq",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     shape_parameter = shape_parameter,
#                     scenarios_eff = scenarios_eff,
#                     censor = censor,
#                     alpha = alpha,
#                     test.type = test.type,
#                     IA = IA,
#                     method_IA = method_IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     seed=seed,
#                     Plot_Power = Plot_Power,
#                     plot_pvalues = plot_pvalues)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim2)
# saveWorkbook(wd, "DEF_SAP_Freq_SampleSizes.xlsx", overwrite = TRUE)

# Aquí los datos se leen del Excel con los datos simulados en el ordenador de multicore.

sheet_names <- c("Sheet1")
SAP_Freq_SampleSizes <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/DEF_SAP_Freq_SampleSizes.xlsx", sheet = sheet_names[1])

DEF_SAP_Freq_Power <- subset(SAP_Freq_SampleSizes, scenario == 1,
                             select = c(scenario, sample_size, prop_significant)) %>%
  mutate(Prior = "Frequentist")

```


2) Prior poco informativa centered at median 15 months (Infla un poco el ET1 a partir del mes 18)

```{r `Final models: Bayes - Prior poco informativa centered at median 15 months - SAP`, eval = T}

shape_parameter <- 1
sample_size <- seq(from = 350, to= 600, by = 5)
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- matrix(c(15,20.75), nrow = 1, ncol = 2, byrow = TRUE) # HR=0.723
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- FALSE
IA <- NULL
method_IA = "OF"
n_exp_events <- 322
HR_1 <- FALSE
Plot_Power = TRUE
plot_pvalues = FALSE
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE
analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(21.64043, 1, 1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Weak Informative prior ~ Ga(21.64043, 1)") 
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = NULL

# sim6 <-  sim_trials(n_sim = n_sim,
#                      analysis = "bayes",
#                      sample_size = sample_size,
#                      ratio = ratio,
#                      rand_type = rand_type,
#                      Tmax = Tmax,
#                      scenarios_eff = scenarios_eff,
#                      shape_parameter = shape_parameter,
#                      censor = censor,
#                      test.type = test.type,
#                      alpha = alpha,
#                      method_IA = method_IA,
#                      IA = IA,
#                      n_exp_events = n_exp_events,
#                      HR_1 = HR_1,
#                      Plot_Power = Plot_Power,
#                      modelo = modelo,
#                      modelo_bayes_test = modelo_bayes_test,
#                      prior_type = prior_type,
#                      P_HR_data_Boundary = P_HR_data_Boundary,
#                      prior_gamma = prior_gamma,
#                      Plot_Power_scenarios = Plot_Power_scenarios,
#                      desired_HRs = desired_HRs,
#                      plot_pvalues = plot_pvalues,
#                      Plot_Control_Scenarios = Plot_Control_Scenarios,
#                      seed = seed)
# sim6[[2]]
# sim6[[3]]
# sim6[[4]]
# 
# sim66 <- sim6[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim66)
# saveWorkbook(wd, "DEF_SAP_Bayes_Weak_Informative_SampleSizes.xlsx", overwrite = TRUE)


sheet_names <- c("Sheet1")
DEF_SAP_Bayes_Weak_Informative_SampleSizes <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/DEF_SAP_Bayes_Weak_Informative_SampleSizes.xlsx", sheet = sheet_names[1])

DEF_SAP_Bayes_Weak_Informative_Seddon_SampleSizes <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/DEF_SAP_Bayes_Weak_Informative_Seddon_SampleSizes.xlsx", sheet = sheet_names[1])



# Weak Informative

DEF_SAP_Weak_informative_Power <- subset(DEF_SAP_Bayes_Weak_Informative_SampleSizes, scenario == 1,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
  mutate(Prior = "Weak Informative Prior centered at 15 months")

# Weak Informative considering Seddon

DEF_SAP_Weak_informative_Seddon_Power <- subset(DEF_SAP_Bayes_Weak_Informative_Seddon_SampleSizes, scenario == 1, select = c(scenario, sample_size, Prior, prop_significant)) %>%
  mutate(Prior = "Weak Informative Prior centered at 19 months (Seddon)")

```


3) Robust Mixture Prior: 0.1 Prior MAP + 0.9 Prior no informativa (lo infla pero por poco)

4) Robust Mixture Prior: 0.2 Prior MAP + 0.8 Prior no informativa (lo infla más)

```{r `Final models: Bayes - Robust Mixture Prior - SAP`, eval = T}

shape_parameter <- 1
sample_size <- seq(from = 350, to= 600, by = 5)
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- matrix(c(15,20.75), nrow = 1, ncol = 2, byrow = TRUE) # HR=0.723
censor <- c(0.15,0.15) # 10% drop-out
Tmax <- 38 # Parece más 37 pero que sea par facilita para después el parámetro gamma de la no informativa (1, 1/19)
alpha <- 0.02
test.type <- 1
HR_1 <- FALSE
IA <- NULL
method_IA = "OF"
n_exp_events <- 322
HR_1 <- FALSE
Plot_Power = TRUE
plot_pvalues = FALSE
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE
analysis = "bayes"
n_sim <- 3000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_freq, beta_map_freq, 1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Robust Mixture MAP Prior w = 0.1") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_mixture_gamma_sh_uniform.stan" ,verbose = F) 
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = 0.1
a0 = NULL

# sim7 <-  sim_trials(n_sim = n_sim,
#                      analysis = "bayes",
#                      sample_size = sample_size,
#                      ratio = ratio,
#                      rand_type = rand_type,
#                      Tmax = Tmax,
#                      scenarios_eff = scenarios_eff,
#                      shape_parameter = shape_parameter,
#                      censor = censor,
#                      test.type = test.type,
#                      alpha = alpha,
#                      method_IA = method_IA,
#                      IA = IA,
#                      n_exp_events = n_exp_events,
#                      HR_1 = HR_1,
#                      Plot_Power = Plot_Power,
#                      modelo = modelo,
#                      modelo_bayes_test = modelo_bayes_test,
#                      prior_type = prior_type,
#                      P_HR_data_Boundary = P_HR_data_Boundary,
#                      prior_gamma = prior_gamma,
#                      Plot_Power_scenarios = Plot_Power_scenarios,
#                      desired_HRs = desired_HRs,
#                      plot_pvalues = plot_pvalues,
#                      Plot_Control_Scenarios = Plot_Control_Scenarios,
#                      seed = seed)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim7)
# saveWorkbook(wd, "DEF_SAP_Bayes_Mixture_MAP_w_0.1_SampleSizes.xlsx", overwrite = TRUE)


sheet_names <- c("Sheet1")

# 3) Robust Mixture Prior: 0.1 Prior MAP + 0.9 Prior no informativa

SAP_Bayes_Mixture_MAP_w_0.1_SampleSizes <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/DEF_SAP_Bayes_Mixture_MAP_w_0.1_SampleSizes.xlsx", sheet = sheet_names[1])

DEF_SAP_Mixture_0.1_Power <- subset(SAP_Bayes_Mixture_MAP_w_0.1_SampleSizes, scenario == 1,
                                     select = c(scenario, sample_size, Prior, prop_significant)) %>%
  mutate(Prior = "Robust MAP Prior w=0.1")

# 4) Robust Mixture Prior: 0.2 Prior MAP + 0.8 Prior no informativa

SAP_Bayes_Mixture_MAP_w_0.2_SampleSizes <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/DEF_SAP_Bayes_Mixture_MAP_w_0.2_SampleSizes.xlsx", sheet = sheet_names[1])

DEF_SAP_Mixture_0.2_Power <- subset(SAP_Bayes_Mixture_MAP_w_0.2_SampleSizes, scenario == 1,
                                     select = c(scenario, sample_size, Prior, prop_significant)) %>%
  mutate(Prior = "Robust MAP Prior w=0.2")


```


# LECTURA RESULTADOS TAMAÑOS MUESTRALES DEL SAP

```{r `Final models: Lectura datos SAP`, eval = T}

# Combinamos los datos 

combined_data_DEF_SAP_Power <- bind_rows(DEF_SAP_Freq_Power,
                                         DEF_SAP_Weak_informative_Power,
                                         DEF_SAP_Weak_informative_Seddon_Power,
                                         DEF_SAP_Mixture_0.1_Power,
                                         DEF_SAP_Mixture_0.2_Power)

# Dibujamos el plot del poder:

desired_powers <- c(0.8, 0.85, 0.9, 0.95)
colors_for_dotted_lines <- c("purple", "blue", "green", "red") 

p1 <- ggplot(combined_data_DEF_SAP_Power, aes(x = sample_size, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +  
  geom_line(linewidth = 1) +  
  geom_hline(yintercept = desired_powers, linetype = "dotted", color = colors_for_dotted_lines, size = 1) + 
  scale_x_continuous(breaks = seq(min(combined_data_DEF_SAP_Power$sample_size)-5, max(combined_data_DEF_SAP_Power$sample_size)+5, 25)) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power for Selected Bayesian Models",
       x = "Sample Size",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 12),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        legend.margin = margin(12, 12, 12, 12),
        legend.text = element_text(size = 11),
        legend.title = element_text(size = 12, face = "bold")) +
  scale_color_discrete(name = "Bayesian Prior Distributions") +
  scale_linetype_discrete(name = "Bayesian Prior Distributions") 

rightmost_x <- max(combined_data_DEF_SAP_Power$sample_size) - 5  
for (i in 1:length(desired_powers)) {
  p1 <- p1 + annotate("text", 
                      x = rightmost_x, 
                      y = desired_powers[i], 
                      label = sprintf("%.0f%% power", desired_powers[i] * 100), 
                      hjust = 1,  
                      vjust = -0.3,  
                      size = 3.5, 
                      color = colors_for_dotted_lines[i])
}

p1




Table_Power_df <- data.frame(
  Power = c(0.75, 0.8, 0.85),
  Frequentist = c(485, 550, NA),
  `Weak Informative Prior centered at 15 months` = c(435, 510, 570),
  `Weak Informative Prior centered at 19 months (Seddon)` = c(550, NA, NA),
  `Robust MAP Prior w=0.1` = c(495, 550, NA),
  `Robust MAP Prior w=0.2` = c(495, 550, NA)
)

colnames(Table_Power_df) <- c("Power", "Frequentist", 
                              "Weak Informative Prior centered at 15 months", 
                              "Weak Informative Prior centered at 19 months (Seddon)", 
                              "Robust MAP Prior w=0.1", 
                              "Robust MAP Prior w=0.2")

# Replace NA values with "-"
Table_Power_df[is.na(Table_Power_df)] <- "-"

# Create the table
table5 <- gt(Table_Power_df) %>%
  tab_header(title = "Sample Sizes Achieving Target Powers with Different Bayesian Models") %>%
  fmt_number(
    columns = -Power,
    decimals = 0
  ) %>%
  tab_style(
    style = cell_text(weight = "bold", align = "center"),
    locations = cells_column_labels(columns = everything())
  ) %>%
  tab_options(
    column_labels.background.color = "lightsteelblue1",
    table.font.size = px(25),
    table.background.color = "white"
  ) %>%
  tab_style(
    style = cell_fill(color = "lightsteelblue1"),
    locations = cells_body(rows = seq(2, nrow(Table_Power_df), by = 2))
  ) %>%
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(1, nrow(Table_Power_df), by = 2))
  ) %>%
  tab_style(
    style = cell_text(align = "center"),
    locations = cells_body(columns = everything())
  ) %>% 
  tab_style(
    style = cell_fill(color = "lightsteelblue1"),  
    locations = cells_body(columns = "Power")
  ) %>%
  tab_style(
    style = cell_borders(sides = "bottom", color = "black", weight = px(2)),
    locations = cells_column_labels(columns = names(Table_Power_df)[names(Table_Power_df) != "Power"])
  ) %>%
  tab_style(
    style = cell_borders(sides = "right", color = "black", weight = px(2)),
    locations = cells_body(columns = "Power")
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),  
    locations = cells_body(columns = "Power")
  ) 

# Display the table
table5



```
La tabla que queda la hago manual ya no llegan casi ninguno al poder deseado, sólo al 80% excepto uno, que llega al 85%.

TABLA DE TAMAÑOS MUESTALES: 

       Freq    Weak    Weak_Seddon   MAP_10%    MAP_20%
0.75   485     435        550          495       495
 0.8   550     510         -           550       550
0.85    -      570         -            -         -

Tamaños muestrales diferentes: 435, 485, 495, 510, 550 y 570

# Datos Reales obtenidos en el ensayo

Se han evaluado los resultados finales de este estudio para hacernos una idea de los parámetros necesarios para los modelos parámetricos que se va a utilizar. Para ello es necesario tener los datos paciente a paciente para poder usar estos datos en R y poder hacer inferencias. Ya que por temas de confidencialidad no ha sido posible, lo que se ha hecho ha sido digitalizar las curvas de Kaplan-Meier para el resultado de PFS (variable principal). 

Esta digitalización se ha hecho usando las funciones propuestas por Lui et al 2021.

Link paper: https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-021-01308-8#availability-of-data-and-materials

Link shiny app: https://www.trialdesign.org/one-page-shell.html#IPDfromKM

Cuando se escriba la tesis pondré el resto de referencias y como funciona esto.

A continuación tenemos el resultado final de PFS:
  

```{r `Results using pseudo-IPD in both arms`, eval = T, echo = T}

# Leemos los datos obtenidos a través de las coordenadas de cada uno de los puntos seleccionados de las dos curvas:

trt <- read.csv("IPD_Lartruvo_survival_EXPERIMENTAL_P3.csv")
trt$arm <- "Lartruvo"
soc <- read.csv("IPD_Lartruvo_survival_CONTROL_P3.csv")
soc$arm <- "SOC"

# Juntamos los datasets para hacer la regresion de Cox
data <- rbind(trt,soc)

# Estimacion Cox (lo que se hace en la simulacion de momento)
fit <- coxph(Surv(Survival.time, Status) ~ arm, data = data)
Estimate <- c(1/exp(confint(fit))[2], 1/summary(fit)$coefficients[2], 1/exp(confint(fit))[1])

fit_soc <- survfit(Surv(Survival.time, Status) ~ 1, data = soc)
fit_trt <- survfit(Surv(Survival.time, Status) ~ 1, data = trt)

time_seq <- seq(min(data$Survival.time), max(data$Survival.time), length.out = 100)

plot(fit_soc, col = "blue", main = "Kaplan-Meier Plot with pseudo IPD", xlab = "Time", ylab = "Survival Probability", lty = 1, conf.int = FALSE)
lines(fit_trt, col = "red", lty = 1, conf.int = FALSE)

```

Podemos observar que las dos curvas son prácticamente iguales usando los pseudo-datos de paciente a paciente estimado con la digitalización. 

Así mismo, los resultados son muy similares ya que en el estudio real se obtuvo un HR en PFS de 1.047 (0.841, 1.303) con medianas de 20.37 (17.84, 22.90) y 19.75 (16.49, 23.75) para el brazo experimental y el control respectivamente. Por otro lado, usando los datos digitalizados tenemos un HR de 1.0296218 (0.8295044, 1.2780174).

# Parametric survival model applied to a pseudo IPD from the Lartruvo

Una vez que tenemos unos datos que se aproximan bastante bien a los reales, vamos a hacer un fit paramétrico. Para ello, vamos a ver que modelo se aproximan mejor a estos datos. La elección para el modelo en cuestión se va a hacer en función del AIC (cuanto más bajo el valor, mejor modelo es comparado con el resto)

```{r `Fitting of parametric models`, eval = T, echo = T}
# Se van a considerar 4 modelos: Exponencial, Weibull, Gompertz y Log-normal
exp_mod <- flexsurvreg(Surv(Survival.time, Status) ~ arm, data = data, dist = "exp")
weibull_mod <- flexsurvreg(Surv(Survival.time, Status) ~ arm, data = data, dist = "weibull")
gompertz_mod <- flexsurvreg(Surv(Survival.time, Status) ~ arm, data = data, dist = "gompertz")
lognormal_mod <- flexsurvreg(Surv(Survival.time, Status) ~ arm, data = data, dist = "lnorm")

# Comparamos los modelos
model_list <- list(Exponential = exp_mod, Weibull = weibull_mod, Gompertz = gompertz_mod, LogNormal = lognormal_mod)
#sapply(model_list, AIC)

```

Podemos ver que la distribución Weibull es la que mejor se aproxima bien.

```{r `Weibull Fit`, eval = T, echo = T}

weibull_surv <- survreg(Surv(Survival.time, Status) ~ arm, dist = "weibull", data = data)

# El shape parameter común es 1.13017  (1/Scale)
# El scale parameter para el brazo experimental es 13.03967 (exp(Intercept))
# El scale parameter para el brazo control es 6.725033 (exp(Intercept+armsoc))
# El HR es 0.516 (exp(-armsoc))

# Por último, dibujar las curvas con el pseudo-IPD y el fit paramétrico del Weibull

time_seq <- seq(min(data$Survival.time), max(data$Survival.time), length.out = 100)

# P de supervivencia usando el modelo Weibull
weibull_soc_surv <- 1 - pweibull(time_seq, shape = 1/weibull_surv$scale, scale = exp(3.29755876-0.02173282))
weibull_trt_surv <- 1 - pweibull(time_seq, shape = 1/weibull_surv$scale, scale = exp(3.29755876))

plot(fit_soc, col = "blue", main = "Kaplan-Meier Plot with Weibull", xlab = "Time", ylab = "Survival Probability", lty = 1, conf.int = FALSE)
lines(fit_trt, col = "red", lty = 1, conf.int = FALSE)
lines(time_seq, weibull_soc_surv, col = "green", lty = 2)
lines(time_seq, weibull_trt_surv, col = "black", lty = 2)


```

Ahora vamos a hacer lo mismo pero brazo a brazo para simular los datos reales y obtener los parámetros de interés para la generación de datos.

```{r `Weibull Fit arm by arm`, eval = T, echo = T}


lartruvo_data <- subset(data, arm == "Lartruvo")
soc_data <- subset(data, arm == "SOC")

fit_lartruvo_data <- coxph(Surv(Survival.time, Status) ~ 1, data = lartruvo_data)
weibull_lartruvo_data <- survreg(Surv(Survival.time, Status) ~ 1, dist = "weibull", data = lartruvo_data)

# El shape parameter común es 1.258723  (1/Scale)
# El scale parameter para el brazo experimental es 26.90414 (exp(Intercept))

fit_soc_data <- survfit(Surv(Survival.time, Status) ~ 1, data = soc_data)
weibull_soc_data <- survreg(Surv(Survival.time, Status) ~ 1, dist = "weibull", data = soc_data)

# El shape parameter común es 1.160619  (1/Scale)
# El scale parameter para el brazo experimental es 27.82225 (exp(Intercept))

fit_soc <- survfit(Surv(Survival.time, Status) ~ 1, data = soc)
fit_trt <- survfit(Surv(Survival.time, Status) ~ 1, data = trt)

time_seq <- seq(min(data$Survival.time), max(data$Survival.time), length.out = 100)

# P de supervivencia usando el modelo Weibull
weibull_lartruvo_surv <- 1 - pweibull(time_seq, shape = 1/weibull_lartruvo_data$scale, scale = 26.90414)
weibull_socc_surv <- 1 - pweibull(time_seq, shape = 1/weibull_soc_data$scale, scale = 27.82225)



# Kaplan-Meier survival data preparation for ggplot
fit_trt_df <- data.frame(time = fit_trt$time, surv = fit_trt$surv, group = "Lartruvo (Pseudo-IPD)")
fit_soc_df <- data.frame(time = fit_soc$time, surv = fit_soc$surv, group = "SoC (Pseudo-IPD)")

# Weibull survival data preparation for ggplot
weibull_lartruvo_surv_df <- data.frame(time = time_seq, surv = weibull_lartruvo_surv, group = "Lartruvo (Weibull Model)")
weibull_soc_df <- data.frame(time = time_seq, surv = weibull_socc_surv, group = "SoC (Weibull Model)")

# Combine all data
combined_surv_df <- rbind(fit_trt_df, fit_soc_df, weibull_lartruvo_surv_df, weibull_soc_df)

# Plot using ggplot2
ggplot(combined_surv_df, aes(x = time, y = surv, color = group, linetype = group)) +
  geom_line(size = 1.2) +
  labs(
    title = "Kaplan-Meier of Pseudo-IPD with Weibull Model",
    x = "Months",
    y = "Survival Probability",
    color = "Group",
    linetype = "Group"
  ) +
  scale_color_manual(values = c("blue", "red", "green", "black")) +
  scale_linetype_manual(values = c("solid", "dashed", "solid", "dashed")) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 10, face = "bold"),
    axis.text = element_text(size = 12),
    axis.title.x = element_text(size = 11),
    axis.title.y = element_text(size = 11),
    legend.position = "bottom",
    legend.text = element_text(size = 10)
  )




plot(fit_trt, col = "blue", main = "Kaplan-Meier Plot with Weibull", xlab = "Time", ylab = "Survival Probability", lty = 1, conf.int = FALSE)
lines(fit_soc, col = "red", lty = 1, conf.int = FALSE)
lines(time_seq, weibull_lartruvo_surv, col = "green", lty = 2)
lines(time_seq, weibull_socc_surv, col = "black", lty = 2)

# A ver, tengo que cambiar el shape para que se puedan generar diferentes shapes y scales. Muy a tener en cuenta, cuando tengamos un vector las comparaciones de MSE y coverage probability ya no son válidas por que le HR teórico no se puede calcular al necesitar un punto en concreto. No pasa nada, lo importante es que salga sifnificativo o no.


# Dibujo para asegurarme las líneas con sus scales y shapes de la weibull de los datos reales

weibull_survival <- function(time_seq, shape, scale) {
  return(exp(- (time_seq / scale)^shape))
}

# Parametros del soc
shape1 <- 1.160619
scale1 <- 27.82225

# Parametros del blincyto
shape2 <- 1.258723
scale2 <- 26.90414

surv1 <- weibull_survival(time_seq, shape1, scale1)
surv2 <- weibull_survival(time_seq, shape2, scale2)

# Plot
# plot(time_seq, surv1, type = "l", col = "red", xlab = "Time", ylab = "Survival Probability", 
#      main = "Weibull Survival Curves")
# lines(time_seq, surv2, col = "blue")
# legend("topright", legend = c("SoC", "Pembro"), col = c("red", "blue"), lty = 1)

# Están bien los valores
```

Para entender qué hubiera pasado si se hubieran aplicado estos modelos en el momento del diseño del estudio, es interesante analizar los datos reales obtenidos y ver cómo estos modelos se comportan con los datos obtenidos.

Para generar los datos simulados lo más realistas posibles se ha hecho lo siguiente:
  
1) Se han obtenido los datos paciente a paciente digitalizando las curvas para cada uno de los brazos
2) Se han sacado el fit para obtener los parámetros Shape y Scala de cada uno de los brazos.
3) Sólo se modifica el código de la función gen_surv_data.r común tanto para la simulación frecuentista como      Bayesiana. Hay que tener en cuenta que estoy forzando de manera artifical al poner estos parámetros dentro     de la función ya que el parámetro shape común y el parámetro de las eficacias se usan para diferentes    c     cosas y métricas, por lo que iba a ser muy jaleo modificar todo esto para ponerlo como input.
4) En el ordenador de multicore también se actualiza ya que de ahí es de donde saco los resultados finales.
5) Se obtienen los resultados como siempre, la diferencia radica en que cada uno de los dataset obtenidos por     cada simulación van a ser mucho más realistas a los obtenidos finalemente.

Por último, dado que estos son los resultados definitivos aproximando lo mejor posible a resultados reales, se han incrementado las simulaciones en cada escenario de 5.000 a 10.000 simulaciones.

Ahora tenemos un ejemplo de los nuevos parámetros que se van a considerar para la simulación frecuentista:

Tamaños muestrales diferentes: 435, 485, 495, 510, 550 y 570

```{r `Freq ejemplo con datos reales`, eval = T}

shape_parameter <- 1 # Esto está obsoleto porque he puesto el bueno en la función de gen_surv_data.r
sample_size <- c(435, 460, 485, 495, 510, 550, 570)
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- matrix(c(15,20.75), nrow = 1, ncol = 2, byrow = TRUE) # HR=0.723 # Esto esta obsoleto también, en la otra función
censor <- c(0.14, 0.12) # 12.25% en el experimental y 13.87% en el control (mirar excel ensayo2y3)
Tmax <- 32 
alpha <- 0.02
test.type <- 1
HR_1 <- FALSE
IA <- NULL
Plot_Power = FALSE
plot_pvalues = FALSE
analysis = "freq" 
n_sim <- 10000
seed <- 24

# sim9 <- sim_trials(n_sim = n_sim,
#                    analysis = "freq",
#                    sample_size = sample_size,
#                    ratio = ratio,
#                    rand_type = rand_type,
#                    Tmax = Tmax,
#                    shape_parameter = shape_parameter,
#                    scenarios_eff = scenarios_eff,
#                    censor = censor,
#                    alpha = alpha,
#                    test.type = test.type,
#                    IA = IA,
#                    method_IA = method_IA,
#                    n_exp_events = n_exp_events,
#                    HR_1 = HR_1,
#                    seed=seed,
#                    Plot_Power = Plot_Power,
#                    plot_pvalues = plot_pvalues)
# 
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim9)
# saveWorkbook(wd, "DEF_RESULTADOS_REALES_Freq_10K_SampleSizes_DEF.xlsx", overwrite = TRUE)

```

Y un ejemplo de un Bayesiano, en este caso el Weak:
  
```{r `Final models: Bayes - Weak prior Gamma(7.213475, 1) - SAP2`, eval = T}


# 1) # ESTE ES EL WEAK PRIOR


shape_parameter <- 1 # Esto está obsoleto porque he puesto el bueno en la función de gen_surv_data.r
sample_size <- c(435, 460, 485, 495, 510, 550, 570)
ratio <- c(1/2,1/2)
rand_type <- "CR" 
scenarios_eff <- matrix(c(15,20.75), nrow = 1, ncol = 2, byrow = TRUE) # HR=0.723 # Esto esta obsoleto también, en la otra función
censor <- c(0.14, 0.12) # 12.25% en el experimental y 13.87% en el control (mirar excel ensayo2y3)
Tmax <- 32 
alpha <- 0.02
test.type <- 1
IA <- NULL
method_IA <- "OF"
n_exp_events <- 322
HR_1 <- FALSE
Plot_Power = FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE
analysis = "bayes"
n_sim <- 10000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(21.64043, 1, 1, 1/19), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Weak Informative prior ~ Ga(21.64043, 1)") 
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL
a0 = NULL

# sim10 <-  sim_trials(n_sim = n_sim,
#                     analysis = "bayes",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# 
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim10)
# saveWorkbook(wd, "DEF_RESULTADOS_REALES_Bayes_Weak_10K_SampleSizes.xlsx", overwrite = TRUE)


```

# LECTURA RESULTADOS TAMAÑOS MUESTRALES DEL REAL

```{r `Final models: Lectura datos REAL`, eval = T}

sheet_names <- c("Sheet1")

# 1) Frecuentista

DEF_REAL_Freq <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/DEF_RESULTADOS_REALES_Freq_10K_SampleSizes_DEF.xlsx", sheet = sheet_names[1])
DEF_REAL_Freq_Power <- subset(DEF_REAL_Freq, scenario == 1,
                              select = c(scenario, sample_size, prop_significant)) %>%
  mutate(Prior = "Frequentist")

colnames(DEF_REAL_Freq)[colnames(DEF_REAL_Freq) == "mean_Lower_IC"] <- "mean_Lower_ICrI"
colnames(DEF_REAL_Freq)[colnames(DEF_REAL_Freq) == "mean_Upper_IC"] <- "mean_Upper_CrI"

DEF_REAL_Freq_HRs <- subset(DEF_REAL_Freq, scenario == 1,
                              select = c(scenario, sample_size, mean_HR, mean_Lower_ICrI, mean_Upper_CrI)) %>%
  mutate(Prior = "Frequentist") 

# 2) Prior poco informativa centered at median 15 months 

DEF_REAL_Weak_informative <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/DEF_RESULTADOS_REALES_Bayes_Weak_10K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_REAL_Weak_informative_Power <- subset(DEF_REAL_Weak_informative, scenario == 1,
                                          select = c(scenario, sample_size, Prior, prop_significant)) %>%
  mutate(Prior = "Weak Informative Prior centered at 15 months")

DEF_REAL_Weak_informative_HRs <- subset(DEF_REAL_Weak_informative, scenario == 1,
                              select = c(scenario, sample_size, Prior, mean_HR, mean_Upper_CrI, mean_Lower_ICrI)) %>%
  mutate(Prior = "Weak Informative Prior\ncentered at 15 months") 


# 3) Prior poco informativa centered at median 19 months (Seddon)

DEF_REAL_Weak_informative_Seddon <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/DEF_RESULTADOS_REALES_Bayes_Weak_Seddon_10K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_REAL_Weak_informative_Seddon_Power <- subset(DEF_REAL_Weak_informative_Seddon, scenario == 1,
                                      select = c(scenario, sample_size, Prior, prop_significant)) %>%
  mutate(Prior = "Weak Informative Prior centered at 19 months (Seddon)")

DEF_REAL_Weak_informative_Seddon_HRs <- subset(DEF_REAL_Weak_informative_Seddon, scenario == 1,
                              select = c(scenario, sample_size, Prior, mean_HR, mean_Upper_CrI, mean_Lower_ICrI)) %>%
  mutate(Prior = "Weak Informative Prior\ncentered at 19 months (Seddon)") 

# 3) Robust Mixture Prior: 0.1 Prior MAP + 0.9 Prior no informativa (lo infla pero por poco)

DEF_REAL_MAP_0.1 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/DEF_RESULTADOS_REALES_Bayes_MAP_0.1_10K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_REAL_MAP_0.1_Power <- subset(DEF_REAL_MAP_0.1, scenario == 1,
                                      select = c(scenario, sample_size, Prior, prop_significant)) %>%
  mutate(Prior = "Robust MAP Prior w=0.1")

DEF_REAL_MAP_0.1_HRs <- subset(DEF_REAL_MAP_0.1, scenario == 1,
                              select = c(scenario, sample_size, Prior, mean_HR, mean_Upper_CrI, mean_Lower_ICrI)) %>%
  mutate(Prior = "Robust MAP Prior w=0.1") 

# 4) Robust Mixture Prior: 0.2 Prior MAP + 0.8 Prior no informativa (lo infla más)

DEF_REAL_MAP_0.2 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival_Lartruvo_STS/Lartruvo_STS_Tablas/DEF_RESULTADOS_REALES_Bayes_MAP_0.2_10K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_REAL_MAP_0.2_Power <- subset(DEF_REAL_MAP_0.2, scenario == 1,
                                      select = c(scenario, sample_size, Prior, prop_significant)) %>%
  mutate(Prior = "Robust MAP Prior w=0.2")

DEF_REAL_MAP_0.2_HRs <- subset(DEF_REAL_MAP_0.2, scenario == 1,
                              select = c(scenario, sample_size, Prior, mean_HR, mean_Upper_CrI, mean_Lower_ICrI)) %>%
  mutate(Prior = "Robust MAP Prior w=0.2") 


# Combinamos los datos 



combined_data_DEF_REAL_Power <- bind_rows(DEF_REAL_Freq_Power,
                                          DEF_REAL_Weak_informative_Power,
                                          DEF_REAL_Weak_informative_Seddon_Power,
                                          DEF_REAL_MAP_0.1_Power,
                                          DEF_REAL_MAP_0.2_Power)


combined_data_DEF_REAL_HRs <- bind_rows(DEF_REAL_Freq_HRs,
                                          DEF_REAL_Weak_informative_HRs,
                                          DEF_REAL_Weak_informative_Seddon_HRs,
                                          DEF_REAL_MAP_0.1_HRs,
                                          DEF_REAL_MAP_0.2_HRs)

# Dibujamos el plot del poder:

desired_powers <- c(0.8, 0.85, 0.9, 0.95)
colors_for_dotted_lines <- c("purple", "blue", "green", "red") 

p1 <- ggplot(combined_data_DEF_REAL_Power, aes(x = sample_size, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +  
  geom_line(linewidth = 1) +  
  geom_hline(yintercept = desired_powers, linetype = "dotted", color = colors_for_dotted_lines, size = 1) + 
  scale_x_continuous(breaks = seq(min(combined_data_DEF_REAL_Power$sample_size)-5, max(combined_data_DEF_REAL_Power$sample_size)+5, 25)) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs. Sample Size",
       x = "Sample Size",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

rightmost_x <- max(combined_data_DEF_REAL_Power$sample_size) - 5  

for (i in 1:length(desired_powers)) {
  p1 <- p1 + annotate("text", 
                      x = rightmost_x, 
                      y = desired_powers[i], 
                      label = sprintf("%.0f%% power", desired_powers[i] * 100), 
                      hjust = 1,  
                      vjust = -0.3,  
                      size = 3.5, 
                      color = colors_for_dotted_lines[i])
}

p1

#Ahora hacemos una tabla para identificar cuál alcanza el poder antes.

combined_data_DEF_REAL_Power <- combined_data_DEF_REAL_Power %>%
  mutate(sample_size = as.integer(sample_size)) %>%
  select(-scenario)

Table_Power <- combined_data_DEF_REAL_Power %>%
  pivot_wider(
    names_from = Prior,
    values_from = prop_significant,
    id_cols = sample_size,
    values_fill = list(prop_significant = NA))
  # ) %>%
  # slice(-1:-2)  # Quito los valores de 75 y 90 porque ya no voy a considerar el MAP para los modelos finales

Table_Power_df <- as.data.frame(Table_Power)

library(gt)

table6 <- gt(Table_Power_df) %>%
  #tab_header(title = "Proportion Significant by Sample Size and Prior Type") %>%
  cols_label(
    sample_size = "Sample Size" 
  ) %>%
  fmt_number(
    columns = c(-sample_size),  
    decimals = 4  
  ) %>%
  tab_style(
    style = cell_text(weight = "bold", align = "center"),
    locations = cells_column_labels(columns = everything())
  ) %>%
  tab_options(
    column_labels.background.color = "lightsteelblue1",
    table.font.size = px(25),
    table.background.color = "white"
  ) %>%
  tab_style(
    style = cell_fill(color = "lightsteelblue1"),
    locations = cells_body(rows = seq(2, nrow(Table_Power_df), by = 2))
  ) %>%
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(1, nrow(Table_Power_df), by = 2))
  ) %>%
  tab_style(
    style = cell_text(align = "center"),
    locations = cells_body(columns = everything())
  ) %>%
  tab_style(
    style = cell_fill(color = "lightsteelblue1"),  
    locations = cells_body(columns = vars(sample_size))
  ) %>%
  tab_style(
    style = cell_borders(sides = "bottom", color = "black", weight = px(2)),
    locations = cells_column_labels(columns = names(Table_Power_df)[names(Table_Power_df) != "sample_size"])
  ) %>%
  tab_style(
    style = cell_borders(sides = "right", color = "black", weight = px(2)),
    locations = cells_body(columns = vars(sample_size))
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),  
    locations = cells_body(columns = vars(sample_size))
  ) 

table6



```

```{r `Final models: Lectura datos REAL2`, eval = T}

library(highcharter)
library(dplyr)

custom_prior_order <- c(
  "Frequentist",
  "Weak Informative Prior\ncentered at 15 months",
  "Weak Informative Prior\ncentered at 19 months (Seddon)",
  "Robust MAP Prior w=0.1",
  "Robust MAP Prior w=0.2"
)

custom_colors <- c(
  "#e31a1c",  # Frequentist
  "#1f78b4",  # Weak Informative Prior centered at 4.2 months
  "#b15928",  # Robust MAP Prior w=0.1
  "#33a02c",  # Robust MAP Prior w=0.2
  "#ff7f00"   # Robust MAP Prior w=0.3
)

combined_data_DEF_REAL_HRs <- combined_data_DEF_REAL_HRs %>%
  mutate(
    sample_size = as.factor(sample_size),  
    Prior = factor(Prior, levels = custom_prior_order)  
  )

combined_data_DEF_REAL_HRs2 <- combined_data_DEF_REAL_HRs %>%
  mutate(
    mean_HR = as.numeric(mean_HR),
    mean_Lower_ICrI = as.numeric(mean_Lower_ICrI),
    mean_Upper_CrI = as.numeric(mean_Upper_CrI),
    sample_size = as.character(sample_size)  
  )

p <- hchart(
  combined_data_DEF_REAL_HRs2,
  type = "columnrange",  
  hcaes(
    x = sample_size,      
    low = mean_Lower_ICrI,  
    high = mean_Upper_CrI,  
    group = Prior           
  )
) %>%
  hc_chart(polar = TRUE) %>%  
  hc_colors(custom_colors) %>%  
  hc_yAxis(
    max = 1.4,    
    min = 0.75,     
    labels = list(
      formatter = JS("function() { return 'HR = ' + this.value; }"),  
      style = list(fontSize = "13px", fontWeight = "bold")
    ),
    title = NULL  
  ) %>%
  hc_xAxis(
    title = NULL,  
    categories = levels(combined_data_DEF_REAL_HRs2$sample_size),  
    labels = list(format = "{value}", style = list(fontSize = "14px", fontWeight = "bold")),
    gridLineWidth = 0.5
  ) %>%
  hc_exporting(enabled = TRUE, buttons = list(contextButton = list(enabled = FALSE))) %>%
  hc_add_theme(hc_theme_elementary()) 

p <- p %>% 
  hc_legend(
    align = "center",
    verticalAlign = "bottom",
    layout = "horizontal",
    title = list(
      text = "Bayesian Prior Distributions",
      style = list(fontSize = "16px", fontWeight = "bold")  
    ),
    itemStyle = list(fontSize = "13px")  
  )

p


```
