---
title: "Evaluation of the KEYNOTE-024 trial for the Pembrolizumab (Keytruda) against Placebo in NSCLC"
author: "Borja García López-Rey"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    code_folding: hide
vignette: >
  %\VignetteIndexEntry{Model checking with simulated data (survival model example)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, eval = T, results = 'hide', echo = F}

knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      fig.width = 9,  
                      fig.height = 6   
                      )
```

```{r load-packages, eval = T, echo = F}
library(ggplot2)
library(survival)
library(tidyr)
library(gsDesign)
library(dplyr)
library(stringr)
library(tibble)
# librerias especificas para bayes
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = min(8, parallel::detectCores()))
library(loo)
library(openxlsx)
library(readxl)
options(digits=7)
library(flexsurv)
library(stringr)
library(SurvRegCensCov)
library(metafor)
library(knitr)
library(flexmix)
library(brms)
library(meta)
library(dmetar)
library(tidybayes)
library(ggdist)
library(forcats)
library(ggridges)
library(glue)
library(stats4)
library(kableExtra)
library(gt)

```

```{r load-functions, eval = T, echo = F}

source("sim_trials.r")
source("sim_freq.r")
source("sim_bayes.r")
source("surv_analysis_freq.r")
source("surv_analysis_bayes.r")
source("rand.r")
source("gen_surv_data.r")
source("get_boundaries_IA.r")
source("analysis_surv_data_freq.r")
source("analysis_surv_data_bayes.r")
source("plot_bayes.r")
source("plot_bayes_sep.r")

# Mirar los modelos STAN estos que son básicos para el cox

#https://github.com/bayesianops/stan-survival-model-workshop/blob/main/README.md

```

# Leer

https://statmodeling.stat.columbia.edu/2023/12/23/bayesians-moving-from-defense-to-offense-i-really-think-its-kind-of-irresponsible-now-not-to-use-the-information-from-all-those-thousands-of-medical-trials-that-came-before-is-that-very/

# Keynote024

Aquí se van a explicar las características de este ensayo tipo un resumen del summary para contextualizar el ensayo que se va a analizar.

# Puesta a punto

Antes de empezar con el análisis principal, se han evaluado los resultados finales de este estudio para hacernos una idea de los parámetros necesarios para los modelos parámetricos que se va a utilizar. Para ello es necesario tener los datos paciente a paciente para poder usar estos datos en R y poder hacer inferencias. Ya que por temas de confidencialidad no ha sido posible, lo que se ha hecho ha sido digitalizar las curvas de Kaplan-Meier para el resultado de PFS (variable principal). 

Esta digitalización se ha hecho usando las funciones propuestas por Lui et al 2021.

Link paper: https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-021-01308-8#availability-of-data-and-materials

Link shiny app: https://www.trialdesign.org/one-page-shell.html#IPDfromKM

Cuando se escriba la tesis pondré el resto de referencias y como funciona esto.

A continuación tenemos el resultado final de PFS:

```{r `Final results on PFS`, eval = T, echo = F}

knitr::include_graphics("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_GRAFICOS/Keynote024_PFS.jpg")

```
```{r `Results using pseudo-IPD in both arms`, eval = T, echo = F}

# Leemos los datos obtenidos a través de las coordenadas de cada uno de los puntos seleccionados de las dos curvas:

trt <- read.csv("IPD_KEYNOTE026_NSCLC_PFS_Pembro.csv")
trt$arm <- "Pembro"
soc <- read.csv("IPD_KEYNOTE026_NSCLC_PFS_SOC.csv")
soc$arm <- "soc"

# Juntamos los datasets para hacer la regresion de Cox
data <- rbind(trt,soc)

# Estimacion Cox (lo que se hace en la simulacion de momento)
fit <- coxph(Surv(time, status) ~ arm, data = data)
Estimate <- c(1/exp(confint(fit))[2], 1/summary(fit)$coefficients[2], 1/exp(confint(fit))[1])

fit_soc <- survfit(Surv(time, status) ~ 1, data = soc)
fit_trt <- survfit(Surv(time, status) ~ 1, data = trt)

time_seq <- seq(min(data$time), max(data$time), length.out = 100)

plot(fit_soc, col = "blue", main = "Kaplan-Meier Plot with pseudo IPD", xlab = "Time", ylab = "Survival Probability", lty = 1, conf.int = FALSE)
lines(fit_trt, col = "red", lty = 1, conf.int = FALSE)

```
Podemos observar que las dos curvas son prácticamente iguales usando los pseudo-datos de paciente a paciente estimado con la digitalización. Así mismo, los resultados son muy similares ya que en el estudio real se obtuvo un HR en PFS de 0.5 (0.37, 0.68) con medianas de 10.3 (6.7, -) y 6 (4.2, 6.2) para el brazo experimental y el control respectivamente. Por otro lado, usando los datos digitalizados tenemos un HR de 0.489 (0.363, 0.657) y medianas de 10.335 (6.957, -) y 6.047 (4.207, 6.310).

# First analysis: Parametric survival model applied to a pseudo IPD from the Keynote024

Una vez que tenemos unos datos que se aproximan bastante bien a los reales, vamos a hacer un fit paramétrico. Para ello, vamos a ver que modelo se aproximan mejor a estos datos. La elección para el modelo en cuestión se va a hacer en función del AIC (cuanto más bajo el valor, mejor modelo es comparado con el resto)

```{r `Fitting of parametric models`, eval = T, echo = F}
# Se van a considerar 4 modelos: Exponencial, Weibull, Gompertz y Log-normal
exp_mod <- flexsurvreg(Surv(time, status) ~ arm, data = data, dist = "exp")
weibull_mod <- flexsurvreg(Surv(time, status) ~ arm, data = data, dist = "weibull")
gompertz_mod <- flexsurvreg(Surv(time, status) ~ arm, data = data, dist = "gompertz")
lognormal_mod <- flexsurvreg(Surv(time, status) ~ arm, data = data, dist = "lnorm")

# Comparamos los modelos
model_list <- list(Exponential = exp_mod, Weibull = weibull_mod, Gompertz = gompertz_mod, LogNormal = lognormal_mod)
sapply(model_list, AIC)

```
Podemos ver que la distribución Weibull es la que mejor se aproxima bien.

```{r `Weibull Fit`, eval = T, echo = F}

weibull_surv <- survreg(Surv(time, status) ~ arm, dist = "weibull", data = data)

# El shape parameter común es 1.13017  (1/Scale)
# El scale parameter para el brazo experimental es 13.03967 (exp(Intercept))
# El scale parameter para el brazo control es 6.725033 (exp(Intercept+armsoc))
# El HR es 0.516 (exp(-armsoc))

# Por último, dibujar las curvas con el pseudo-IPD y el fit paramétrico del Weibull

time_seq <- seq(min(data$time), max(data$time), length.out = 100)

# P de supervivencia usando el modelo Weibull
weibull_soc_surv <- 1 - pweibull(time_seq, shape = 1/weibull_surv$scale, scale = exp(2.5679960-0.6621592))
weibull_trt_surv <- 1 - pweibull(time_seq, shape = 1/weibull_surv$scale, scale = exp(2.5679960))

plot(fit_soc, col = "blue", main = "Kaplan-Meier Plot with Weibull", xlab = "Time", ylab = "Survival Probability", lty = 1, conf.int = FALSE)
lines(fit_trt, col = "red", lty = 1, conf.int = FALSE)
lines(time_seq, weibull_soc_surv, col = "green", lty = 2)
lines(time_seq, weibull_trt_surv, col = "black", lty = 2)



```

Ahora vamos a hacer lo mismo pero brazo a brazo para simular los datos reales y obtener los parámetros de interés para la generación de datos.

```{r `Weibull Fit arm by arm`, eval = T, echo = F}


pembro_data <- subset(data, arm == "Pembro")
soc_data <- subset(data, arm == "soc")

fit_pembro_data <- coxph(Surv(time, status) ~ 1, data = pembro_data)
weibull_pembro_data <- survreg(Surv(time, status) ~ 1, dist = "weibull", data = pembro_data)

# El shape parameter común es 0.8690831  (1/Scale)
# El scale parameter para el brazo experimental es 14.88501 (exp(Intercept))

fit_soc_data <- survfit(Surv(time, status) ~ 1, data = soc_data)
weibull_soc_data <- survreg(Surv(time, status) ~ 1, dist = "weibull", data = soc_data)

# El shape parameter común es 1.369615  (1/Scale)
# El scale parameter para el brazo experimental es 6.761968 (exp(Intercept))

fit_soc <- survfit(Surv(time, status) ~ 1, data = soc)
fit_trt <- survfit(Surv(time, status) ~ 1, data = trt)

time_seq <- seq(min(data$time), max(data$time), length.out = 100)

# P de supervivencia usando el modelo Weibull
weibull_pembro_surv <- 1 - pweibull(time_seq, shape = 1/weibull_pembro_data$scale, scale = 14.88501)
weibull_socc_surv <- 1 - pweibull(time_seq, shape = 1/weibull_soc_data$scale, scale = 6.761968)

plot(fit_trt, col = "blue", main = "Kaplan-Meier Plot with Weibull", xlab = "Time", ylab = "Survival Probability", lty = 1, conf.int = FALSE)
lines(fit_soc, col = "red", lty = 1, conf.int = FALSE)
lines(time_seq, weibull_pembro_surv, col = "green", lty = 2)
lines(time_seq, weibull_socc_surv, col = "black", lty = 2)

# A ver, tengo que cambiar el shape para que se puedan generar diferentes shapes y scales. Muy a tener en cuenta, cuando tengamos un vector las comparaciones de MSE y coverage probability ya no son válidas por que le HR teórico no se puede calcular al necesitar un punto en concreto. No pasa nada, lo importante es que salga sifnificativo o no.


# Dibujo para asegurarme las líneas con sus scales y shapes de la weibull de los datos reales

weibull_survival <- function(time_seq, shape, scale) {
  return(exp(- (time_seq / scale)^shape))
}

# Parametros del soc
shape1 <- 1.369615
scale1 <- 6.761968

# Parametros del pembro
shape2 <- 0.8690831
scale2 <- 14.88501

surv1 <- weibull_survival(time_seq, shape1, scale1)
surv2 <- weibull_survival(time_seq, shape2, scale2)

# Plot
plot(time_seq, surv1, type = "l", col = "red", xlab = "Time", ylab = "Survival Probability", 
     main = "Weibull Survival Curves")
lines(time_seq, surv2, col = "blue")
legend("topright", legend = c("SoC", "Pembro"), col = c("red", "blue"), lty = 1)

# Están bien los valores
```

## Frequentist simulation

Ahora vamos a usar las funciones creadas (no se va a profundizar en absoluto aquí en qué consisten porque esto es para la presentación de resultados) para simular datos usando ciertos parámetros para simular las características operantes desde un punto de vista frecuentista. 

Lo primero vamos a considerar los parámetros que ha usado la compañía en el SAP para el diseño de este estudio. Cabe destacar que todo el ejercicio de simulación tanto para la generación de datos simulados como para el análisis bayesiano se va a usar la distribución Weibull.

1) Shape parameter = 1. La Cy usa una exponencial para el diseño por lo que el parámetro es =1. Una Weibull con shape = 1 es una exponencial.
2) Sample size = 300.
3) ratio = 1:1.
4) rand_type = CR. Esto es la manera que se asigna grupo a cada paciente, esto es complete randomization.
5) Tmax = 20. En el SAP se dice esto: "an enrollment period of 14 months and at least 6 months PFS follow-up after enrollment completion"
6) Censor = 0.1/0.1. Se han escogido estos valores por lo que se dice en el SAP "a dropout rate of 10% per year". Son casi 2 años por lo que sería en verdad 0.08335 cada brazo pero para simplificar lo dejo así.
7) Alpha = 0.05.
8) test.type = 2. Esto es que el test es de dos colas.
9) seed = 24. A lo largo de este estudio en casi todas las simulaciones se usará esta semilla.

Con respecto a los efectos esperados, en el SAP está lo siguiente: 

"The planned PFS analysis will be conducted after approximately 175 PFS events are
observed between the MK-3475 arm and control. The study has ~98% power to detect a
HR of 0.55 at alpha = 2.5% (one-sided) at the final PFS analysis. A p-value less than
2.5% (one-sided) for PFS approximately corresponds to an empirical hazard ratio of < 0.744
(or approximately at least 7.4 months of median PFS in MK-3475 vs. 5.5 months of median
PFS in SOC).
The sample size calculation was based on PFS with the following assumptions: 1) PFS follows an exponential distribution with a median of 5.5 months in the control arm, 2) hazard ratio between pembrolizumab and control is 0.55, 3) enrollment period of 14 months and at least 6 months PFS follow-up after enrollment completion, and 4) a dropout rate of 10% per year."

A modo de introducción para la herramiento de simulación que he hecho, vamos a usar los parámetros considerados en el SAP, incluyendo las asunciones de que HR=0.55 y las medianas para el brazo control y experimental son 5.5 y 10 meses respectivamente.

```{r `Simulation of the trial with the parameters from the protocol`, eval = T, echo = F}

#shape_parameter <- c(1.369615, 0.8690831)
shape_parameter <- 1

sample_size <- 300
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- matrix(c(5.5,10), nrow = 1, ncol = 2, byrow = TRUE) # HR=0.55
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE
IA <- NULL
Plot_Power = FALSE 
plot_pvalues = FALSE
analysis = "freq" 
n_sim <- 10000
seed <- 24

 sim1 <- sim_trials(n_sim = n_sim,
                    analysis = "freq",
                    sample_size = sample_size,
                    ratio = ratio,
                    rand_type = rand_type,
                    Tmax = Tmax,
                    shape_parameter = shape_parameter,
                    scenarios_eff = scenarios_eff,
                    censor = censor,
                    alpha = alpha,
                    test.type = test.type,
                    IA = IA,
                    method_IA = method_IA,
                    n_exp_events = n_exp_events,
                    HR_1 = HR_1,
                    seed=seed,
                    Plot_Power = Plot_Power,
                    plot_pvalues = plot_pvalues)

pvalues1 <- sim1[[2]]
sim11 <- sim1[[1]]

# Aquí hemos obtenido los siguientes valores para las 10K simulaciones de ensayos clínicos con estas características:
# sim11

```

Como podemos ver, la media del HR es 0.5532 (0.42, 0.73) y el % de estudios que han salido significativos es el 99% y donde el 95% de las veces el valor verdadero del HR (0.55) está contenido en los intervalos de confianza (Coverage Probability). 

También se ha obtenido otra tabla donde se incorporan todos los p-valores de los estudios, la utilidad de esto se verá más adelante.

# Evaluación del tamaño muestral con un efecto dado

Lo siguiente es evaluar mediante simulaciones como varían las características operantes en función de diferentes tamaños muestrales (así también se confirmará que 300 pacientes tiene un poder aprox del 98%).

Nota: A partir de ahora cuando debajo de la función principal para simular aparezcan unas líneas para leer un Excel es porque las simulaciones de mucha carga computacional se han hecho con el ordenador de Ferran. 

```{r `Simulation of the trial with the parameters from the SAP w different sample size`, eval = T, echo = F}

shape_parameter <- 1
sample_size <- seq(from = 50, to= 350, by = 5)
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- matrix(c(5.5,10), nrow = 1, ncol = 2, byrow = TRUE) # HR=0.55
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- TRUE
IA <- NULL
Plot_Power = TRUE
plot_pvalues = FALSE
analysis = "freq" 
n_sim <- 10000
seed <- 24

# sim2 <- sim_trials(n_sim = n_sim, 
#                    analysis = "freq",
#                    sample_size = sample_size,
#                    ratio = ratio,
#                    rand_type = rand_type,
#                    Tmax = Tmax, 
#                    shape_parameter = shape_parameter,
#                    scenarios_eff = scenarios_eff,
#                    censor = censor,
#                    alpha = alpha,
#                    test.type = test.type,
#                    IA = IA,
#                    method_IA = method_IA,
#                    n_exp_events = n_exp_events,
#                    HR_1 = HR_1,
#                    seed=seed,
#                    Plot_Power = Plot_Power,
#                    plot_pvalues = plot_pvalues)
# 
# sim2_power <- sim2[[2]]
# sim2_power_T1E <- sim2[[3]]
# sim2_power_MSE <- sim2[[4]]
# sim2_results <- sim2[[1]]

# Aquí los datos se leen del Excel con los datos simulados en el ordenador de Ferran.

sheet_names <- c("Sheet1")
sim2_results <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/Power_samplesize_SAP_Parameters_10K_each.xlsx", sheet = sheet_names[1])

# Dibujamos el plot del poder:

data <- subset(sim2_results, scenario == 1)

desired_powers <- c(0.8, 0.9)
sample_sizes_for_desired_powers <- sapply(desired_powers, function(x) 
  min(data$sample_size[data$prop_significant >= x]))

p1 <- ggplot(data, aes(x = sample_size, y = prop_significant)) +
  geom_point(size = 3, color = "steelblue") +  
  geom_line(linewidth = 1, color = "steelblue") +  
  geom_hline(yintercept = desired_powers, linetype = "dotted", color = c("purple", "green"), size = 1) + 
  scale_x_continuous(breaks = seq(min(data$sample_size)-5, max(data$sample_size)+5, 25)) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs. Sample Size",
       x = "Sample Size",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

colors_for_annotation <- c("purple", "green")  
leftmost_x <- min(data$sample_size) + 5  
for (i in 1:length(desired_powers)) {
  p1 <- p1 + annotate("text", 
                      x = leftmost_x, 
                      y = desired_powers[i], 
                      label = sprintf("%.0f%% power", desired_powers[i] * 100), 
                      hjust = 0,  
                      vjust = -0.3,  
                      size = 3.5, 
                      color = colors_for_annotation[i])
}

p1

```

Como se puede ver, efectivamente con 300 pacientes se tiene un poder por encima del 98% (98.87%). También se identifica que con estos datos simulados, 130 pacientes son suficientes para obtener un poder del 80%, 150 pacientes para tener un poder del 85% y 175 pacientes para un poder del 90%.

La compañía fue bastante conservadora a la hora de diseñar este estudio aunque el tamaño muestral se puede deducir que se diseñó para OS por lo que necesitan más eventos que para PFS.

Ahora miramos los datos para el poder junto al Error de Tipo I:

```{r `Simulation of sample sizes: T1E`, eval = T, echo = F}

df <- data.frame(
      sample_size = sim2_results$sample_size,
      HR = sim2_results$scenario,
      power_and_T1E = sim2_results$prop_significant)
    
    # Nueva columna para indicar a partir de cuándo se hace el zoom para el ET1
    
    df$zoom <- ifelse(df$power_and_T1E <= 0.1, "Zoomed", "Regular")
    
    suppressWarnings(
      p2 <- ggplot(data = df) +
      geom_line(aes(x = sample_size, y = power_and_T1E, color = as.factor(HR))) +
      labs(title = "Power and Type I Error by Sample Size",
           x = "Sample Size",
           y = "%",
           color = "scenarios") +
      scale_color_discrete(labels = c("HR = 0.55", "HR = 1")) +
      scale_linetype_discrete(labels = c("Power", "Type I Error")) +
      theme_minimal() +
      facet_grid(zoom ~ ., scales = "free_y") + 
      geom_hline(yintercept = 0.1, linetype = "dashed", color = "black", data = subset(df, zoom == "Zoomed"))
    )
    p2
    

```

Así mismo podemos ver que el error de tipo I se mantiene por debajo del valor especificado (5%, 2-sided) por lo que se demuestra que no se infla el ET1 (como era de esperar en un estudio frecuentista si está bien hecho).

Por último, se puede ver valores del MSE como varían en valores más pequeños según se aumenta el tamaño muestral.

```{r `Simulation sample sizes: MSE`, eval = T, echo = F}

p4 <- ggplot(data, aes(x = sample_size, y = MSE)) +
      geom_point() +
      geom_line() +  labs(title = "MSE vs. Sample Size",
                          x = "Sample Size",
                          y = "MSE")
p4
```
# Evaluación de diferentes de efecto 

Ahora vamos a evaluar cómo afectan los compartamientos del efecto para cada uno de los brazos a un tamaño muestral dado. No sólo se va a hacer con los 300 pacientes planeados, si no también para X, Y y Z.

Ya que el tamaño del efecto depende de dos variables diferentes, efecto del brazo control y efecto del brazo experimental, tenemos que ver los dos escenarios de interés en base a un factor común, el HR.

Para ello, primero vamos a evaluar los diferentes escenarios en términos de medianas. Vamos a dejar fijo primero la mediana para el brazo control como está definido en el SAP (5.5) y luego dejamos fija la mediana para el brazo experimental (10). De esto modo, vemos cómo evoluciona todo el rango de Hazard Ratios relevantes a lo largo de los diferentes escenarios.

```{r `Medians w control arm at 5.5` , eval = T, echo = F}

shape_parameter <- 1

desired_HRs <- c(1.5, 1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(5.5, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}
medians_control_fijo

```

Ahora dejando fija la mediana del brazo experimental a 10.

```{r `Medians w experimental arm at 10` , eval = T, echo = F}

shape_parameter <- 1

desired_HRs <- c(1.5, 1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
medians_experimiental_fijo <- rep(10, times = length(desired_HRs))

treatment_medians <- matrix(NA, nrow = length(desired_HRs), ncol = 2, 
                  dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  treatment_medians[i, "Control"] <- medians_experimiental_fijo[i] * (desired_HRs[i]^(1/shape_parameter))
  treatment_medians[i, "Treatment"] <- medians_experimiental_fijo[i]
}

treatment_medians
```
Ahora vamos a simular ensayos clínicos yendo de un rango a otro tanto dejando la mediana del control fija como después la del tratamiento para los siguientes tamaños muestrales.

1) 300 -> Poder: 99%
2) 175 -> Poder: 90%
3) 150 -> Poder: 85%
4) 130 -> Poder: 80%

```{r `Simulation scenarios w fixed control`, eval = T, echo = F}
shape_parameter <- 1

desired_HRs <- c(1.5, 1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(5.5, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}
medians_control_fijo

shape_parameter <- 1
sample_size <- 300
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = length(medians_control_fijo), ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE
IA <- NULL
Plot_Power = FALSE
plot_pvalues = FALSE
Plot_Power_scenarios = FALSE
analysis = "freq" 
n_sim <- 10000
seed <- 24

# sim3 <- sim_trials(n_sim = n_sim, 
#                    analysis = "freq",
#                    sample_size = sample_size,
#                    ratio = ratio,
#                    rand_type = rand_type,
#                    Tmax = Tmax, 
#                    shape_parameter = shape_parameter,
#                    scenarios_eff = scenarios_eff,
#                    censor = censor,
#                    alpha = alpha,
#                    test.type = test.type,
#                    IA = IA,
#                    method_IA = method_IA,
#                    n_exp_events = n_exp_events,
#                    HR_1 = HR_1,
#                    seed=seed,
#                    Plot_Power = Plot_Power,
#                    plot_pvalues = plot_pvalues)

sheet_names <- c("Sheet1")
sim3_results <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/power_esc_10K_300.xlsx", sheet = sheet_names[1])

 data <- sim3_results %>%
      group_by(scenario) %>%
      summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                mean_HR = mean(mean_HR), mean_MSE = mean(MSE))

    data$desired_HRs <- desired_HRs[1:nrow(data)]

    p3 <- ggplot(data, aes(x = desired_HRs, y = prop_significant)) +
      geom_point(size = 3, color = "steelblue") +
      geom_line(linewidth = 1, color = "steelblue") +
      geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
      geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
      scale_x_continuous(breaks = seq(min(desired_HRs), max(desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(desired_HRs), max(desired_HRs), 0.1))) +
      scale_y_continuous(labels = scales::percent) +
      labs(title = "Power vs Hazard Ratio",
           x = "Theoretical Hazard Ratio",
           y = "Power") +
      theme_minimal() +
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12))

    # Add labels for 80% and 90% power
    p3 <- p3 +
      annotate("text", x = max(desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
      annotate("text", x = max(desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green")

    # Add small labels to indicate the corresponding mean_HR
    for (i in 1:nrow(data)) {
      p3 <- p3 + annotate("text", x = data$desired_HRs[i], y = data$prop_significant[i],
                          label = sprintf("%.2f", data$mean_HR[i]), hjust = 1, vjust = 1.5, size = 4, color = "black")
    }

p3

```

Y ahora dejando el tratamiento fijo (sale igual por lo que solo lo haremos con el control fijo a partir de ahora).

1) 300 -> Poder: 99%

Con el tamaño muestral de 300 es necesario:

1) Un HR de 0.66 para tener un poder del 80%
2) Un HR de 0.65 para tener un poder del 85%
3) Un HR de 0.60 para tener un poder del 90%

```{r `Simulation scenarios w fixed experimental`, eval = T, echo = F}

sheet_names <- c("Sheet1")
sim3_results <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/power_esc_10K_300_experimentalfijo.xlsx", sheet = sheet_names[1])

 data <- sim3_results %>%
      group_by(scenario) %>%
      summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                mean_HR = mean(mean_HR), mean_MSE = mean(MSE))

    # Add desired_HRs vector to the aggregated data
    data$desired_HRs <- desired_HRs[1:nrow(data)]

    # Plot power for different scenarios of treatment with desired_HRs on the x-axis
    p4 <- ggplot(data, aes(x = desired_HRs, y = prop_significant)) +
      geom_point(size = 3, color = "steelblue") +
      geom_line(linewidth = 1, color = "steelblue") +
      geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
      geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
      scale_x_continuous(breaks = seq(min(desired_HRs), max(desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(desired_HRs), max(desired_HRs), 0.1))) +
      scale_y_continuous(labels = scales::percent) +
      labs(title = "Power vs Hazard Ratio",
           x = "Theoretical Hazard Ratio",
           y = "Power") +
      theme_minimal() +
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12))

    # Add labels for 80% and 90% power
    p4 <- p4 +
      annotate("text", x = max(desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
      annotate("text", x = max(desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green")

    # Add small labels to indicate the corresponding mean_HR
    for (i in 1:nrow(data)) {
      p4 <- p4 + annotate("text", x = data$desired_HRs[i], y = data$prop_significant[i],
                          label = sprintf("%.3f", data$mean_HR[i]), hjust = 1, vjust = 1.5, size = 3, color = "black")
    }
p4
```


Vamos a ver los gráficos variando el tamaño muestral para el poder que antes se especificó:

1) 175 -> Poder con 300 pacientes: 90%

Con este tamaño muestral es necesario:

1) Un HR de 0.61 para tener un poder del 80%
2) Un HR de 0.58 para tener un poder del 85%
3) Un HR de 0.55 para tener un poder del 90%

```{r `Simulation scenarios w SS = 175`, eval = T, echo = F}

sheet_names <- c("Sheet1")
sim3_results <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/power_esc_10K_175.xlsx", sheet = sheet_names[1])

 data <- sim3_results %>%
      group_by(scenario) %>%
      summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                mean_HR = mean(mean_HR), mean_MSE = mean(MSE))

    # Add desired_HRs vector to the aggregated data
    data$desired_HRs <- desired_HRs[1:nrow(data)]

    # Plot power for different scenarios of treatment with desired_HRs on the x-axis
    p4 <- ggplot(data, aes(x = desired_HRs, y = prop_significant)) +
      geom_point(size = 3, color = "steelblue") +
      geom_line(linewidth = 1, color = "steelblue") +
      geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
      geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
      scale_x_continuous(breaks = seq(min(desired_HRs), max(desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(desired_HRs), max(desired_HRs), 0.1))) +
      scale_y_continuous(labels = scales::percent) +
      labs(title = "Power vs Hazard Ratio",
           x = "Theoretical Hazard Ratio",
           y = "Power") +
      theme_minimal() +
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12))

    # Add labels for 80% and 90% power
    p4 <- p4 +
      annotate("text", x = max(desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
      annotate("text", x = max(desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green")

    # Add small labels to indicate the corresponding mean_HR
    for (i in 1:nrow(data)) {
      p4 <- p4 + annotate("text", x = data$desired_HRs[i], y = data$prop_significant[i],
                          label = sprintf("%.3f", data$mean_HR[i]), hjust = 1, vjust = 1.5, size = 3, color = "black")
    }
p4


```

2) 150 -> Poder con 300 pacientes: 85%

Con este tamaño muestral es necesario:

1) Un HR de 0.58 para tener un poder del 80%
2) Un HR de 0.56 para tener un poder del 85%
3) Un HR de 0.53 para tener un poder del 90%

```{r `Simulation scenarios w SS = 150` , eval = T, echo = F}

sheet_names <- c("Sheet1")
sim3_results <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/power_esc_10K_150.xlsx", sheet = sheet_names[1])

 data <- sim3_results %>%
      group_by(scenario) %>%
      summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                mean_HR = mean(mean_HR), mean_MSE = mean(MSE))

    # Add desired_HRs vector to the aggregated data
    data$desired_HRs <- desired_HRs[1:nrow(data)]

    # Plot power for different scenarios of treatment with desired_HRs on the x-axis
    p4 <- ggplot(data, aes(x = desired_HRs, y = prop_significant)) +
      geom_point(size = 3, color = "steelblue") +
      geom_line(linewidth = 1, color = "steelblue") +
      geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
      geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
      scale_x_continuous(breaks = seq(min(desired_HRs), max(desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(desired_HRs), max(desired_HRs), 0.1))) +
      scale_y_continuous(labels = scales::percent) +
      labs(title = "Power vs Hazard Ratio",
           x = "Theoretical Hazard Ratio",
           y = "Power") +
      theme_minimal() +
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12))

    # Add labels for 80% and 90% power
    p4 <- p4 +
      annotate("text", x = max(desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
      annotate("text", x = max(desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green")

    # Add small labels to indicate the corresponding mean_HR
    for (i in 1:nrow(data)) {
      p4 <- p4 + annotate("text", x = data$desired_HRs[i], y = data$prop_significant[i],
                          label = sprintf("%.3f", data$mean_HR[i]), hjust = 1, vjust = 1.5, size = 3, color = "black")
    }
p4
```

3) 130 -> Poder con 300 pacientes: 80%

Con este tamaño muestral es necesario:

1) Un HR de 0.56 para tener un poder del 80%
2) Un HR de 0.53 para tener un poder del 85%
3) Un HR de 0.5 para tener un poder del 90%

```{r `Simulation scenarios w SS = 130`, eval = T, echo = F}

sheet_names <- c("Sheet1")
sim3_results <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/power_esc_10K_130.xlsx", sheet = sheet_names[1])

 data <- sim3_results %>%
      group_by(scenario) %>%
      summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                mean_HR = mean(mean_HR), mean_MSE = mean(MSE))

    # Add desired_HRs vector to the aggregated data
    data$desired_HRs <- desired_HRs[1:nrow(data)]

    # Plot power for different scenarios of treatment with desired_HRs on the x-axis
    p4 <- ggplot(data, aes(x = desired_HRs, y = prop_significant)) +
      geom_point(size = 3, color = "steelblue") +
      geom_line(linewidth = 1, color = "steelblue") +
      geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
      geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
      scale_x_continuous(breaks = seq(min(desired_HRs), max(desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(desired_HRs), max(desired_HRs), 0.1))) +
      scale_y_continuous(labels = scales::percent) +
      labs(title = "Power vs Hazard Ratio",
           x = "Theoretical Hazard Ratio",
           y = "Power") +
      theme_minimal() +
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12))

    # Add labels for 80% and 90% power
    p4 <- p4 +
      annotate("text", x = max(desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
      annotate("text", x = max(desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green")

    # Add small labels to indicate the corresponding mean_HR
    for (i in 1:nrow(data)) {
      p4 <- p4 + annotate("text", x = data$desired_HRs[i], y = data$prop_significant[i],
                          label = sprintf("%.3f", data$mean_HR[i]), hjust = 1, vjust = 1.5, size = 3, color = "black")
    }
p4
```

Es de interés evaluar diferentes escenarios en medianas tanta de un brazo como de otro en función de HR dado. Esto será útil más adelante cuando evaluemos los Bayesianos ya que en principio, se van a comparar los modelos para diferentes tamaños muestrales y en función de diferentes escenarios de HR.

Los HR que se van a considerar son los siguientes en base al estudio anterior de cómo varía el HR necesario para conseguir un poder el 80%, 85% y 90% para los tamaños muestrales de 300, 175, 150 y 130.

1) HR: 0.5
2) HR: 0.55
3) HR: 0.6
4) HR: 0.65
5) HR: 0.70
6) HR: 0.75
7) HR: 0.8

Esto se va a calcular para evaluar las características operantes de Poder, Error de Tipo I y MSE. Los que cumplan buenas condiciones para los escenarios plausibles se calculará el tamaño muestral y el poder para seleccionar el ahorro que ofrecen con respecto al frecuentista.

```{r `Medians W HR fixed`, eval = T, echo = F}

shape_parameter <- 1 

control_medians <- c(3.5,3.75,4,4.25,4.5,4.75,5,5.25,5.5,5.75,6,6.25,6.5,6.75,7,7.25,7.5)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "Hazard Ratio")))

desired_HR = 0.55

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "Hazard Ratio"] = (lambda_control / lambda_treatment)^shape_parameter
}

medians
```
# Análisis Bayesiano

No voy a enrollarme mucho con esto aquí. Se usa para la variable de tipo hasta un modelo Weibull para la inferencia de los datos.
He cambiado el código para, en vez de calcular el tamaño efecto como variable aleatoria, ahora calculo cada uno de los brazos por separado como variables aleatorias; de este modo podré especificar prior distributions para cada uno de los brazos por separado ya que vamos a asumir siempre que el brazo tratamiento va a tener una prior no informativa.

A continuación muestro el modelo STAN que tenía como efectos conjuntos (variable aleatorio diferencia de tratamiento en función de la pendiente de la regresión):

```{r `stan-file slope`, eval = T}

cat(readLines("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/weibull_model_def.stan"), sep = "\n")

```

Y este es el modelo que uso a partir de ahora evaluando el efecto de cada uno de los brazos por separado.

Tenemos tres parámetros a estimar en el modelo.

1. Shape: Este es común para los dos brazos.
2. Scale_control: Es es el parámetro escala para el brazo control en el modelo Weibull.
3. Scale_treatment: Es es el parámetro escala para el brazo tratamiento en el modelo Weibull.

Para las 3 variables se usa una distribución a prior gamma. 

Estas distribuciones a priori son proper ya que integra 1 en todo su dominio, ya que todas las prior son gamma y están parametrizadas para tener sólo valores positivos en los parámetros. Esto tiene sentido ya que en análisis de supervivencia sólo valores positivos son considerados. Una ejemplo común de improper es escoger una dist. uniforme sobre todos los valores reales por una media ya que esta dist. tiene valores infinitos bajo la curva.

Así mismo, estas prior son commensurate. Esto se refiere a que si puede tomar valores lógicos con el análisis de supervivencia. Estos priors lo son porque sólo pueden tomar valores positivos respetando la naturaleza y la escala de los datos. 

```{r `stan-file separate arms`, eval = T}

cat(readLines("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/weibull_gamma_def.stan"), sep = "\n")

```

Antes de continuar, merece la pena evaluar la comparabilidad entre un ensayo frecuentista y Bayesiano en términos no sólo de decision (rechazar o no rechazar una hipótesis) si no que la fuerza con que se rechaza tiene que ser similar para el mismo dataset de cada uno de los ensayos simulados. Hay que tener en cuenta que los resultados no son para nada lo mismo (el concepto de p-valores y 1-Posterior son totalmente diferentes) y que para el frecuentista se hace una regresión de Cox y en el Bayesiano una regresión de Weibull.

Cabe destacar que aunque aquí no lo muestro, también he comparado los resultados de una regresión Weibull con mi frecuentista con el Weibull de Bayesiano y arroja resultados similares.

A continuación muestro los parámetros que he usado para el ensayo frecuentista para obtener los p-valores:

```{r `pvalues freq`, eval = T}
shape_parameter <- 1.130169 
sample_size <- 300
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- matrix(c(5.5,7.4), nrow = 1, ncol = 2, byrow = TRUE)
censor <- c(0.2,0.2)
Tmax <- 16
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE
IA <- NULL
Plot_Power = FALSE 
plot_pvalues = TRUE
analysis = "freq" 
n_sim <- 10000
seed <- 55

sim1 <- sim_trials(n_sim = n_sim, 
                   analysis = "freq",
                   sample_size = sample_size,
                   ratio = ratio,
                   rand_type = rand_type,
                   Tmax = Tmax, 
                   shape_parameter = shape_parameter,
                   scenarios_eff = scenarios_eff,
                   censor = censor,
                   alpha = alpha,
                   test.type = test.type,
                   IA = IA,
                   method_IA = method_IA,
                   n_exp_events = n_exp_events,
                   HR_1 = HR_1,
                   seed=seed,
                   Plot_Power = Plot_Power,
                   plot_pvalues = plot_pvalues)

pvalues1 <- sim1[[2]]
sim11 <- sim1[[1]]
```

Ahora muestro el código que se ha usado para el Bayesiano con una distribución a priori no informativa para los dos brazos:

```{r `1-Posterior values`, eval = T}
shape_parameter <- 1.130169 
sample_size <- 300
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- matrix(c(5.5,7.4), nrow = 1, ncol = 2, byrow = TRUE)
censor <- c(0.2,0.2)
Tmax <- 16
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE
IA <- NULL
Plot_Power = FALSE 
plot_pvalues = TRUE 
analysis = "bayes"
n_sim <- 10000
seed <- 55
method_IA <- "Bayes"
n_exp_events <- 175
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(0.00001, 0.00001, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim4 <- sim_trials(n_sim = n_sim,  
#                    analysis = "bayes",
#                    sample_size = sample_size, 
#                    ratio = ratio,
#                    rand_type = rand_type,
#                    Tmax = Tmax, 
#                    scenarios_eff = scenarios_eff,
#                    shape_parameter = shape_parameter,
#                    censor = censor,
#                    test.type = test.type,
#                    alpha = alpha,
#                    method_IA = method_IA,
#                    IA = IA,
#                    n_exp_events = n_exp_events,
#                    HR_1 = HR_1,
#                    Plot_Power = Plot_Power,
#                    prior = prior,
#                    modelo = modelo,
#                    modelo_bayes_test = modelo_bayes_test,
#                    prior_type = prior_type,
#                    P_HR_data_Boundary = P_HR_data_Boundary,
#                    prior_gamma = prior_gamma,
#                    Plot_Power_scenarios = Plot_Power_scenarios,
#                    desired_HRs = desired_HRs,
#                    plot_pvalues = plot_pvalues,
#                    Plot_Control_Scenarios = Plot_Control_Scenarios,
#                    seed = seed)
# 
# pvalues4 <- sim4[[2]]
# sim44 <- sim4[[1]]

# Leemos los datos porque la simulación es de muchas horas:

sheet_names <- c("Sheet1","Sheet2")
pvalues4 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/p_values_non_informative_gamma.xlsx", sheet = sheet_names[2])

```

Juntamos los datos de los dos enfoques para los mismos resultados de cada uno de los ensayos clínicos y se enseña un plot donde se muestra que todos los puntos están alineados alrededor de la línea identidad, mostrando una gran consistencia entre el enfoque frecuentista y el Bayesiano con priors no informativas.

```{r `Comparison pvalues vs 1-Posterior 1`, eval = T}

pvalues1$p_value <- pvalues1$p_value/2
plot_pvalues <- inner_join(pvalues1, pvalues4, by = "n_sim") # Si solo cogemos los de Cox y Bayes

# Cuando el "p-valor" Bayesiano supera el 50% hay que hacer 1-(1-post_prob) para hacer este valor
# comparable con el p-valor frecuentista
plot_pvalues <- plot_pvalues %>%
  mutate(`1-post_prob` = ifelse(`1-post_prob` > 0.5, (1 - `1-post_prob`), `1-post_prob`))

ggplot(plot_pvalues, aes(x = p_value, y = `1-post_prob`)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  xlab("P-value") +
  ylab("1 - Posterior Probability") +
  ggtitle("Frequentist P-value vs Bayesian 1 - Posterior Probability") +
  theme_minimal()

```

Además, en la siguiente figura (donde se ha hecho zoom en la zona entre 0 y 0.05) se muestra que en la mayoría de los casos simulados, el frecuentista y Bayesiano llegan a la misma conclusión con respecto a la significancia de los datos.

```{r `Comparison pvalues vs 1-Posterior 2`, eval = T}

plot_pvalues_table <- plot_pvalues %>%
  mutate(
    p_value_significant = ifelse(p_value < 0.025, 1, 0),
    post_prob_significant = ifelse(`1-post_prob` < 0.025, 1, 0)
  )

agreement_proportion <- mean(plot_pvalues_table$p_value_significant == plot_pvalues_table$post_prob_significant)

disagree_trials <- plot_pvalues_table %>%
  filter(p_value_significant != post_prob_significant)

n_disagree <- nrow(disagree_trials)

frequentist_only <- disagree_trials %>%
  filter(p_value_significant == 1 & post_prob_significant == 0)

bayesian_only <- disagree_trials %>%
  filter(p_value_significant == 0 & post_prob_significant == 1)

freq_only_prop <- nrow(frequentist_only) / n_disagree
print(freq_only_prop)

bayes_only_prop <- nrow(bayesian_only) / n_disagree
print(bayes_only_prop)

ggplot(plot_pvalues, aes(x = p_value, y = `1-post_prob`)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "black") +
  geom_vline(xintercept = 0.025, linetype = "dashed", color = "red") +
  geom_hline(yintercept = 0.025, linetype = "dashed", color = "red") +
  annotate("text", x = 0.0375, y = 0.0125, label = "Only Bayesian test is significant") +
  annotate("text", x = 0.0125, y = 0.0375, label = "Only frequentist test is significant") +
  xlim(c(0, 0.05)) +
  ylim(c(0, 0.05)) +
  xlab("P-value") +
  ylab("1- Posterior Probability") +
  ggtitle("P-value vs 1-Posterior Probability") +
  theme_minimal() +
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12))

```
Por ultimo, aquí muestro la comparación entre los dos enfoques con el porcentaje total de "fallos", lo que se ve que es muy residual.

```{r `Comparison pvalues vs 1-Posterior 3`, eval = T}

freq_only_prop <- nrow(frequentist_only) / nrow(plot_pvalues_table)
bayes_only_prop <- nrow(bayesian_only) / nrow(plot_pvalues_table)

ggplot(plot_pvalues_table, aes(x = p_value, y = `1-post_prob`)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  geom_vline(xintercept = 0.05, linetype = "dashed", color = "red") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "red") +
  annotate("text", x = 0.3, y = 0.02, label = paste("Only Bayesian test is significant (", round(bayes_only_prop*100, 2), "%)", sep = "")) +
  annotate("text", x = 0.15, y = 0.4, label = paste("Only frequentist test is significant (", round(freq_only_prop*100, 2), "%)", sep = "")) +
  xlim(c(0, 0.55)) +
  ylim(c(0, 0.55)) +
  xlab("P-value") +
  ylab("1-Posterior Probability") +
  ggtitle("P-value vs 1-Posterior Probability") +
  theme_minimal() +
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12))

```

De manera preliminar (y sin detallar nada a modo de resumen). Se van a considerar 3 modelos Bayesianos diferentes donde se van a considerar 3 prior distribution diferentes para el brazo control considerando siempre una prior no informativa para el brazo tratamiento.
Esta información a priori sigue una distribución beta y aunque los resultados a partir de ahora se muestran en términos de medianas, las prior se van a especificar en términos de la escala weibull para este tratamiento.

La fórmula para transformar las medianas en parámetro escala para la Weibull es la siguiente (asumiendo un shape=1 que es exponencial):

parameters <- medians / (log(2)^(1/shape_parameter))

Por ejemplo, si estos análisis consideramos como hasta ahora con un tamaño muestral de 300 que la mediana para el brazo control es 5.5 y 10 para el brazo experimental considerando un HR de 0.55, la transformación en términos de escala Weibull es:

Control: Mediana de 5.5 es 7.934823
Experimental: Mediana de 10 es 14.42695

A continuación muestro los 3 plots de densidad para la prior del brazo control.

1) Prior no informativa: Equivalente al frecuentista porque "dejamos que los datos hablen por sí mismos" a través de la función de verosimilitud.

2) Prior débil: Damos una idea de en qué rangos debería estar el parámetro para la mediana pero es información muy vaga y que no aporta gran conocimiento.

3) Prior muy informativa: Tenemos una certeza absoluta de qué valor es el valor real de la mediana y dejamos poco espacio a la incertidumbre (muy poca variabilidad).

```{r `Gamma priors`, eval = T}

parameters_gamma <- data.frame(shape = c(0.00001, 7.213475, 793.4823),
                         rate = c( 0.00001, 1, 100),
                         label = c("Non-informative: Gamma(0.00001, 0.00001)", "Weak: Gamma(7.213475, 1)", "Informative: Gamma(793.4823, 100)"))

gamma_density <- function(shape, rate, label) {
  data.frame(x = seq(0, 20, length.out = 100)) %>%
    mutate(density = dgamma(x, shape = shape, rate = rate),
           parameter = label)
}

densities <- purrr::pmap_df(parameters_gamma, gamma_density)

ggplot(densities, aes(x, density, color = parameter)) +
  geom_line() +
  labs(title = "Gamma Prior Distributions")


```

A continuación, pasamos a evaluar las características operantes para los 3 modelos Bayesianos propuestos. Se va a estudiar como anteriormente, el Poder, el Error de Tipo I y el MSE.

Se va a evaluar un escenario donde el HR = 0.55 y se asume una exponencial. Se van a considerar los parámetros del SAP como si no tuviéramos conocimiento de los resultados finales.

A continuación está el Bayesiano con prior no informativa:
```{r `Bayes: Non informative prior distribution`, eval = T}

shape_parameter <- 1 

control_medians <- c(3.5,3.75,4,4.25,4.5,4.75,5,5.25,5.5,5.75,6,6.25,6.5,6.75,7,7.25,7.5)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "Hazard Ratio")))

desired_HR = 0.55

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "Hazard Ratio"] = (lambda_control / lambda_treatment)^shape_parameter
}

medians <- medians[,1:2]
sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- TRUE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(0.00001, 0.00001, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("prueba3_sep.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim5 <-  sim_trials(n_sim = n_sim,  
#                     analysis = "bayes",
#                     sample_size = sample_size, 
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax, 
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# sim5[[2]]
# sim5[[3]]
# sim5[[4]]
# 
# sim55 <- sim5[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim55)
# saveWorkbook(wd, "non_informative_gamma_300_2000_HR_055.xlsx", overwrite = TRUE)
# 


sheet_names <- c("Sheet1")
non_informative_gamma <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/non_informative_gamma_300_2000_HR_055.xlsx", sheet = sheet_names[1])


```
El Bayesiano con una prior muy poco informativa (débil):

```{r `Bayes: Weak informative prior distribution`, eval = T}

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- TRUE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(7.213475, 1, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Weak informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim6 <-  sim_trials(n_sim = n_sim,  
#                     analysis = "bayes",
#                     sample_size = sample_size, 
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax, 
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# sim6[[2]]
# sim6[[3]]
# sim6[[4]]
# 
# sim66 <- sim6[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim66)
# saveWorkbook(wd, "weak_informative_gamma_300_2000_HR_055.xlsx", overwrite = TRUE)


sheet_names <- c("Sheet1")
weak_informative_gamma <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/weak_informative_gamma_300_2000_HR_055.xlsx", sheet = sheet_names[1])


```


Y el Bayesiano con prior muy informativa:

```{r `Bayes: Strong informative prior distribution`, eval = T}

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- TRUE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(793.4823, 100, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Strong informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim7 <-  sim_trials(n_sim = n_sim,  
#                     analysis = "bayes",
#                     sample_size = sample_size, 
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax, 
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# sim7[[2]]
# sim7[[3]]
# sim7[[4]]
# 
# sim77 <- sim7[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim77)
# saveWorkbook(wd, "strong_informative_gamma_300_2000_HR_055.xlsx", overwrite = TRUE)


sheet_names <- c("Sheet1")
strong_informative_gamma <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/strong_informative_gamma_300_2000_HR_055.xlsx", sheet = sheet_names[1])


```

Por último, leemos los datos del frecuentista para HR=0.55:

```{r `Bayes: freq HR=0.55`, eval = T}

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- TRUE
IA <- NULL
method_IA <- "freq"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE

analysis = "freq"
n_sim <- 2000
seed <- 24


# sim8 <-  sim_trials(n_sim = n_sim,  
#                     analysis = "freq",
#                     sample_size = sample_size, 
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax, 
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# sim8[[2]]
# sim8[[3]]
# sim8[[4]]
# 
# sim88 <- sim8[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim8)
# saveWorkbook(wd, "freq_300_2000_HR_055.xlsx", overwrite = TRUE)


sheet_names <- c("Sheet1")
frequentist <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/freq_300_2000_HR_055.xlsx", sheet = sheet_names[1])


```
Una vez que hemos recogido los resultados de la simulación vamos a comparar en los mismos gráficos el rendimiento de cada uno de los modelos. También se incorpora el frecuentista para tenerlo como referencia.

A continuación recogemos los datos para poder dibujarlos

```{r `Data gathering `, eval = T}

if(HR_1 == TRUE){hr_1 <- replicate(n = ncol(scenarios_eff), scenarios_eff[, 1])
  scenarios_eff <- rbind(scenarios_eff, hr_1)}

scenarios_eff_df <- as.data.frame(scenarios_eff)
colnames(scenarios_eff_df) <- c("median_control", "median_treatment")

scenarios_eff_df$scenario <- seq_len(nrow(scenarios_eff_df))

# Frequentist
data_frequentist <- frequentist %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Frequentist")

data_filtered_diff_not_zero_frequentist <- data_frequentist %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_frequentist <- data_frequentist %>%
  filter(median_control == median_treatment)

# Non-informative
data_non_informative <- non_informative_gamma %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Non-informative")

data_filtered_diff_not_zero_non_informative <- data_non_informative %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_non_informative <- data_non_informative %>%
  filter(median_control == median_treatment)

# Very informative
data_informative <- strong_informative_gamma %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Strongly Informative")

data_filtered_diff_not_zero_informative <- data_informative %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_informative <- data_informative %>%
  filter(median_control == median_treatment)

# Mid informative
data_mid_informative <- weak_informative_gamma %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Weakly informative")

data_filtered_diff_not_zero_mid_informative <- data_mid_informative %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_mid_informative <- data_mid_informative %>%
  filter(median_control == median_treatment)

median_control_center <- median(data_filtered_diff_not_zero_informative$median_control)
median_control_center_zero_diff <- median(data_filtered_diff_zero_informative$median_control)

combined_data_diff_not_zero <- bind_rows(data_filtered_diff_not_zero_frequentist,
                                         data_filtered_diff_not_zero_non_informative, 
                                         data_filtered_diff_not_zero_informative, 
                                         data_filtered_diff_not_zero_mid_informative)

combined_data_diff_zero <- bind_rows(data_filtered_diff_zero_frequentist,
                                     data_filtered_diff_zero_non_informative, 
                                     data_filtered_diff_zero_informative, 
                                     data_filtered_diff_zero_mid_informative)

```

Primero observamos cómo de bien se ajustan las inferencias a los datos verdaderos (poblacionales dentro de la simulación). 

Los resultados de MSE son prácticamente iguales para el modelo frecuentista como el bayesiano con las priors no informativas. Con respecto a la Weak, 

Por último la informativa tiene un comportamiento espectacular en escenarios donde la mediana real para el control son muy cercanos a 5.5 pero muy malos según se van alejando del valores central de esta prior.

```{r MSE, eval = T}

p6 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = mean_MSE, color = Prior, linetype = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "MSE", x = "Median Control", y = "Mean MSE") +
  theme_minimal()  + 
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.075))
p6

```

Con respecto al poder...

```{r Power, eval = T}

p7 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "Power", x = "Median Control", y = "Prop. Significant") +
  theme_minimal() + 
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 1))
p7

```

Por último, el error de Tipo I vemos que se controla para el bayesiano no informativo y el weak. Sin embargo, en el bayesiano informativo para valores mayores de 5.5 el error de tipo I se infla de una manera exponencial. Esto demuestra que este modelo no debería usarse a no ser que exista un consenso de que estos valores son "seguros".

```{r T1E, eval = T}

p8 <- ggplot(combined_data_diff_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  geom_hline(yintercept = 0.025, linetype = "dashed", color = "green") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(title = "Type I Error", x = "Median Control", y = "Prop. Significant") +
  theme_minimal() +
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_zero$median_control), max(combined_data_diff_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.5))
p8

```

También resulta valioso cómo afectan diferentes asunciones para la prior informativa en el brazo control si vamos moviendo el HR. Esto nos va a permitir ver cómo asunciones demasiado pesimistas para el brazo control favorece que sea más fácil ver diferencias entre brazos y por lo tanto, considerar el estudio como significativo. Por otro lado, es relativamente común ver cuando un estudio sale fallido achacar la culpa a que la mediana del brazo control ha sido más buena de lo esperado y por lo tanto, es más difícil demostrar eficacia.

Ahora vamos a usar el gráfico que ya se presentó anteriormente sólo para el frecuentista con priors muy informativas y comparar cómo afectan estas asunciones informativas a la facilidad o dificultad para declarar eficacia. Esto también se puede ver traducido a la importancia de elegir un threshold para demostrar eficacia ya que esto es totalmente comparable a usar un prior muy muy informativa a un valor determinado.

Recordar que siempre se usa un prior no informativa para el brazo experimental.

Se va a hacer sólo para los parámetros que la compañía usó en el SAP, i.e., distribución exponencial (shape parameter = 1), tamaño muestral de 300 y mediana de control de 5.5

Aquí recuerdo los datos para el efecto dadas estas asunciones (sólo a partir de 1.3 esta vez):

```{r `Medians w control arm at 5.5 Part 2` , eval = T, echo = F}

shape_parameter <- 1

desired_HRs <- c(1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(5.5, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}
#medians_control_fijo

```

A continuación se cogen los datos del frecuentista para los que se hizo en la gráfica original:

```{r `Bayes: freq HR=0.55 Power changing HRs`, eval = T}

shape_parameter <- 1
sample_size <- 300
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = length(medians_control_fijo), ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE
IA <- NULL
Plot_Power = FALSE
plot_pvalues = FALSE
Plot_Power_scenarios = FALSE
analysis = "freq" 
n_sim <- 1000
seed <- 24

# sim3 <- sim_trials(n_sim = n_sim,
#                    analysis = "freq",
#                    sample_size = sample_size,
#                    ratio = ratio,
#                    rand_type = rand_type,
#                    Tmax = Tmax,
#                    shape_parameter = shape_parameter,
#                    scenarios_eff = scenarios_eff,
#                    censor = censor,
#                    alpha = alpha,
#                    test.type = test.type,
#                    IA = IA,
#                    method_IA = method_IA,
#                    n_exp_events = n_exp_events,
#                    HR_1 = HR_1,
#                    seed=seed,
#                    Plot_Power = Plot_Power,
#                    plot_pvalues = plot_pvalues)

#
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim3)
# saveWorkbook(wd, "freq_2000_HR_changing.xlsx", overwrite = TRUE)
# 
sheet_names <- c("Sheet1")
sim3_results <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/freq_2000_HR_changing.xlsx", sheet = sheet_names[1])


```

Vamos a ver la densidad para cada una de las distribuciones informativas del control forzando valores de 3.5, 5.5 y 7.5.

```{r `Gamma priors informatives`, eval = T}

# Vamos a presentarlo en términos de medianas en vez de escala Weibull (aunque luego sea esto lo que se meta como input en el model STAN)

# En términos del parámetro escala
# parameters <- medians / (log(2)^(1/shape_parameter))
# En términos de mediana
# medians <- parameters * (log(2))^(1/shape_parameter)


parameters_gamma <- data.frame(shape = c(350, 550, 750),
                         rate = c( 100, 100, 100),
                         label = c("Informative: Gamma(504.9433, 100)", "Informative: Gamma(793.4823, 100)", "Informative: Gamma(1082.021, 100)"))

# parameters_gamma <- data.frame(shape = c(504.9433, 793.4823, 1082.021),
#                          rate = c( 100, 100, 100),
#                          label = c("Informative: Gamma(504.9433, 100)", "Informative: Gamma(793.4823, 100)", "Informative: Gamma(1082.021, 100)"))


gamma_density <- function(shape, rate, label) {
  data.frame(x = seq(0, 20, length.out = 100)) %>%
    mutate(density = dgamma(x, shape = shape, rate = rate),
           parameter = label)
}

densities <- purrr::pmap_df(parameters_gamma, gamma_density)

ggplot(densities, aes(x, density, color = parameter)) +
  geom_line() +
  labs(title = "Gamma Informative Prior Distributions")

```

Simulamos los resultados forzando a la prior informativa del control a ser de 3.5 

```{r `Bayes: Bayes infor med: 3.5 HR=0.55 Power changing HRs`, eval = T}

shape_parameter <- 1 

desired_HRs <- c(1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(5.5, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = dim(medians_control_fijo)[1], ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(504.9433, 100, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim5 <-  sim_trials(n_sim = n_sim,  
#                     analysis = "bayes",
#                     sample_size = sample_size, 
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax, 
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim5)
# saveWorkbook(wd, "informative_gamma35_300_2000_HR_changing.xlsx", overwrite = TRUE)
# 


sheet_names <- c("Sheet1")
informative_gamma_3.5 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/informative_gamma35_300_2000_HR_changing.xlsx", sheet = sheet_names[1])

```
Simulamos los resultados forzando a la prior informativa del control a ser de 5.5 

```{r `Bayes: Bayes infor med: 5.5 HR=0.55 Power changing HRs`, eval = T}

shape_parameter <- 1 

desired_HRs <- c(1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(5.5, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = dim(medians_control_fijo)[1], ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(793.4823, 100, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim5 <-  sim_trials(n_sim = n_sim,  
#                     analysis = "bayes",
#                     sample_size = sample_size, 
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax, 
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim5)
# saveWorkbook(wd, "informative_gamma55_300_2000_HR_changing.xlsx", overwrite = TRUE)
# 


sheet_names <- c("Sheet1")
informative_gamma_5.5 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/informative_gamma55_300_2000_HR_changing.xlsx", sheet = sheet_names[1])

```

Simulamos los resultados forzando a la prior informativa del control a ser de 7.5 

```{r `Bayes: Bayes infor med: 7.5 HR=0.55 Power changing HRs`, eval = T}

shape_parameter <- 1 

desired_HRs <- c(1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(5.5, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = dim(medians_control_fijo)[1], ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(1082.021, 100, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim5 <-  sim_trials(n_sim = n_sim,  
#                     analysis = "bayes",
#                     sample_size = sample_size, 
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax, 
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim5)
# saveWorkbook(wd, "informative_gamma75_300_2000_HR_changing.xlsx", overwrite = TRUE)
# 


sheet_names <- c("Sheet1")
informative_gamma_7.5 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/informative_gamma75_300_2000_HR_changing.xlsx", sheet = sheet_names[1])

```


Aquí juntamos los datos:

```{r `Simulation scenarios w fixed experimental & bayes w data`, eval = T, echo = F}


 data_sim3_results <- sim3_results %>%
                      group_by(scenario) %>%
                      summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
                                mutate(Prior = "Frequentist")

 data_informative_gamma_3.5 <- informative_gamma_3.5 %>%
                                group_by(scenario) %>%
                                summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                          mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
                                          mutate(Prior = "Informative: Median control at 3.5")
                                         
 data_informative_gamma_5.5 <- informative_gamma_5.5 %>%
                                group_by(scenario) %>%
                                summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                          mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
                                          mutate(Prior = "Informative:  Median control at 5.5")
                                         
 data_informative_gamma_7.5 <- informative_gamma_7.5 %>%
                                group_by(scenario) %>%
                                summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                          mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
                                          mutate(Prior = "Informative:  Median control at 7.5")  

 
  combined_data <- bind_rows(data_sim3_results,
                            data_informative_gamma_3.5, 
                            data_informative_gamma_5.5, 
                            data_informative_gamma_7.5)


combined_data <- combined_data %>%
  group_by(Prior) %>%
  mutate(desired_HRs = (desired_HRs))

# La diferencia la he considerado en valor absoluto (se podría también al cuadrado para que salgan siempre positivos)
combined_data <- combined_data %>%
  mutate(diff_HR = abs(mean_HR - desired_HRs))


```

Ahora los dibujamos juntos: 

```{r `Simulation scenarios w fixed experimental & bayes w plot`, eval = T, echo = F}


p4 <- ggplot(combined_data, aes(x = desired_HRs, y = prop_significant, group = Prior, color = Prior)) +
    geom_point(size = 3) +
    geom_line(linewidth = 1) +
    geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
    geom_vline(xintercept = 0.55, linetype = "dashed", color = "red", size = 1) +
    geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
    geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
    scale_x_continuous(breaks = seq(min(combined_data$desired_HRs), max(combined_data$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data$desired_HRs), max(combined_data$desired_HRs), 0.1))) +
    scale_y_continuous(labels = scales::percent) +
    labs(title = "Power vs Hazard Ratio",
         x = "Theoretical Hazard Ratio",
         y = "Power") +
    theme_minimal() +
    theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
          axis.title = element_text(size = 10, face = "bold"),
          axis.text = element_text(size = 10),
          legend.position = "right")


p4 <- p4 +
  annotate("text", x = max(combined_data$desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(combined_data$desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green") +
  geom_label(aes(x = 1, y = Inf, label = "H0"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.55, y = Inf, label = "H1"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.5, y = Inf, label = "Real HR"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold")

p4

p5 <- ggplot(combined_data, aes(x = desired_HRs, y = diff_HR, color = Prior)) +
  geom_point(size = 3) +
  geom_line(aes(group = Prior), linewidth = 1) +
  scale_x_continuous(breaks = seq(min(combined_data$desired_HRs), max(combined_data$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data$desired_HRs), max(combined_data$desired_HRs), 0.1))) +
  labs(title = "Diff btw Simulated HR and Theoretical HRs",
       x = "Theoretical Hazard Ratio",
       y = "Difference") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 8, face = "bold"),
        axis.text = element_text(size = 8),
        legend.position = "right")

p5
```


También es interesante en vez de dibujar todas las prior muy informativas con diferentes asunciones para la mediana del control (e.g., 3.5, 5.5 y 7.5) es ver cómo se comportan diferentes priors (i.e., no informativa, weak informative y strong informative) para la misma asunción del efecto de mediana 3.5, 5.5 y 7.5.

Como ya tenemos estos valores para las strong informative para cada uno de los escenarios, vamos a poner ahora los parámetros para la no informativa (son los mismos datos para todos los escenarios) y la weak.

Empezamos con la no informativa para todos los escenarios 

```{r `Bayes: Bayes Non-Infor HR=0.55 Power changing HRs`, eval = T}

shape_parameter <- 1 

desired_HRs <- c(1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(5.5, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = dim(medians_control_fijo)[1], ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(0.00001, 0.00001, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim5 <-  sim_trials(n_sim = n_sim,
#                     analysis = "bayes",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim5)
# saveWorkbook(wd, "non_informative_gamma_300_2000_HR_changing.xlsx", overwrite = TRUE)



sheet_names <- c("Sheet1")
non_informative_gamma <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/non_informative_gamma_300_2000_HR_changing.xlsx", sheet = sheet_names[1])

```
Ahora dibujamos las densidades para las weak prior que estarán centradas en los valores de 3.5, 5.5 y 7.5:

```{r `Gamma priors weak informatives`, eval = T}

# Vamos a presentarlo en términos de medianas en vez de escala Weibull (aunque luego sea esto lo que se meta como input en el model STAN)

# En términos del parámetro escala
# parameters <- medians / (log(2)^(1/shape_parameter))
# En términos de mediana
# medians <- parameters * (log(2))^(1/shape_parameter)

# parameters_gamma <- data.frame(shape = c(3.50, 5.50, 7.50),
#                          rate = c( 1, 1, 1),
#                          label = c("Informative: Gamma(5.049433, 1)", "Informative: Gamma(7.934823, 1)", "Informative: Gamma(10.82021, 1)"))

parameters_gamma <- data.frame(shape = c(5.049433, 7.934823, 10.82021),
                         rate = c( 1, 1, 1),
                         label = c("Informative: Gamma(5.049433, 1)", "Informative: Gamma(7.934823, 1)", "Informative: Gamma(10.82021, 1)"))


gamma_density <- function(shape, rate, label) {
  data.frame(x = seq(0, 20, length.out = 100)) %>%
    mutate(density = dgamma(x, shape = shape, rate = rate),
           parameter = label)
}

densities <- purrr::pmap_df(parameters_gamma, gamma_density)

ggplot(densities, aes(x, density, color = parameter)) +
  geom_line() +
  labs(title = "Gamma Informative Prior Distributions")

```
Parámetros weak para mediana del 3.5

```{r `Bayes: Bayes weak infor med: 3.5 HR=0.55 Power changing HRs`, eval = T}

shape_parameter <- 1 

desired_HRs <- c(1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(5.5, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = dim(medians_control_fijo)[1], ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(5.049433, 1, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim5 <-  sim_trials(n_sim = n_sim,
#                     analysis = "bayes",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim5)
# saveWorkbook(wd, "weak_informative_gamma_3.5_300_2000_HR_changing.xlsx", overwrite = TRUE)



sheet_names <- c("Sheet1")
weak_informative_gamma_3.5 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/weak_informative_gamma_3.5_300_2000_HR_changing.xlsx", sheet = sheet_names[1])

```
Parámetros weak informative de 5.5

```{r `Bayes: Bayes weak infor med: 5.5 HR=0.55 Power changing HRs`, eval = T}

shape_parameter <- 1 

desired_HRs <- c(1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(5.5, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = dim(medians_control_fijo)[1], ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(7.934823, 1, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim5 <-  sim_trials(n_sim = n_sim,
#                     analysis = "bayes",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim5)
# saveWorkbook(wd, "weak_informative_gamma_5.5_300_2000_HR_changing.xlsx", overwrite = TRUE)



sheet_names <- c("Sheet1")
weak_informative_gamma_5.5 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/weak_informative_gamma_5.5_300_2000_HR_changing.xlsx", sheet = sheet_names[1])

```

Parámetros weak informative de 7.5

```{r `Bayes: Bayes weak infor med: 7.5 HR=0.55 Power changing HRs`, eval = T}

shape_parameter <- 1 

desired_HRs <- c(1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(5.5, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = dim(medians_control_fijo)[1], ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(10.82021, 1, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim5 <-  sim_trials(n_sim = n_sim,
#                     analysis = "bayes",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim5)
# saveWorkbook(wd, "weak_informative_gamma_7.5_300_2000_HR_changing.xlsx", overwrite = TRUE)



sheet_names <- c("Sheet1")
weak_informative_gamma_7.5 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/weak_informative_gamma_7.5_300_2000_HR_changing.xlsx", sheet = sheet_names[1])

```

Ahora juntamos los datos para dibujar cada uno de los escenarios.

```{r `Simulation scenarios w fixed experimental & bayes w data 2`, eval = T, echo = F}


 data_sim3_results <- sim3_results %>%
                      group_by(scenario) %>%
                      summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
                                mutate(Prior = "Frequentist")

 data_non_informative <- non_informative_gamma %>%
                         group_by(scenario) %>%
                         summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
                                mutate(Prior = "Non informative")

  data_weak_informative_gamma_3.5 <- weak_informative_gamma_3.5 %>%
                                      group_by(scenario) %>%
                                      summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
                                            mutate(Prior = "Weak Informative: Median control at 3.5")
             

 data_informative_gamma_3.5 <- informative_gamma_3.5 %>%
                                group_by(scenario) %>%
                                summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                          mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
                                          mutate(Prior = "Informative: Median control at 3.5")
 
 data_weak_informative_gamma_5.5 <- weak_informative_gamma_5.5 %>%
                                      group_by(scenario) %>%
                                      summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
                                            mutate(Prior = "Weak Informative: Median control at 5.5")
   
 data_informative_gamma_5.5 <- informative_gamma_5.5 %>%
                                group_by(scenario) %>%
                                summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                          mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
                                          mutate(Prior = "Informative:  Median control at 5.5")
  
data_weak_informative_gamma_7.5 <- weak_informative_gamma_7.5 %>%
                                      group_by(scenario) %>%
                                      summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
                                            mutate(Prior = "Weak Informative: Median control at 7.5")      

 data_informative_gamma_7.5 <- informative_gamma_7.5 %>%
                                group_by(scenario) %>%
                                summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                          mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
                                          mutate(Prior = "Informative:  Median control at 7.5")  

 # 3.5
  combined_data_3.5 <- bind_rows(data_sim3_results,
                            data_non_informative, 
                            data_weak_informative_gamma_3.5, 
                            data_informative_gamma_3.5)

combined_data_3.5 <- combined_data_3.5 %>%
  group_by(Prior) %>%
  mutate(desired_HRs = (desired_HRs))

combined_data_3.5 <- combined_data_3.5 %>%
  mutate(diff_HR = abs(mean_HR - desired_HRs))

# 5.5
  combined_data_5.5 <- bind_rows(data_sim3_results,
                            data_non_informative, 
                            data_weak_informative_gamma_5.5, 
                            data_informative_gamma_5.5)


combined_data_5.5 <- combined_data_5.5 %>%
  group_by(Prior) %>%
  mutate(desired_HRs = (desired_HRs))

combined_data_5.5 <- combined_data_5.5 %>%
  mutate(diff_HR = abs(mean_HR - desired_HRs))

 # 7.5

  combined_data_7.5 <- bind_rows(data_sim3_results,
                            data_non_informative, 
                            data_weak_informative_gamma_7.5, 
                            data_informative_gamma_7.5)

combined_data_7.5 <- combined_data_7.5 %>%
  group_by(Prior) %>%
  mutate(desired_HRs = (desired_HRs))

combined_data_7.5 <- combined_data_7.5 %>%
  mutate(diff_HR = abs(mean_HR - desired_HRs))

```

Dibujamos los de la mediana de 3.5

```{r `Simulation scenarios w fixed experimental & bayes w plot 3.5`, eval = T, echo = F}


p6 <- ggplot(combined_data_3.5, aes(x = desired_HRs, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.55, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(data$desired_HRs), max(data$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(data$desired_HRs), max(data$desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio w prior median control at 3.5",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 10),
        legend.position = "right")

p6 <- p6 +
  annotate("text", x = max(data$desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(data$desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green") +
  geom_label(aes(x = 1, y = Inf, label = "H0"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.55, y = Inf, label = "H1"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.5, y = Inf, label = "Real HR"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold")

p6

# Graficamos las diferencias

p7 <- ggplot(combined_data_3.5, aes(x = desired_HRs, y = diff_HR, color = Prior)) +
  geom_point(size = 3) +
  geom_line(aes(group = Prior), linewidth = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_3.5$desired_HRs), max(combined_data_3.5$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_3.5$desired_HRs), max(combined_data_3.5$desired_HRs), 0.1))) +
  labs(title = "Diff btw Simulated HR and Theoretical HRs w prior median control = 3.5",
       x = "Theoretical Hazard Ratio",
       y = "Difference") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 8, face = "bold"),
        axis.text = element_text(size = 8),
        legend.position = "right")

p7
```

Dibujamos los de la mediana de 5.5

```{r `Simulation scenarios w fixed experimental & bayes w plot 5.5`, eval = T, echo = F}


p8 <- ggplot(combined_data_5.5, aes(x = desired_HRs, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.55, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(data$desired_HRs), max(data$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(data$desired_HRs), max(data$desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio w prior median control at 5.5",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 10),
        legend.position = "right")

p8 <- p8 +
  annotate("text", x = max(data$desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(data$desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green") +
  geom_label(aes(x = 1, y = Inf, label = "H0"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.55, y = Inf, label = "H1"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.5, y = Inf, label = "Real HR"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold")

p8

# Graficamos las diferencias

p9 <- ggplot(combined_data_5.5, aes(x = desired_HRs, y = diff_HR, color = Prior)) +
  geom_point(size = 3) +
  geom_line(aes(group = Prior), linewidth = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_5.5$desired_HRs), max(combined_data_5.5$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_5.5$desired_HRs), max(combined_data_5.5$desired_HRs), 0.1))) +
  scale_y_continuous(limits = c(NA, 0.4)) + # Setting the y-axis max limit here
  labs(title = "Diff btw Simulated HR and Theoretical HRs w prior median control = 5.5",
       x = "Theoretical Hazard Ratio",
       y = "Difference") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 10),
        legend.position = "right")

p9

```

Dibujamos los de la mediana de 7.5

```{r `Simulation scenarios w fixed experimental & bayes w plot 7.5`, eval = T, echo = F}


p10 <- ggplot(combined_data_7.5, aes(x = desired_HRs, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.55, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(data$desired_HRs), max(data$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(data$desired_HRs), max(data$desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio w prior median control at 7.5",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 10),
        legend.position = "right")

p10 <- p10 +
  annotate("text", x = max(data$desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(data$desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green") +
  geom_label(aes(x = 1, y = Inf, label = "H0"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.55, y = Inf, label = "H1"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.5, y = Inf, label = "Real HR"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold")

p10

# Graficamos las diferencias

p11 <- ggplot(combined_data_7.5, aes(x = desired_HRs, y = diff_HR, color = Prior)) +
  geom_point(size = 3) +
  geom_line(aes(group = Prior), linewidth = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_7.5$desired_HRs), max(combined_data_7.5$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_7.5$desired_HRs), max(combined_data_7.5$desired_HRs), 0.1))) +
  labs(title = "Diff btw Simulated HR and Theoretical HRs w prior median control = 7.5",
       x = "Theoretical Hazard Ratio",
       y = "Difference") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 10),
        legend.position = "right")

p11
```


Ahora hacemos lo mismo para asumiendo un HR = 0.7. Aquí las curvas van a ser más pronunciadas porque no está todo tan a favor como para que salgan significativos.

```{r `Bayes: Non informative prior distribution HR=0.7`, eval = T}

shape_parameter <- 1 

control_medians <- c(3.5,3.75,4,4.25,4.5,4.75,5,5.25,5.5,5.75,6,6.25,6.5,6.75,7,7.25,7.5)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "Hazard Ratio")))

desired_HR = 0.7

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "Hazard Ratio"] = (lambda_control / lambda_treatment)^shape_parameter
}

medians <- medians[,1:2]
sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- TRUE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(0.00001, 0.00001, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim5 <-  sim_trials(n_sim = n_sim,  
#                     analysis = "bayes",
#                     sample_size = sample_size, 
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax, 
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# sim5[[2]]
# sim5[[3]]
# sim5[[4]]
# 
# sim55 <- sim5[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim55)
# saveWorkbook(wd, "non_informative_gamma_300_2000_HR_07.xlsx", overwrite = TRUE)
# 


sheet_names <- c("Sheet1")
non_informative_gamma <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/non_informative_gamma_300_2000_HR_07.xlsx", sheet = sheet_names[1])


```
El Bayesiano con una prior muy poco informativa (débil):

```{r `Bayes: Weak informative prior distribution HR=0.7`, eval = T}

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- TRUE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(7.213475, 1, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Weak informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim6 <-  sim_trials(n_sim = n_sim,  
#                     analysis = "bayes",
#                     sample_size = sample_size, 
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax, 
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# sim6[[2]]
# sim6[[3]]
# sim6[[4]]
# 
# sim66 <- sim6[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim66)
# saveWorkbook(wd, "weak_informative_gamma_300_2000_HR_07.xlsx", overwrite = TRUE)


sheet_names <- c("Sheet1")
weak_informative_gamma <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/weak_informative_gamma_300_2000_HR_07.xlsx", sheet = sheet_names[1])


```


Y el Bayesiano con prior muy informativa:

```{r `Bayes: Strong informative prior distribution HR=0.7`, eval = T}

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- TRUE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(793.4823, 100, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Strong informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim7 <-  sim_trials(n_sim = n_sim,  
#                     analysis = "bayes",
#                     sample_size = sample_size, 
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax, 
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# sim7[[2]]
# sim7[[3]]
# sim7[[4]]
# 
# sim77 <- sim7[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim77)
# saveWorkbook(wd, "strong_informative_gamma_300_2000_HR_07.xlsx", overwrite = TRUE)


sheet_names <- c("Sheet1")
strong_informative_gamma <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/strong_informative_gamma_300_2000_HR_07.xlsx", sheet = sheet_names[1])


```

Por último, leemos los datos del frecuentista para HR=0.7:

```{r `Bayes: freq HR=0.7`, eval = T}

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- TRUE
IA <- NULL
method_IA <- "freq"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE

analysis = "freq"
n_sim <- 2000
seed <- 24


# sim8 <-  sim_trials(n_sim = n_sim,  
#                     analysis = "freq",
#                     sample_size = sample_size, 
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax, 
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# sim8[[2]]
# sim8[[3]]
# sim8[[4]]
# 
# sim88 <- sim8[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim8)
# saveWorkbook(wd, "freq_300_2000_HR_07.xlsx", overwrite = TRUE)


sheet_names <- c("Sheet1")
frequentist <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/freq_300_2000_HR_07.xlsx", sheet = sheet_names[1])


```

Una vez que hemos recogido los resultados de la simulación vamos a comparar en los mismos gráficos el rendimiento de cada uno de los modelos. También se incorpora el frecuentista para tenerlo como referencia.

A continuación recogemos los datos para poder dibujarlos

```{r `Data gathering HR=0.7`, eval = T}

if(HR_1 == TRUE){hr_1 <- replicate(n = ncol(scenarios_eff), scenarios_eff[, 1])
  scenarios_eff <- rbind(scenarios_eff, hr_1)}

scenarios_eff_df <- as.data.frame(scenarios_eff)
colnames(scenarios_eff_df) <- c("median_control", "median_treatment")

scenarios_eff_df$scenario <- seq_len(nrow(scenarios_eff_df))

# Frequentist
data_frequentist <- frequentist %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Frequentist")

data_filtered_diff_not_zero_frequentist <- data_frequentist %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_frequentist <- data_frequentist %>%
  filter(median_control == median_treatment)

# Non-informative
data_non_informative <- non_informative_gamma %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Non-informative: Gamma(0.00001, 0.00001)")

data_filtered_diff_not_zero_non_informative <- data_non_informative %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_non_informative <- data_non_informative %>%
  filter(median_control == median_treatment)

# Very informative
data_informative <- strong_informative_gamma %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Informative: Gamma(760.6836, 100)")

data_filtered_diff_not_zero_informative <- data_informative %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_informative <- data_informative %>%
  filter(median_control == median_treatment)

# Mid informative
data_mid_informative <- weak_informative_gamma %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "Mid informative: Gamma(7.213475, 1)")

data_filtered_diff_not_zero_mid_informative <- data_mid_informative %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_mid_informative <- data_mid_informative %>%
  filter(median_control == median_treatment)

median_control_center <- median(data_filtered_diff_not_zero_informative$median_control)
median_control_center_zero_diff <- median(data_filtered_diff_zero_informative$median_control)

combined_data_diff_not_zero <- bind_rows(data_filtered_diff_not_zero_frequentist,
                                         data_filtered_diff_not_zero_non_informative, 
                                         data_filtered_diff_not_zero_informative, 
                                         data_filtered_diff_not_zero_mid_informative)

combined_data_diff_zero <- bind_rows(data_filtered_diff_zero_frequentist,
                                     data_filtered_diff_zero_non_informative, 
                                     data_filtered_diff_zero_informative, 
                                     data_filtered_diff_zero_mid_informative)

```



```{r `MSE HR=0.7`, eval = T}

p6 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = mean_MSE, color = Prior, linetype = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "MSE", x = "Median Control", y = "Prop. Significant") +
  theme_minimal() +
  theme_minimal() +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.13))
p6

```

Con respecto al poder...

```{r `Power HR=0.7`, eval = T}

p7 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "Power", x = "Median Control", y = "Prop. Significant") +
  theme_minimal() +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 1))
p7

```

Por último, el error de Tipo I vemos que se controla para el bayesiano no informativo y el weak. Sin embargo, en el bayesiano informativo para valores mayores de 5.5 el error de tipo I se infla de una manera exponencial. Esto demuestra que este modelo no debería usarse a no ser que exista un consenso de que estos valores son "seguros".

```{r `T1E HR=0.7`, eval = T}

p8 <- ggplot(combined_data_diff_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  geom_hline(yintercept = 0.025, linetype = "dashed", color = "green") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(title = "T1E", x = "Median Control", y = "Prop. Significant") +
  theme_minimal() +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_zero$median_control), max(combined_data_diff_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.3))
p8

```


## Incorporación de información externa para el brazo control de otros estudios.

Como brazo control usand el best standard of care (BSC) y para ello hay diferentes posibles tratamientos:

1) gemcitabine + carboplatin (20 Pts)
2) gemcitabine + cisplatin (11 Pts)
3) pemetrexed + carboplatin (66 Pts)
4) pemetrexed + cisplatin (36 Pts)

He encontrado diferentes fuentes externas (todas mencionadas por la Cy a la hora de diseñar este estudio)

Aquí también he usado la digitalización de las curvas KM para obtener un dataset con datos pseudo-IPD

1) Gemcitabine plus cisplatin vs. gemcitabine plus carboplatin in stage IIIb and IV non-small cell lung cancer: a phase III randomized trial  

```{r `Gemcitabine plus cisplatin vs. gemcitabine plus carboplatin in stage IIIb and IV non-small cell lung cancer: a phase III randomized trial` , eval = T}

# Este estudio compara 2 de los brazos control del Keynote024

data_hist1 <- read.csv("IPD_KEYNOTE026_NSCLC_PFS_gemcis_gemcarbo.csv")
data_hist1$arm <- str_trim(data_hist1$arm)

gem_cis <- subset(data_hist1, arm == "GP")
gem_carbo <- subset(data_hist1, arm == "GC")

# gemcitabine + cisplatin (GP) 
# gemcitabine + carboplatin (GC) 


fit_hist1 <- coxph(Surv(time, status) ~ arm, data = data_hist1)
Estimate <- c(1/exp(confint(fit_hist1))[2], 1/summary(fit_hist1)$coefficients[2], 1/exp(confint(fit_hist1))[1])

fit_gem_cis <- survfit(Surv(time, status) ~ 1, data = gem_cis)
fit_gem_carbo <- survfit(Surv(time, status) ~ 1, data = gem_carbo)

# Regresión de Weibull para sacar los parametros de interes
weibull_gem_cis <- survreg(Surv(time, status) ~ 1, dist = "weibull", data = gem_cis)
weibull_gem_carbo <- survreg(Surv(time, status) ~ 1, dist = "weibull", data = gem_carbo)

time_seq <- seq(min(data_hist1$time), max(data_hist1$time), length.out = 100)

# P de supervivencia usando el modelo Weibull
weibull_gem_cis_surv <- 1 - pweibull(time_seq, shape = 1/weibull_gem_cis$scale, scale = exp(weibull_gem_cis$coefficients))
weibull_gem_carbo_surv <- 1 - pweibull(time_seq, shape = 1/weibull_gem_carbo$scale, scale = exp(weibull_gem_carbo$coefficients))

# Comparamos
plot(fit_gem_cis, col = "blue", main = "Kaplan-Meier Plot with Weibull Fits", xlab = "Time", ylab = "Survival Probability", lty = 1, conf.int = FALSE)
lines(fit_gem_carbo, col = "red", lty = 1, conf.int = FALSE)
lines(time_seq, weibull_gem_cis_surv, col = "green", lty = 2)
lines(time_seq, weibull_gem_carbo_surv, col = "black", lty = 2)

# Vemos que modelos se ajustan mejor
weibull_mod <- flexsurvreg(Surv(time, status) ~ arm, data = data_hist1, dist = "weibull")
exp_mod <- flexsurvreg(Surv(time, status) ~ arm, data = data_hist1, dist = "exp")
gompertz_mod <- flexsurvreg(Surv(time, status) ~ arm, data = data_hist1, dist = "gompertz")
lognormal_mod <- flexsurvreg(Surv(time, status) ~ arm, data = data_hist1, dist = "lnorm")

# Compare models
model_list <- list(Weibull = weibull_mod, Exponential = exp_mod, Gompertz = gompertz_mod, LogNormal = lognormal_mod)
sapply(model_list, AIC)

```

2) Maintenance pemetrexed plus best supportive care versus placebo plus best supportive care for non-small-cell lung cancer: a randomised, double-blind, phase 3 study.


```{r `Maintenance pemetrexed + BSC vs placebo + BSC.`, eval = T}
# Aqui solo nos interesa uno de los brazos, Pemetrexed + BSC en non-squamous population

data_hist2 <- read.csv("IPD_KEYNOTE026_NSCLC_PFS_pem_bsc.csv")

fit_hist2 <- coxph(Surv(time, status) ~ 1, data = data_hist2)

fit_pem_bsc <- survfit(Surv(time, status) ~ 1, data = data_hist2)

# Regresión de Weibull para sacar los parametros de interes
weibull_pem_bsc <- survreg(Surv(time, status) ~ 1, dist = "weibull", data = data_hist2)

time_seq <- seq(min(data_hist2$time), max(data_hist2$time), length.out = 100)

# P de supervivencia usando el modelo Weibull
weibull_pem_bsc_surv <- 1 - pweibull(time_seq, shape = 1/weibull_pem_bsc$scale, scale = exp(weibull_pem_bsc$coefficients))

# Comparamos
plot(fit_pem_bsc, col = "blue", main = "Kaplan-Meier Plot with Weibull Fits", xlab = "Time", ylab = "Survival Probability", lty = 1, conf.int = FALSE)
lines(time_seq, weibull_pem_bsc_surv, col = "green", lty = 2)

# Vemos que modelos se ajustan mejor
weibull_mod <- flexsurvreg(Surv(time, status) ~ 1, data = data_hist2, dist = "weibull")
exp_mod <- flexsurvreg(Surv(time, status) ~ 1, data = data_hist2, dist = "exp")
gompertz_mod <- flexsurvreg(Surv(time, status) ~ 1, data = data_hist2, dist = "gompertz")
lognormal_mod <- flexsurvreg(Surv(time, status) ~ 1, data = data_hist2, dist = "lnorm")

# Compare models
model_list <- list(Weibull = weibull_mod, Exponential = exp_mod, Gompertz = gompertz_mod, LogNormal = lognormal_mod)
sapply(model_list, AIC)

```

3) Maintenance therapy with pemetrexed plus best supportive care versus placebo plus best supportive care after induction therapy with pemetrexed plus cisplatin for advanced non-squamous non-small-cell lung cancer (PARAMOUNT): a double-blind, phase 3, randomised controlled trial. 

```{r `Maintenance therapy with pemetrexed + BSC vs placebo + BSC (PARAMOUNT)` , eval = T}
# Aqui solo nos interesa uno de los brazos, Pemetrexed + BSC en non-squamous population
# Tambien lo que nos interesa es el Pemetrexed+BSC

data_hist3 <- read.csv("IPD_POSEIDON_NSCLC_PFS_pem_bsc.csv")

fit_hist3 <- coxph(Surv(time, status) ~ 1, data = data_hist3)

fit_pem_bsc_poseidon <- survfit(Surv(time, status) ~ 1, data = data_hist3)

# Regresión de Weibull para sacar los parametros de interes
weibull_pem_bsc_poseidon <- survreg(Surv(time, status) ~ 1, dist = "weibull", data = data_hist3)

time_seq <- seq(min(data_hist3$time), max(data_hist3$time), length.out = 100)

# P de supervivencia usando el modelo Weibull
weibull_pem_bsc_surv_poseidon <- 1 - pweibull(time_seq, shape = 1/weibull_pem_bsc_poseidon$scale, scale = exp(weibull_pem_bsc_poseidon$coefficients))

# Comparamos
plot(fit_pem_bsc_poseidon, col = "blue", main = "Kaplan-Meier Plot with Weibull Fits", xlab = "Time", ylab = "Survival Probability", lty = 1, conf.int = FALSE)
lines(time_seq, weibull_pem_bsc_surv_poseidon, col = "green", lty = 2)

# Vemos que modelos se ajustan mejor
weibull_mod <- flexsurvreg(Surv(time, status) ~ 1, data = data_hist3, dist = "weibull")
exp_mod <- flexsurvreg(Surv(time, status) ~ 1, data = data_hist3, dist = "exp")
gompertz_mod <- flexsurvreg(Surv(time, status) ~ 1, data = data_hist3, dist = "gompertz")
lognormal_mod <- flexsurvreg(Surv(time, status) ~ 1, data = data_hist3, dist = "lnorm")

# Compare models
model_list <- list(Weibull = weibull_mod, Exponential = exp_mod, Gompertz = gompertz_mod, LogNormal = lognormal_mod)
sapply(model_list, AIC)

```

# Análisis Bayesiano incorporando información externa

El primer método que se va a considerar es el MAP donde incorpora en la Prior información top-level externa para el brazo control.

Primer paper de referencia es "Robust Meta-Analytic-Predictive Priors in Clinical Trials with
Historical Control Information". Aquí discuten el uso de una mezcla entre MAP y no informativo o weak en caso de que haya un conflicto entre los datos del control con el histórico. También se discute el uso de la medida ESS que cuantifica cuánta muestra equivaldría una prior usada al final.
Esta medida puede ser más interesante si consideramos IAs pero quizás se pueda ver.

Este paper es muy interesante para organizar ideas y estrategias.

Al tener varios estudios históricos se puede organizar el análisis en 4 apartados:

1) MAP -> Un ensayo histórico.
2) MAP -> Varios ensayos históricos
3) Conjugate MAP w weak prior info -> Un ensayo histórico. (Peso de tau con experto)
4) Conjugate MAP w weak prior info -> Varios ensayos históricos. (Peso de tau estimado por ensayos)
5) Power Prior -> Un ensayo histórico. (Peso de tau con experto) Se debe hacer proper normalisation

# Effective Sample Size para cuantificar la influencia de una Prior. (ESS)

Bonus: Medida de ESS para cuantificar cuánto tamaño muestral nos ahorramos por cada prior.

Paper muy útil para coger ideas de medidas y usar EHSS. También usan Weibull.
Paper: "Borrowing From Historical Control Data in Cancer Drug Development: A Cautionary Tale and Practical Guidelines".

A continuación va una manera de calcular la fuerza que tiene una prior en términos de nº de pacientes (ESS for Prior Strength).

La pregunta a la que responde este método es "si no tuviera datos reales y sólo consideramos la prior, cuántas hipotéticas observaciones/pacientes vale esta prior?"

Es decir, esto cuantifica en términos de tamaño muestral la influencia de una prior para uno de los tres parámetros seleccionados. Esto se puede hacer relativamente fácil analíticamente si consideraramos una exponencia en vez de una Weibull. Sin embargo, y en consonancia con todo lo que he hecho, voy a calcular este valor mediante simulaciones.

Otra manera de ver esot es si tenemos una prior informativa, una manera de verlo es como si ya tuviéramos observados ciertos pacientes que proporcionan la misma info que se ve reflejada en la prior.

La idea clave de esto es, ¿Cuántas observaciones desde un punto de vista de una prior no informativa hacen falta para conseguir la misma dist. a posteriori como tengo con mi prior informartiva sin ningún dato?

Esta misma idea se puede aplicar cuando se usen controles externos, esta vez la pregunta sería: Si incorporáramos datos históricos en mi análisis, ¿cuántos pacientes adicionales equivalen en este nuevo ensayo?.

La pregunta es prácticamente la misma por lo que la diferencia está en la fuente de la info.

- Cálculo de ESS.

Schimdli (2014): When designing a new clinical trial, fewer patients can be randomized to control by borrowing strength from historical information. However, the prior effective sample size (ESS) needs then to be quantified. For conjugate priors, the ESS is easily obtained for the exponential family (Bernardo and Smith, 1994), for example, for binary endpoints ESS = a + b with a Beta(a, b) prior. For non-conjugate priors, normal approximations can be used (Morita et al., 2008; Neuenschwander et al., 2010; Morita, Thall, and M¨uller, 2012). Here we apply the methodology by Morita et al.
(2008). The ESS is the sample size such that the expected information of the posterior under a non-informative prior is the same as the information of the informative prior p(ψ), where the information is evaluated at the mode ˜ψ of the
informative prior. 

1) Usamos la prior de interés para obtener la dist. a posteriori sin ningún dato. Para este ejemplo voy a usar el de la prior fuertmente informativa con dist. Gamma(793.4823,100)

```{r ESS_1 , eval = T}

# Alpha_Gamma <- 7.934823
# Beta_Gamma <- 1
# 
# parameters_gamma <- data.frame(shape = c(Alpha_Gamma),
#                          rate = c(Beta_Gamma),
#                          label = c("Informative: Gamma(793.4823, 100)"))
# 
# 
# gamma_density <- function(shape, rate, label) {
#   data.frame(x = seq(0, 20, length.out = 1000)) %>%
#     mutate(density = dgamma(x, shape = shape, rate = rate),
#            parameter = label)
# }
# 
# densities <- purrr::pmap_df(parameters_gamma, gamma_density)
# 
# 
#     lista_prior <- list(N = 0,
#                         t = integer(0),
#                         y = double(0),
#                         v = double(0),
#                         alpha_shape0 = 0.00001,
#                         beta_shape0 = 0.00001,
#                         alpha_scale0_control = Alpha_Gamma,
#                         beta_scale0_control = Beta_Gamma,
#                         alpha_scale0_treatment = 0.00001,
#                         beta_scale0_treatment = 0.00001)
#       
#   posterior_no_data <- rstan::sampling(
#                                        object  = modelo,
#                                        data    = lista_prior,
#                                        chains  = 3,       
#                                        thin    = 3,
#                                        iter    = 3400,   
#                                        warmup  = 1900,
#                                        refresh = 0,
#                                        seed = 12)
#       
# print(posterior_no_data, pars = c("shape", "scale_control", "scale_treatment"), digits = 3, probs = c(0.025, 0.5, 0.975))
# 
# samples_scale_control <- extract(posterior_no_data, pars = "scale_control")$scale_control
# 
# density_posterior_no_data <- density(samples_scale_control)
# # 
# # df_posterior <- data.frame(x = density_est$x, density = density_est$y, parameter = "Posterior")
# # 
# # 
# # combined_df <- rbind(df_posterior, densities)
# 
# # ggplot(combined_df, aes(x = x, y = density, color = parameter)) +
# #   geom_line() +
# #   labs(title = "Gamma Informative Prior and Posterior Distributions",
# #        x = "Value",
# #        y = "Density") +
# #   scale_color_manual(values = c("red", "blue")) 
# # Plot the posterior density first
# plot(density_posterior_no_data, main = "Posterior and Prior Density of scale_control", xlab = "Value", ylab = "Density", col = "steelblue", lwd = 2, ylim=c(0, max(densities$density, density_posterior_no_data$y)))
# abline(v = quantile(samples_scale_control, c(0.025, 0.5, 0.975)), lty = c(2,1,2), col = "red") 
# lines(densities$x, densities$density, col = "green", lwd = 2)
# legend("topright", legend = c("Median", "95% CI", "Posterior", "Prior"), lty = c(1,2,1,1), col = c("red", "red", "steelblue", "green"))

```
2) Usamos una prior no informativa y simulamos diferentes tamaños de muestra para tener una dist. a posteriori similar.

```{r ESS , eval = T}

# sample_size <- seq(from = 10, to= 300, by = 10)  
# shape_parameter <- 1
# ratio <- c(0.5,0.5)
# Tmax <- 20
# rand_type <- "CR" 
# scenarios_eff <- matrix(c(5.5,10), nrow = 1, ncol = 2, byrow = TRUE) # HR=0.55
# censor <- c(0.1,0.1)
# Tmax <- 20
# alpha <- 0.05 
# test.type <- 2
# seed <- 24
# 
# compute_kl <- function(samples_p, samples_q) {
#   
#   p_density <- density(samples_p)
#   q_density <- density(samples_q)
#     
#   common_range <- seq(min(p_density$x, q_density$x), max(p_density$x, q_density$x), length.out = length(p_density$x))
#   
#   p_common <- approx(p_density$x, p_density$y, xout = common_range)$y
#   q_common <- approx(q_density$x, q_density$y, xout = common_range)$y
#   
#   epsilon <- 1e-10
#   p_common <- p_common + epsilon
#   q_common <- q_common + epsvilon
#   
#   na_indices <- which(is.na(p_common) | is.na(q_common))
#   
#   p_common <- p_common[-na_indices]
#   q_common <- q_common[-na_indices]
#   
#   kl_div <- sum(p_common * log(p_common/q_common))
#     
#   return(kl_div)
# }
# 
# kl_divergences <- numeric(length(sample_size))
# 
# for (i in 1:length(sample_size)) {
# 
#   pat <- rand(sample_size[i], ratio, rand_type = rand_type)
# 
#   parameters <- scenarios_eff / (log(2)^(1/shape_parameter))
# 
#   simul_data <- gen_surv_data(pat, parameters,
#                               shape_parameter,
#                               censor, Tmax)
#   
#   lista_prior <- list(N = dim(simul_data)[1],
#                         t = as.integer(simul_data$arm == 2),
#                         y = simul_data$time,
#                         v = simul_data$status,
#                         alpha_shape0 = 0.00001,
#                         beta_shape0 = 0.00001,
#                         alpha_scale0_control = Alpha_Gamma,
#                         beta_scale0_control = Beta_Gamma,
#                         alpha_scale0_treatment = 0.00001,
#                         beta_scale0_treatment = 0.00001)
#       
#       f <- rstan::sampling(
#         object  = modelo,
#         data    = lista_prior,
#         chains  = 3,       
#         thin    = 3,
#         iter    = 3400,   
#         warmup  = 1900,
#         refresh = 0,
#         seed = 12
#       )
#   
#   samples_from_noninfor_data <- extract(f, pars = "scale_control")$scale_control
# 
#   kl_divergences[i] <- compute_kl(samples_scale_control, samples_from_noninfor_data)
#   #kl_divergences[i] <- KLdiv(p = samples_scale_control, q = samples_from_noninfor_data)
# 
#   
#   rm(samples_from_noninfor_data)
# }
# 
# kl_divergences
# ess <- sample_size[which.min(kl_divergences)]
# ess

# Tambien usan el Kullback-Leibler en Schmidli 2015. Parece el más apropiado
```
Esto ya ofrece una medida cuantitativa de cuántos pacientes incluye una u otra prior. Esto es muy útil y puede ser complementaria a calcular el poder con ciertos modelos.

O debería usarse antes de proponer priors?

El caso es que la no informativa se ve dirigida principalmente por los datos obtenidos por lo que hacer solo un dataset por cada tamaño muestral no vale de nada. Para esto debería incorporarse en la simulación pero dejando fijos los efectos de tratamiento y moviendo el tamaño muestral.

Debería meterse dentro un bucle de estos (por ejemplo a la hora de elegir un modelo ya una vez que se ven las características operantes) para ver el poder que se obtiene con este modelo y esto puede ser una medida cuantitativa adicional.

Dejar para más adelante cuando tenga todos los modelos y cojamos 2 o 3.

Mejor meterlo como opcion como ESS = TRUE para luego dejar fuera del bucle el cálculo de la no informativa sin datos. Luego se puede calcular para cada iteracción de Sim_surv_data_Bayes y dejarlo como columna. Luego en la función sim_bayes sería calcular la media y la mediana de estos valores de ESS para cada tamaño muestral y/o cada efecto porque realmente depende del tipo de prior que se seleccione.

## Modelos incorporando información externa

A continuación se va a seguir un procedimiento para la incorporación de información externa a través de la pre-especificación de una prior con parámetros sacados de un meta-análisis (MAP).

Hay muchas maneras de hacer esto pero vamos a centrarnos en la siguiente metodología:

1- Obtener la información externa para el brazo control: Me he centrado principalmente en coger los ensayos especificados por el promotor para justificar la asunción para el tamaño muestral del brazo control.

2- Obtener los parámetros para la dist exponencial/weibull de los ensayos históricos: Al ser datos de supervivencia (con binario no existe este problema), es necesario tener los datos paciente a paciente para poder sacar los valores de la distribución paramétrica que consideremos, ya sea exponencial/weibull. Al no ser posible, se digitalizan las curvas para poder hacer esta regresión paramétrica.

3- Una vez tengamos estos valores de la regresión paramétrica, lambda para la exponencial, y lambda y shape para la weibull, en todos los ensayos históricos, vamos a realizar el meta-análisis.

En el caso de la Weibull, que es el caso que nos ha ocupado principalmente, el parámetro de interés es el lambda del brazo control. Sin embargo, este está muy relacionado con el shape ya que si cambiamos uno cambiamos el otro. El problema con esto es que hasta ahora, sólo había especificado una prior para el scale del control dejando como no-informativa la prior del shape común para los dos brazos de tratamiento. 

Si en este paso del MAP, decido cambiar la estrategia y establecer prior informativas para el scale y el shape en común, los valores no van a ser comparables entre diferentes modelos o diferentes priors. 

Es por esto que para el meta-analísis voy a considerar el hacer regresiones usando la exponencial (en vez de con la weibull) para aprovecharme de que se asume automáticamente que el shape para todos los estudios es igual 1, mientras varían las escalas en consonancia. Después una vez obtenido un valor para la media y la SD del MAP, entonces tiene más sentido dejar la prior del shape en común como no informativo y establecer la prior del brazo control con estos resultados.

4- El meta-análisis aquí se va a considerar de dos maneras, frecuentista y Bayesiano. Para el enfoque frecuentista se va a utilizar el paquete ampliamente usado "metafor" y para el Bayesiano el "brms". Se van a comparar los dos en este informe y ya vemos si usamos uno, otro o los dos.

5- El MAP se va a hacer para la propuesta de una prior usando sólo 1 ensayo clínico y varios.

6- Una vez tengamos la media y SD del MAP, se transformarán estos valores en términos de la distribución Gamma para el modelo STAN.

Estos son links útiles para lo siguiente que se va a hacer:

1) https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/pooling-es.html
2) https://solomonkurz.netlify.app/blog/bayesian-meta-analysis/#rough-draft-meta-analysis
3) https://mvuorre.github.io/posts/2016-09-29-bayesian-meta-analysis/#the-model
A continuación pasamos a los modelos.

Random effects using Paule-Mandel estimator since there are few studies with high heterogeneity

Knapp-Hartung Adjustments

Example of reporting method: “As we anticipated considerable between-study heterogeneity, a random-effects model was used to pool effect sizes. The restricted maximum likelihood estimator (Viechtbauer, 2005) was used to calculate the heterogeneity variance τ squared. We used Knapp-Hartung adjustments (Knapp & Hartung, 2003) to calculate the confidence interval around the pooled effect.” 
Así mismo, mirar la guía de la EMA de meta-análisis (poca cosa útil la verdad). 

En los advices se puede sacar ideas y alguna fómrmula para los modelos bayesianos:
1) Siponimod-Ofatumumab (MAP)
2) PF-06939926 (MAP)
3) SAR442168 (Power-Prior)

Tras darle muchas vueltas y viendo los datos he llegado a la siguiente conclusión:

1) Al ser un modelo de tiempo-hasta con un comportamiento multiplicativo, es quizás más adecuando considerar como prior para mis parámetros una log-normal que una gamma. No es erróneo lo otro, pero quizás pueda ser más adecuado (mirar por ejemplo el SAR442468). Hay que tener en cuenta que, creo, hay que limitar el parámetro sd, por ejemplo otra distribución uniforme de (0,1000), parece que hay problemas en el Bayesiano si no delimito la variabilidad.

Por esto, voy a proponer el mismo análisis que los anteriores y los que vienen ahora con usando esta prior.

2) Por otro lado, al estilo de lo que se usa en el SAR442468, se podría considerar directamente un modelo de Cox en vez de una regresión Weibull a la hora de analizar los datos. Digo esto, porque así quitamos complejidad al modelo y tenemos que especificar menos parámetros y nos quitamos el problema del shape común. Esto va de la mano el considerar como prior sólo una log-normal y hacer los análisis con el HR. Particularmente útil cuando haga la power prior. 

HACER PARA ANTES DE VACACIONES:

1) Ejecturar el MAP con un estudio y varios estudios teniendo en cuenta la heterogeneidad usando el método de Knapp. HECHO

2) Meter dentro del código el cálculo del ESS para cuantificar la info en términos de pacientes. LO ULTIMO

3) Hacer simulaciones usando estas prior para hacer lo de las características operantes. HECHO

4) Hacer 2 nuevos códigos de STAN: Weibull usando log-normal como prior. Asegurar que salen bien. LUEGO

5) Hacer 2 nuevos códigos de STAN: Cox usando log-normal como prior. Asegurar que salen bien. LUEGO

6) En función de lo que vea, hacer el mixture variando los pesos y evaluar las características operantes. Ya veo si lo hago con un modelo u otro. Aquí cambiar el STAN metiendo por defecto la misma prior no informativa (input el peso). HECHO

7) Ejecutar simulaciones. HECHO

8) Hacer lo de la power prior solo para el código de log-normal con Cox como en el SAR442168 usando sólo 1 fuente de info externa.(Cambiar código STAN claro). QUIZÁS NO


# Meta-Analytic Prior

1) MAP -> Un ensayo histórico. 

Esto es básicamente a poner como prior las asunciones de este ensayo. Se va a hacer con la exponencial en vez de con la weibull porque 1) aunque estamos usando una regresión weibull, la generación de datos se hace en función de una exponencial como está especificado en el SAP y 2) Al haber 2 parámetros que varían, es muy difícil elegir un valor para la scale del control porque estamos comparando diferentes shapes y la variable shape, de momento, voy a dejarlo como no informativo.

Son detalles porque afectará poco, pero al menos tenerlo en cuenta.

```{r MAP_1_Historical_Data , eval = T}

# Ensayo: Gemcitabine plus cisplatin vs. gemcitabine plus carboplatin in stage IIIb and IV non-small cell lung cancer: a phase III randomized trial

data_hist1 <- read.csv("IPD_KEYNOTE026_NSCLC_PFS_gemcis_gemcarbo.csv")
data_hist1$arm <- str_trim(data_hist1$arm)

gem_cis <- subset(data_hist1, arm == "GP")
gem_carbo <- subset(data_hist1, arm == "GC") # Este es el bueno para el control

exp_gem_carbo <- survreg(Surv(time, status) ~ 1, dist = "exponential", data = gem_carbo)

scale_param_gem_carbo <- exp(exp_gem_carbo$coefficients[1])
se_scale_gem_carbo <- sqrt(vcov(exp_gem_carbo)[1])


MAP <- rma.uni(yi = scale_param_gem_carbo, sei = se_scale_gem_carbo)
forest(MAP)

mu <- MAP$b[1]
sigma <- MAP$se

alpha <- (mu/sigma)^2
beta <- mu/sigma^2

curve(dgamma(x, shape=alpha, rate=beta), from=0, to=15, lwd=2, ylab="Density", xlab="Value", main="Gamma Density Plot")
grid()

```
Se puede ver por la densidad que la distribución es muy informativa con esta Gamma(2755.138, 451.531). Aquí no hacía falta hacer el MAP pero bueno, lo dejo así.

Cabe destacar que a diferencia de la mayoría de los meta-análisis, aquí no estoy sacando el efecto que se ha obtenido si no que estoy cogiendo los pseudos IPDs para luego hacer una regresión exponencial y entonces, así saco la escala para el brazo control para poner la prior en el análisis de la simulación.

2) MAP -> Varios ensayos históricos

Aquí se podría considerar un meta-análisis de 3 niveles agrupando por clusters cada uno de los diferentes SOCs.
Aunque como solo tenemos casi un ensayo por cada SOC casi mejor lo dejamos aunque si consigo que añada más variabilidad, mejor!

```{r MAP_Multiple_Historical_Data , eval = T}

# Ensayo: Gemcitabine plus cisplatin vs. gemcitabine plus carboplatin in stage IIIb and IV non-small cell lung cancer: a phase III randomized trial

data_hist1 <- read.csv("IPD_KEYNOTE026_NSCLC_PFS_gemcis_gemcarbo.csv")
data_hist1$arm <- str_trim(data_hist1$arm)

gem_cis <- subset(data_hist1, arm == "GP")
gem_carbo <- subset(data_hist1, arm == "GC") # Este es el bueno para el control

exp_gem_carbo <- survreg(Surv(time, status) ~ 1, dist = "exponential", data = gem_carbo)

n_gem_carbo <- dim(gem_carbo)[1]
scale_param_gem_carbo <- exp(exp_gem_carbo$coefficients[1])
se_scale_gem_carbo <- sqrt(vcov(exp_gem_carbo)[1])


# 2) Maintenance pemetrexed plus best supportive care versus placebo plus best supportive care for non-small-cell lung cancer: a randomised, double-blind, phase 3 study.

data_hist2 <- read.csv("IPD_KEYNOTE026_NSCLC_PFS_pem_bsc.csv")

exp_pem_bsc <- survreg(Surv(time, status) ~ 1, dist = "exponential", data = data_hist2)

n_pem_bsc <- dim(data_hist2)[1]
scale_param_pem_bsc <- exp(exp_pem_bsc$coefficients[1])
se_scale_pem_bsc <- sqrt(vcov(exp_pem_bsc)[1])


# 3) Maintenance therapy with pemetrexed plus best supportive care versus placebo plus best supportive care after induction therapy with pemetrexed plus cisplatin for advanced non-squamous non-small-cell lung cancer (PARAMOUNT): a double-blind, phase 3, randomised controlled trial. 

data_hist3 <- read.csv("IPD_POSEIDON_NSCLC_PFS_pem_bsc.csv")

exp_pem_bsc_poseidon <- survreg(Surv(time, status) ~ 1, dist = "exponential", data = data_hist3)

n_pem_bsc_poseidon <- dim(data_hist3)[1]
scale_param_pem_bsc_poseidon <- exp(exp_pem_bsc_poseidon$coefficients[1])
se_scale_pem_bsc_poseidon <- sqrt(vcov(exp_pem_bsc_poseidon)[1])

# Juntamos los datos

df <- data.frame(
  study = c('Zatloukal et al. (2003)', 'Ciuleanu et al. (2009)', 'Poseidon'),
  year = c(2003, 2009, 2011),
  ni = c(n_gem_carbo, n_pem_bsc, n_pem_bsc_poseidon),
  yi = c(scale_param_gem_carbo, scale_param_pem_bsc, scale_param_pem_bsc_poseidon),
  vi = c(se_scale_gem_carbo^2, se_scale_pem_bsc^2, se_scale_pem_bsc_poseidon^2),
  sei = c(se_scale_gem_carbo, se_scale_pem_bsc, se_scale_pem_bsc_poseidon)
)


# map <- rma(data = df, yi = yi, sei = sei, slab = df$study)
# 
# forest(map)
# forest(map, refline=1, col=c(rep("gray",2),"red"))

# “As we anticipated considerable between-study heterogeneity, a random-effects model was used to pool effect sizes. The restricted maximum likelihood estimator (Viechtbauer, 2005) was used to calculate the heterogeneity variance τ squared. We used Knapp-Hartung adjustments (Knapp & Hartung, 2003) to calculate the confidence interval around the pooled effect.” 
map_meta <- metagen(TE = df$yi, seTE = df$sei, studlab = df$study, data = df, prediction = TRUE, method.tau = "PM",test = "knha")           

map_meta$lower[1]
map_meta$upper[1]

forest.meta(map_meta,
       sortvar = TE,
       prediction = TRUE, 
       print.tau2 = FALSE,
       leftlabs = c("Study", "Scale parameter", "SE"),
       cex = 1.5,           
       lwd = 3,             
       xlim = c(5.5, 7.5),layout = "RevMan5")

# Para el plot siguiente hace falta esto para instalarlo:
# if (!require("remotes")) {
#   install.packages("remotes")
# }
# remotes::install_github("MathiasHarrer/dmetar")
# https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/heterogeneity.html
# Baujat, Bertrand, Cédric Mahé, Jean-Pierre Pignon, and Catherine Hill. 2002. “A Graphical Method for Exploring Heterogeneity in Meta-Analyses: Application to a Meta-Analysis of 65 Trials.” Statistics in Medicine 21 (18): 2641–52.

map_influence <- InfluenceAnalysis(map_meta, random = TRUE)
plot(map_influence, "baujat")

```
```{r MAP_Multiple_Historical_Data_Bayes , eval = T}

df <- data.frame(
  study = c('Zatloukal et al. (2003)', 'Ciuleanu et al. (2009)', 'Poseidon'),
  year = c(2003, 2009, 2011),
  ni = c(n_gem_carbo, n_pem_bsc, n_pem_bsc_poseidon),
  yi = c(scale_param_gem_carbo, scale_param_pem_bsc, scale_param_pem_bsc_poseidon),
  vi = c(se_scale_gem_carbo^2, se_scale_pem_bsc^2, se_scale_pem_bsc_poseidon^2),
  sei = c(se_scale_gem_carbo, se_scale_pem_bsc, se_scale_pem_bsc_poseidon)
)

#https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/bayesian-ma.html

# Estas priors son muy comunes para los meta-análisis

priors <- c(prior(normal(0,1), class = Intercept),
            prior(cauchy(0,0.5), class = sd))

brm_out <- brm( yi | se(sei) ~ 1 + (1 | study),
             data = df,
             prior = priors,
             iter = 4000)

out_f <- spread_draws(brm_out, b_Intercept) %>% 
  mutate(study = "Average")

out_r <- spread_draws(brm_out, r_study[study,term], b_Intercept) %>% 
  mutate(b_Intercept = r_study + b_Intercept)

avg_effects <- out_r %>% group_by(.iteration) %>% summarise(avg_r_study = mean(r_study, na.rm = TRUE))

out_f <- out_f %>%
  left_join(avg_effects, by = ".iteration") %>%
  mutate(b_Intercept = b_Intercept + avg_r_study)

out_all <- bind_rows(out_r, out_f) %>% 
  ungroup() %>%
  mutate(study = fct_relevel(study, "Average"),
         study = str_replace_all(study, "\\.", " "))

out_all_sum <- group_by(out_all, study) %>% 
  mean_qi(b_Intercept)

out_all %>%   
  ggplot(aes(b_Intercept, study)) +
  geom_vline(xintercept = 6.5, size = .25, lty = 2) +
  stat_halfeye(.width = c(.8, .95), fill = "dodgerblue") +
  geom_text(
    data = mutate_if(out_all_sum, is.numeric, round, 2),
    aes(label = str_glue("{b_Intercept} [{.lower}, {.upper}]"), x = 4.1),  
    hjust = "inward"
  ) +
  geom_point(
    data = df %>% mutate(study = str_replace_all(study, "\\.", " ")), 
    aes(x=yi), position = position_nudge(y = -.05), shape = 1
  ) +
  coord_cartesian(xlim = c(4, 9)) 

```
Ahora vamos a extraer los parámetros para especificar nuestras prior informativas basado en los MAPs.

```{r MAP_Multiple_Historical_Data_Bayes_Freq , eval = T}

# MAP Frecuentista

# Esto cuanta con toda la variabilidad, intra y externa


yi <- map_meta$TE # Efecto observado en el estudio i
vi <- map_meta$seTE^2  # Within-study variance for each study
tau2 <- map_meta$tau2  # Estimated between-study variance
mu <- map_meta$TE.random  # Overall average effect size in random-effects model

# Los efectos estimados para cada estudio
#Borenstein, M., Hedges, L. V., Higgins, J. P. T., & Rothstein, H. R. (2009). Introduction to Meta-Analysis. John Wiley & Sons.
theta_hat <- (1/vi + 1/tau2)^(-1) * (yi/vi + mu/tau2)

# Cuantificamos la variabilidad:
frequentist_sd <- sd(theta_hat)

# Esto calcula solo tau^2 que calcula la heterogeneida entre estudios pero no cuenta con la intra
# mean_estimate <- map_meta$TE.random
# CI_lower <- map_meta$lower.random
# CI_upper <- map_meta$upper.random
# 
# SE <- (CI_upper - CI_lower) / (2 * 1.96)
#SE <- SE*3
# Gamma parameters
alpha_map_freq <- (mu/frequentist_sd)^2
beta_map_freq  <- mu/frequentist_sd^2

## MAP Bayesiano ##

post_samples <- brms::posterior_samples(brm_out)

fixed_effects <- post_samples$b_Intercept

colnames(post_samples)[grep("r_study", colnames(post_samples))]


random_effects <- post_samples[, grep("r_study", colnames(post_samples))]

combined_effects <- sweep(as.matrix(random_effects), 2, fixed_effects, `+`)

true_avg_effect <- apply(combined_effects, 1, mean)

# Aqui dibujo la densidad de este resultado para ver cómo se distribuye
# dens <- density(true_avg_effect)
# plot(dens, main="Density of true_avg_effect", xlab="Value", ylab="Density")
# grid(col="gray", lty="dotted")


combined_mean <- mean(true_avg_effect)
combined_sd <- sd(true_avg_effect)

# Gamma
# Gamma distribution's parameters from sample statistics using the method of moments

alpha_map_bayes <- (combined_mean/combined_sd)^2
beta_map_bayes <- combined_mean / combined_sd^2

parameters_gamma <- data.frame(shape = c(0.00001, 1, 7.934823, 793.4823, alpha_map_freq, alpha_map_bayes),
                         rate = c( 0.00001, 1/6, 1, 100, beta_map_freq, beta_map_bayes),
                         label = c("Non-informative: Gamma(0.00001, 0.00001)", "Non-informative: Gamma(1, 1)", "Weak: Gamma(7.934823, 1)", "Informative: Gamma(793.4823, 100)", "MAP freq", "MAP bayes"))

gamma_density <- function(shape, rate, label) {
  data.frame(x = seq(0, 35, length.out = 100)) %>%
    mutate(density = dgamma(x, shape = shape, rate = rate),
           parameter = label)
}

densities <- purrr::pmap_df(parameters_gamma, gamma_density)

ggplot(densities, aes(x, density, color = parameter)) +
  geom_line() +
  labs(title = "Gamma Prior Distributions")

# El Bayesiano ofrece mucha más variabilidad!

## Cuando haga la prior log-normal comparamos todo

# Log-Normal

# lognorm_mu <- mean(log(posterior_samples))
# lognorm_sigma <- sd(log(posterior_samples))

# Como nota, usar una distribucion a posterior para formar una prior se llama empirical Bayes.
```
A continuación, vamos a evaluar las características operantes cogiendo los parámetros obtenido por el MAP tanto frecuentista como Bayesiano.

Características Operantes del MAP Frecuentista:

```{r `Bayes: Frequentist MAP`, eval = T}

shape_parameter <- 1 

control_medians <- c(3.5,3.75,4,4.25,4.5,4.75,5,5.25,5.5,5.75,6,6.25,6.5,6.75,7,7.25,7.5)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "Hazard Ratio")))

desired_HR = 0.55

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "Hazard Ratio"] = (lambda_control / lambda_treatment)^shape_parameter
}

medians <- medians[,1:2]

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- TRUE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_freq, beta_map_freq, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Strong informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
# 
# sim10 <-  sim_trials(n_sim = n_sim,
#                     analysis = "bayes",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# sim10[[2]]
# sim10[[3]]
# sim10[[4]]
# 
# sim100 <- sim10[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim100)
# saveWorkbook(wd, "MAP_freq_informative_gamma_300_2000_HR_055.xlsx", overwrite = TRUE)


sheet_names <- c("Sheet1")
MAP_freq <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_freq_300_2000_HR_055.xlsx", sheet = sheet_names[1])

```

Características Operantes del MAP Frecuentista:

```{r `Bayes: Frequentist MAP_CO1`, eval = T}

shape_parameter <- 1 

control_medians <- c(3.5,3.75,4,4.25,4.5,4.75,5,5.25,5.5,5.75,6,6.25,6.5,6.75,7,7.25,7.5)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "Hazard Ratio")))

desired_HR = 0.55

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "Hazard Ratio"] = (lambda_control / lambda_treatment)^shape_parameter
}

medians <- medians[,1:2]

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(control_medians), ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- TRUE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_bayes, beta_map_bayes, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Strong informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
# 
# sim11 <-  sim_trials(n_sim = n_sim,
#                     analysis = "bayes",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# sim11[[2]]
# sim11[[3]]
# sim11[[4]]
# 
# sim110 <- sim11[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim110)
# saveWorkbook(wd, "MAP_freq_300_2000_HR_055.xlsx", overwrite = TRUE)


sheet_names <- c("Sheet1")
MAP_bayes <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_300_2000_HR_055.xlsx", sheet = sheet_names[1])

```


```{r `Data gathering2 `, eval = T}

# MAP Frequentist
data_map_freq <- MAP_freq %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Frequentist")

data_filtered_diff_not_zero_map_freq <- data_map_freq %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_freq <- data_map_freq %>%
  filter(median_control == median_treatment)

# MAP Bayes
data_map_bayes <- MAP_bayes %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes")

data_filtered_diff_not_zero_map_bayes <- data_map_bayes %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes <- data_map_bayes %>%
  filter(median_control == median_treatment)


combined_data_diff_not_zero <- bind_rows(data_filtered_diff_not_zero_frequentist,
                                         data_filtered_diff_not_zero_non_informative, 
                                         data_filtered_diff_not_zero_informative, 
                                         data_filtered_diff_not_zero_mid_informative,
                                         data_filtered_diff_not_zero_map_freq,
                                         data_filtered_diff_not_zero_map_bayes)

combined_data_diff_zero <- bind_rows(data_filtered_diff_zero_frequentist,
                                     data_filtered_diff_zero_non_informative, 
                                     data_filtered_diff_zero_informative, 
                                     data_filtered_diff_zero_mid_informative,
                                     data_filtered_diff_zero_map_freq,
                                     data_filtered_diff_zero_map_bayes)

```

Quizás para ver mejor las diferencias podría hacerlo con un HR de 0.75 en vez de en 0.75 pero bueno, se puede distinguir el rendimiento.

En el gráfico de MSE vemos que el MAP frecuentista funciona muy bien en escenarios donde la mediana control está cerca de 4.5 pero el rendimiento va empeorando según se aleja, tiene un rendimiento parecido a la muy informativa (no tan bueno en valores centrales de su densidad porque no es tan informativa). Por otro lado, el MAP bayesiano tiene un mejor rendimiento a lo largo de todos los valores de la mediana, estando por debajo incluso de la weak, frecuentista, etc.

```{r MSE_w_MAP, eval = T}

p6 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = mean_MSE, color = Prior, linetype = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "MSE", x = "Median Control", y = "Mean MSE") +
  theme_minimal()  + 
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.075))
p6

```

Con respecto al poder, al ser un tamaño muestral muy alto para lo que se necesita es difícil distinguirlo. Aún así, se ven que los dos métodos tienen bastante poder.

```{r Power_w_MAP, eval = T}

p7 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "Power", x = "Median Control", y = "Prop. Significant") +
  theme_minimal() + 
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 1))
p7

```

Por último, el error de Tipo I: El MAP frecuentista se dispara según se aleja de 4.5 (media aproximada), podemos ver que tiene un peor comportamiento que la muy informativa. Coger alguna de las dos es como la lotería, si aciertas te llevas el premio gordo pero si no, el ET1 se dispara. Por eso es más importante ser conservador.

Por otro lado, el MAP Bayes infla el ET1 en valores superiores a 6. Aún así, no se dispara mucho y con un poco de ajuste podemos controlar este error mientras que guardamos el buen rendimiento de este modelo.

```{r T1E_w_MAP, eval = T}

p8 <- ggplot(combined_data_diff_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  geom_hline(yintercept = 0.025, linetype = "dashed", color = "green") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(title = "Type I Error", x = "Median Control", y = "Prop. Significant") +
  theme_minimal() +
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_zero$median_control), max(combined_data_diff_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.5))
p8

```


Ahora evaluamos los diferentes HRs para el MAP frecuentista

```{r ` MAP Freq Power changing HRs`, eval = T}

shape_parameter <- 1 

desired_HRs <- c(1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(5.5, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = dim(medians_control_fijo)[1], ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_freq, beta_map_freq, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim12 <-  sim_trials(n_sim = n_sim,
#                     analysis = "bayes",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim12)
# saveWorkbook(wd, "MAP_freq_300_2000_HR_changing.xlsx", overwrite = TRUE)



sheet_names <- c("Sheet1")
MAP_freq_HRs <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_freq_300_2000_HR_changing.xlsx", sheet = sheet_names[1])

```

Ahora evaluamos los diferentes HRs para el MAP Bayes

```{r `Bayes: MAP Bayes Power changing HRs`, eval = T}

shape_parameter <- 1 

desired_HRs <- c(1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(5.5, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = dim(medians_control_fijo)[1], ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_bayes, beta_map_bayes, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim13 <-  sim_trials(n_sim = n_sim,
#                     analysis = "bayes",
#                     sample_size = sample_size,
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax,
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim13)
# saveWorkbook(wd, "MAP_bayes_300_2000_HR_changing.xlsx", overwrite = TRUE)



sheet_names <- c("Sheet1")
MAP_bayes_HRs <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_300_2000_HR_changing.xlsx", sheet = sheet_names[1])

```

```{r `Data combined MAPs`, eval = T}

data_sim3_results <- sim3_results %>%
                      group_by(scenario) %>%
                      summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
                                mutate(Prior = "Frequentist")

 data_non_informative <- non_informative_gamma %>%
                         group_by(scenario) %>%
                         summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
                                mutate(Prior = "Non informative")

data_weak_informative_gamma_3.5 <- weak_informative_gamma_3.5 %>%
                                      group_by(scenario) %>%
                                      summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
                                            mutate(Prior = "Weak Informative: Median control at 3.5")
             

 data_informative_gamma_3.5 <- informative_gamma_3.5 %>%
                                group_by(scenario) %>%
                                summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                          mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
                                          mutate(Prior = "Informative: Median control at 3.5")
 
 data_weak_informative_gamma_5.5 <- weak_informative_gamma_5.5 %>%
                                      group_by(scenario) %>%
                                      summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
                                            mutate(Prior = "Weak Informative: Median control at 5.5")
   
 data_informative_gamma_5.5 <- informative_gamma_5.5 %>%
                                group_by(scenario) %>%
                                summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                          mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
                                          mutate(Prior = "Informative:  Median control at 5.5")
  
data_weak_informative_gamma_7.5 <- weak_informative_gamma_7.5 %>%
                                      group_by(scenario) %>%
                                      summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                            mean_HR = mean(mean_HR), mean_MSE = mean(MSE))%>%
                                            mutate(Prior = "Weak Informative: Median control at 7.5")      

 data_informative_gamma_7.5 <- informative_gamma_7.5 %>%
                                group_by(scenario) %>%
                                summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
                                          mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
                                          mutate(Prior = "Informative:  Median control at 7.5")  
 # MAP FREQ
 
  data_map_freq_HRs <- MAP_freq_HRs %>%
                                group_by(scenario) %>%
                                summarize(prop_significant = sum(count_significant) / (sum(count_significant) +                                          sum(count_not_significant)),
                                mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
                                mutate(Prior = "MAP FREQ") 
 
 # MAP BAYES

  data_map_bayes_HRs <- MAP_bayes_HRs %>%
                                group_by(scenario) %>%
                                summarize(prop_significant = sum(count_significant) / (sum(count_significant) +                                          sum(count_not_significant)),
                                mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
                                mutate(Prior = "MAP BAYES")   
  
 # 3.5
  combined_data_3.5 <- bind_rows(data_sim3_results,
                            #data_non_informative, 
                            data_weak_informative_gamma_3.5, 
                            data_informative_gamma_3.5,
                            data_map_freq_HRs,
                            data_map_bayes_HRs)

combined_data_3.5 <- combined_data_3.5 %>%
  group_by(Prior) %>%
  mutate(desired_HRs = (desired_HRs))

combined_data_3.5 <- combined_data_3.5 %>%
  mutate(diff_HR = abs(mean_HR - desired_HRs))

# 5.5
  combined_data_5.5 <- bind_rows(data_sim3_results,
                           # data_non_informative, 
                            data_weak_informative_gamma_5.5, 
                            data_informative_gamma_5.5,
                            data_map_freq_HRs,
                            data_map_bayes_HRs)


combined_data_5.5 <- combined_data_5.5 %>%
  group_by(Prior) %>%
  mutate(desired_HRs = (desired_HRs))

combined_data_5.5 <- combined_data_5.5 %>%
  mutate(diff_HR = abs(mean_HR - desired_HRs))

 # 7.5

  combined_data_7.5 <- bind_rows(data_sim3_results,
                         #   data_non_informative, 
                            data_weak_informative_gamma_7.5, 
                            data_informative_gamma_7.5,
                            data_map_freq_HRs,
                            data_map_bayes_HRs)

combined_data_7.5 <- combined_data_7.5 %>%
  group_by(Prior) %>%
  mutate(desired_HRs = (desired_HRs))

combined_data_7.5 <- combined_data_7.5 %>%
  mutate(diff_HR = abs(mean_HR - desired_HRs))

```

Dibujamos los de la mediana de 3.5

```{r `Simulation scenarios w fixed experimental & bayes w plot 3.5_2`, eval = T, echo = F}


p6 <- ggplot(combined_data_3.5, aes(x = desired_HRs, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.55, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_3.5$desired_HRs), max(combined_data_3.5$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_3.5$desired_HRs), max(combined_data_3.5$desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio w prior median control at 3.5",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 10),
        legend.position = "right")

p6 <- p6 +
  annotate("text", x = max(combined_data_3.5$desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(combined_data_3.5$desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green") +
  geom_label(aes(x = 1, y = Inf, label = "H0"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.55, y = Inf, label = "H1"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.5, y = Inf, label = "Real HR"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold")

p6

# Graficamos las diferencias

p7 <- ggplot(combined_data_3.5, aes(x = desired_HRs, y = diff_HR, color = Prior)) +
  geom_point(size = 3) +
  geom_line(aes(group = Prior), linewidth = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_3.5$desired_HRs), max(combined_data_3.5$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_3.5$desired_HRs), max(combined_data_3.5$desired_HRs), 0.1))) +
  labs(title = "Diff btw Simulated HR and Theoretical HRs w prior median control = 3.5",
       x = "Theoretical Hazard Ratio",
       y = "Difference") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 8, face = "bold"),
        axis.text = element_text(size = 8),
        legend.position = "right")

p7
```

Dibujamos los de la mediana de 5.5

```{r `Simulation scenarios w fixed experimental & bayes w plot 5.5_2`, eval = T, echo = F}


p8 <- ggplot(combined_data_5.5, aes(x = desired_HRs, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.55, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_5.5$desired_HRs), max(combined_data_5.5$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_5.5$desired_HRs), max(combined_data_5.5$desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio w prior median control at 5.5",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 10),
        legend.position = "right")

p8 <- p8 +
  annotate("text", x = max(combined_data_5.5$desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(combined_data_5.5$desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green") +
  geom_label(aes(x = 1, y = Inf, label = "H0"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.55, y = Inf, label = "H1"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.5, y = Inf, label = "Real HR"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold")

p8

# Graficamos las diferencias

p9 <- ggplot(combined_data_5.5, aes(x = desired_HRs, y = diff_HR, color = Prior)) +
  geom_point(size = 3) +
  geom_line(aes(group = Prior), linewidth = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_5.5$desired_HRs), max(combined_data_5.5$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_5.5$desired_HRs), max(combined_data_5.5$desired_HRs), 0.1))) +
  scale_y_continuous(limits = c(NA, 0.4)) + # Setting the y-axis max limit here
  labs(title = "Diff btw Simulated HR and Theoretical HRs w prior median control = 5.5",
       x = "Theoretical Hazard Ratio",
       y = "Difference") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 8, face = "bold"),
        axis.text = element_text(size = 8),
        legend.position = "right")

p9

```

Dibujamos los de la mediana de 7.5

```{r `Simulation scenarios w fixed experimental & bayes w plot 7.5_2`, eval = T, echo = F}


p10 <- ggplot(combined_data_7.5, aes(x = desired_HRs, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.55, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_7.5$desired_HRs), max(combined_data_7.5$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_7.5$desired_HRs), max(combined_data_7.5$desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio w prior median control at 7.5",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 10),
        legend.position = "right")

p10 <- p10 +
  annotate("text", x = max(combined_data_7.5$desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(combined_data_7.5$desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green") +
  geom_label(aes(x = 1, y = Inf, label = "H0"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.55, y = Inf, label = "H1"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.5, y = Inf, label = "Real HR"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold")

p10

# Graficamos las diferencias

p11 <- ggplot(combined_data_7.5, aes(x = desired_HRs, y = diff_HR, color = Prior)) +
  geom_point(size = 3) +
  geom_line(aes(group = Prior), linewidth = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_7.5$desired_HRs), max(combined_data_7.5$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_7.5$desired_HRs), max(combined_data_7.5$desired_HRs), 0.1))) +
  labs(title = "Diff btw Simulated HR and Theoretical HRs w prior median control = 7.5",
       x = "Theoretical Hazard Ratio",
       y = "Difference") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 8, face = "bold"),
        axis.text = element_text(size = 8),
        legend.position = "right")

p11
```

# Mixture Meta-Analytic Prior with non-informative

Para hacer el mixture es apropiado considerar la 2 diferentes priors, informativa usando MAP y no informativa, de la misma distribución.

Por otro lado, para elegir el % de información que se usa en una distribución y en otra, podemos hacer una simulación que vaya de 0.1 en 0.1.

En el 1) Siponimod-Ofatumumab (MAP) usan: Due to the great amount of data available for fingolimod (including paediatric data) 20% weight will be given to the non-informative component of the prior, while for ofatumumab and siponimod 50% weight will be given to the non-informative component.
Also in BB (pg 370): In detail, denote 􀀄􀀅􀀆 the predicted log-ARR in the pediatric population for treatment 􀀇 based on study ℎ and let 􀀉􀀅􀀊 be the standard error. Then, 􀀄􀀅􀀆 is modelled through a standard hierarchical
model 􀀄􀀅􀀆|􀀌􀀅􀀆 ∼ 􀀎(􀀌􀀅􀀆, 􀀉􀀅􀀆 􀀑 ) with 􀀌􀀅􀀆 = 􀀔􀀅 + 􀀖􀀆 and 􀀖􀀆 ∼ 􀀎(0, 􀀘􀀑). Here, 􀀎(0, 􀀘􀀑) denotes a normal distribution with mean zero and variance 􀀘􀀑. In this model, the log-ARRs from the same study share the study specific error term 􀀖􀀆 and each treatment is assigned a mean-parameter 􀀔􀀅. A vague normal distribution with a large variance, 􀀎(0, 49), is chosen as prior distribution for each of the means 􀀔􀀅 and a half-normal distribution, 􀀜􀀎(0, 􀀝 = 0.5), is considered as the prior distribution for the between-study standard deviation 􀀘.

2) PF-06939926 (MAP): The MAP prior is approximated using a normal distribution.  In addition a weakly informative component has been added to handle possible prior-data conflict (Schmidli, 2014)85.  The final prior for the placebo group is a 2-component normal mixture with apriori weight of 75% and 25% for the informative part and non-informative part respectively.  It has the functional form: 0.75*N(-4.508, 0.784^2) + 0.25*N(-4.508, 9^2).  The mixture weight and between trial heterogeneity (τ) parameters are calibrated to ensure good operating characteristics (false positive rate and power) for clinically plausible scenarios.

Por un lado tenemos:

1) Mixture Prior: Combinación de 2 priors

2) Robust Mixture Prior: Combinación de un MAP + Componente no informativo (the MAP prior is not available in analytical form. To allow for a concise description of the prior and tractable posterior analysis we approximate the MAP prior by a mixture of conjugate priors, with the Kullback–Leibler divergence as a measure of discrepancy.)

Ver también cuánto tamaño muestral nos ahorramos por cada calibración del peso puede ser interesante. Claro, pero aquí serían diferentes shape parameter para el componen.

Ahora vamos a ejecutar el código para obtener los resultados para diferentes mixture priors con diferentes w. 
W es el peso que se da a la parte informativa, en este caso obtenido en el MAP.
No tiene sentido proponer un diseño donde se de un peso de más del 50% a la parte informativa, por lo que este valor va a ser el cap.

Vamos a ir de 5 en 5 para el porcentaje de información para la parte informativa, i.e., 0.05, 0.1, 0.15, 0.20, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5.

Sólo se va a ensañar un código en el informe pero se ha realizado para todos los pesos y mostramos sus características.

Primero se van a mostrar los resultados usando un MAP hecho desde un punto de vista frecuentista. Así mismo, como tenía dudas de qué prior no informativa usar, he cogido 2:

- La anteriormente usada, una Gamma(1, 1/6)

- Otra nueva que es no informativa pero restringe a valores que son lógicos (e.g, aunque se pasa abarca puntos hasta que se permite la duración del ensayo ya que puntos más lejanos se censuran automáticamente). Esta es una Gamma(0.00001, 0.00001).

Para ilustrar primero el tema de las priors no informativas, a continuación vemos su forma usando diferentes pesos:

```{r Densities_mixture_MW2, eval = T}

# Dibujamos las densidades del mixture

gamma_density_1 <- function(x, alpha, beta) {
  dgamma(x, shape=alpha, rate=beta)
}

gamma_density_2 <- function(x) {
 # dgamma(x, shape=0.00001, rate=0.00001)
  dgamma(x, shape=1, rate=0.1666667)

}

gamma_density_3 <- function(x, alpha, beta) {
  dgamma(x, shape=alpha, rate=beta)
}

mixture_density_1 <- function(x, w1, alpha1, beta1) {
  w1 * gamma_density_1(x, alpha1, beta1) + (1-w1) * gamma_density_2(x)
}

mixture_density_2 <- function(x, w2, alpha2, beta2) {
  w2 * gamma_density_3(x, alpha2, beta2) + (1-w2) * gamma_density_2(x)
}

alpha1 = alpha_map_bayes
beta1 = beta_map_bayes
alpha2 = alpha_map_bayes
beta2 = beta_map_bayes
w1 = 0.15
w2 = 0.50

x_seq <- seq(0, 20, length.out = 400)
gamma1_densities <- gamma_density_1(x_seq, alpha1, beta1)
gamma2_densities <- gamma_density_2(x_seq)
gamma3_densities <- gamma_density_3(x_seq, alpha2, beta2)
mixture1_densities <- sapply(x_seq, function(x) mixture_density_1(x, w1, alpha1, beta1))
mixture2_densities <- sapply(x_seq, function(x) mixture_density_2(x, w2, alpha2, beta2))

df <- data.frame(x = x_seq, 
                 Gamma1 = gamma1_densities, 
                 Gamma2 = gamma2_densities,
                 Gamma3 = gamma3_densities,
                 Mixture_weight_0.15_informative_part = mixture1_densities,
                 Mixture_weight_0.5_informative_part = mixture2_densities)

ggplot(df, aes(x = x)) + 
  geom_line(aes(y = Gamma1, color = "Gamma 1")) + 
  geom_line(aes(y = Gamma2, color = "Gamma 2")) +
  geom_line(aes(y = Gamma3, color = "Gamma 3")) +
  geom_line(aes(y = Mixture_weight_0.15_informative_part, color = "Mixture_weight_0.15_informative_part")) +
  geom_line(aes(y = Mixture_weight_0.5_informative_part, color = "Mixture_weight_0.5_informative_part")) +
  labs(title = "Gamma Densities and Their Mixtures", 
       x = "x", 
       y = "Density", 
       color = "Density Legend") + 
  theme_minimal()

# Dibujamos todas las densidades inclyendo las mixture

parameters_gamma <- data.frame(shape = c(0.00001, 1, 7.934823, 793.4823, alpha_map_freq, alpha_map_bayes),
                         rate = c(0.00001, 1/6, 1, 100, beta_map_freq, beta_map_bayes),
                         label = c("Non-informative: Gamma(0.00001, 0.00001)", "Non-informative: Gamma(1, 1)", "Weak: Gamma(7.934823, 1)", "Informative: Gamma(793.4823, 100)", "MAP freq", "MAP bayes"))

gamma_density <- function(shape, rate, label) {
  data.frame(x = seq(0, 35, length.out = 400)) %>%
    mutate(density = dgamma(x, shape = shape, rate = rate),
           parameter = label)
}

densities <- purrr::pmap_df(parameters_gamma, gamma_density)

alpha1 = alpha_map_bayes  
beta1 = beta_map_bayes   
w1 = 0.15
w2 = 0.50

gamma_density_1 <- function(x, alpha, beta) {
  dgamma(x, shape=alpha, rate=beta)
}

gamma_density_2 <- function(x) {
  dgamma(x, shape=1, rate=0.1666667)
}

gamma_density_3 <- function(x, alpha, beta) {
  dgamma(x, shape=alpha, rate=beta)
}

Mixture_weight_0.15_informative_part <- function(x, w1, alpha1, beta1) {
  w1 * gamma_density_1(x, alpha1, beta1) + (1-w1) * gamma_density_2(x)
}

Mixture_weight_0.5_informative_part <- function(x, w2, alpha2, beta2) {
  w2 * gamma_density_3(x, alpha2, beta2) + (1-w2) * gamma_density_2(x)
}

x_seq <- seq(0, 20, length.out = 400) # Make sure this covers the range of both plots

mixture_df <- data.frame(
  x = x_seq,
  density = c(
    sapply(x_seq, function(x) Mixture_weight_0.15_informative_part(x, w1, alpha1, beta1)),
    sapply(x_seq, function(x) Mixture_weight_0.5_informative_part(x, w2, alpha1, beta1))
  ),
  parameter = c(rep("Mixture_weight_0.15_informative_part", length(x_seq)), rep("Mixture_weight_0.5_informative_part", length(x_seq)))
)

final_df <- bind_rows(densities, mixture_df)

ggplot(mixture_df, aes(x = x, y = density, color = parameter)) +
  geom_line() +
  labs(title = "Gamma Prior Distributions and Their Mixtures", 
       x = "x", 
       y = "Density", 
       color = "Density Legend") +
  theme_minimal()


```

Primero se hicieron unos resultados usando un MAP hecho desde un punto de vista frecuentista. Aún así, como se puede ver anteriormente en la sección de los MAPs, el frecuentista es muy informativo con poca variabilidad por lo que va a inflar el ET1 en muchos escenarios. Por eso, las simulaciones a partir de ahora están hechas con el MAP Bayesiano. De todos modos, este se podría dar artificialemtne más variabilidad para que sea menos informativa. Se podría ver.

```{r MAP_Freq_Mixture_Prior_w, eval = T}

shape_parameter <- 1 

control_medians <- c(3.5,3.75,4,4.25,4.5,4.75,5,5.25,5.5,5.75,6,6.25,6.5,6.75,7,7.25,7.5)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "Hazard Ratio")))

desired_HR = 0.55

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "Hazard Ratio"] = (lambda_control / lambda_treatment)^shape_parameter
}

medians <- medians[,1:2]
sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(medians), ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- TRUE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_freq, beta_map_freq, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Non informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_mixture_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = 0.15

# sim14 <-  sim_trials(n_sim = n_sim,
#                      analysis = "bayes",
#                      sample_size = sample_size,
#                      ratio = ratio,
#                      rand_type = rand_type,
#                      Tmax = Tmax,
#                      scenarios_eff = scenarios_eff,
#                      shape_parameter = shape_parameter,
#                      censor = censor,
#                      test.type = test.type,
#                      alpha = alpha,
#                      method_IA = method_IA,
#                      IA = IA,
#                      n_exp_events = n_exp_events,
#                      HR_1 = HR_1,
#                      Plot_Power = Plot_Power,
#                      modelo = modelo,
#                      modelo_bayes_test = modelo_bayes_test,
#                      prior_type = prior_type,
#                      P_HR_data_Boundary = P_HR_data_Boundary,
#                      prior_gamma = prior_gamma,
#                      Plot_Power_scenarios = Plot_Power_scenarios,
#                      desired_HRs = desired_HRs,
#                      plot_pvalues = plot_pvalues,
#                      Plot_Control_Scenarios = Plot_Control_Scenarios,
#                      w = w,
#                      seed = seed)
# 
# sim14[[2]]
# sim14[[3]]
# sim14[[4]]
# 
# sim140 <- sim14[[1]]

# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim140)
# saveWorkbook(wd, "MAP_bayes_mixture_w_0.15_300_2000_HR_changing.xlsx", overwrite = TRUE)

## Leemos los datos de las tablas obtenidas con el ordenador de Ferran ##

sheet_names <- c("Sheet1")
MAP_freq_mixture_w_0.15 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_mixture_w_0.15_300_2000_HR_changing.xlsx", sheet = sheet_names[1])
MAP_freq_mixture_w_0.25 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_mixture_w_0.25_300_2000_HR_changing.xlsx", sheet = sheet_names[1])

## Ahora juntamos los datos para graficarlos ##

# MAP Frequentist Mixture prior w weight of 0.15

data_map_freq_mixture_w_0.15 <- MAP_freq_mixture_w_0.15 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Frequentist Mixture prior w weight of 0.15")

data_filtered_diff_not_zero_map_freq_mixture_w_0.15 <- data_map_freq_mixture_w_0.15 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_freq_mixture_w_0.15 <- data_map_freq_mixture_w_0.15 %>%
  filter(median_control == median_treatment)

# MAP Frequentist Mixture prior w weight of 0.25

data_map_freq_mixture_w_0.25 <- MAP_freq_mixture_w_0.25 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Frequentist Mixture prior w weight of 0.25")

data_filtered_diff_not_zero_map_freq_mixture_w_0.25 <- data_map_freq_mixture_w_0.25 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_freq_mixture_w_0.25 <- data_map_freq_mixture_w_0.25 %>%
  filter(median_control == median_treatment)

combined_data_diff_not_zero <- bind_rows(data_filtered_diff_not_zero_frequentist,
                                         data_filtered_diff_not_zero_map_freq,
                                         data_filtered_diff_not_zero_map_freq_mixture_w_0.15,
                                         data_filtered_diff_not_zero_map_freq_mixture_w_0.25)

combined_data_diff_zero <- bind_rows(data_filtered_diff_zero_frequentist,
                                     data_filtered_diff_zero_map_freq,
                                     data_filtered_diff_zero_map_freq_mixture_w_0.15,
                                     data_filtered_diff_zero_map_freq_mixture_w_0.25)

## Ahora sacamos las gráficas de las características operantes usando como prior el MAP frecuentista con pesos ##

# Este es el gráfico de las densidades de las prior que se han considerado 

# Parte informativa: MAP frecuentista
gamma_density_1 <- function(x, alpha, beta) {
  dgamma(x, shape=alpha, rate=beta)
}

# Parte no informativa (antigua)
gamma_density_2 <- function(x) {
 dgamma(x, shape=0.00001, rate=0.00001)
}

# La unión de las dos prior con peso prespecificado
mixture_density_1 <- function(x, w1, alpha, beta) {
  w1 * gamma_density_1(x, alpha, beta) + (1-w1) * gamma_density_2(x)
}

mixture_density_2 <- function(x, w2, alpha2, beta2) {
  w2 * gamma_density_3(x, alpha, beta) + (1-w2) * gamma_density_2(x)
}

# Parámetros obtenidos del MAP frecuentista para la distribución Gamma
alpha = alpha_map_freq
beta = beta_map_freq

w1 = 0.15
w2 = 0.25

x_seq <- seq(0, 20, length.out = 400)
gamma1_densities <- gamma_density_1(x_seq, alpha, beta)
gamma2_densities <- gamma_density_2(x_seq)
mixture1_densities <- sapply(x_seq, function(x) mixture_density_1(x, w1, alpha, beta))
mixture2_densities <- sapply(x_seq, function(x) mixture_density_2(x, w2, alpha, beta))

df <- data.frame(x = x_seq, 
                 MAP_Frequentist = gamma1_densities, 
                 Non_Informative = gamma2_densities,
                 Mixture_weight_0.15 = mixture1_densities,
                 Mixture_weight_0.25 = mixture2_densities)

ggplot(df, aes(x = x)) + 
  geom_line(aes(y = MAP_Frequentist, color = "MAP_Frequentist")) + 
  geom_line(aes(y = Non_Informative, color = "Non-Informative")) +
  geom_line(aes(y = Mixture_weight_0.15, color = "Mixture_weight_0.15")) +
  geom_line(aes(y = Mixture_weight_0.25, color = "Mixture_weight_0.25")) +
  labs(title = "Gamma Pior Distributions and Their Mixtures", 
       x = "x", 
       y = "Density", 
       color = "Density Legend") + 
  theme_minimal()

# MSE

p6 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = mean_MSE, color = Prior, linetype = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "MSE", x = "Median Control", y = "Mean MSE") +
  theme_minimal()  + 
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.075))
p6

# Power

p7 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "Power", x = "Median Control", y = "Prop. Significant") +
  theme_minimal() + 
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 1))
p7

# ET1

p8 <- ggplot(combined_data_diff_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  geom_hline(yintercept = 0.025, linetype = "dashed", color = "green") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(title = "Type I Error", x = "Median Control", y = "Prop. Significant") +
  theme_minimal() +
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_zero$median_control), max(combined_data_diff_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.5))
p8

```

Tras ver los resultados anteriores, se ha decidido ignorar el MAP frecuentista ya que al haber tan poca variabilidad entre estudios (y la poca cantidad de estudios) la parte informative sale muy informativa, inflando el error de tipo 1, incluso en las mixture con poco peso. Esto se ve afectado por lo poco informativa que es la parte no informativa, dejándose llevar por cualquier tipo de evidencia por poca fuerza que tenga en la likelihood.

Es por ello que se ha llegado a dos conclusiones, 

1) Vamos a usar un MAP Bayesiando en vez de frecuentista (se podría usar el último aumentando la incertidumbre artificialmente) y 

2) Se va a coger una prior no informativa mucho más robusta, es decir que no permita dejarse llevar por cualquier ruido por poco sentido que tenga. Por ello, se va a coger una prior Ga(1, 1/6), permitiendo rangos plausibles que van más allá de la duración permitida del estudio y permitiendo tomar cualquier valor, en ese rango posible, haciéndola no informativa. Por otro lado y desde el punto de vista logístico, esta prior más robusta permite tardar mucho menos tiempo (de 11h a 6h) a la hora de obtener resultados, porque el algoritmo de MCMC no tiene que evaluar tantos puntos en un rangio tan amplio.

A continuación se muestran los resultados incorporando esta nueva decisión.


```{r MAP_Bayes_Mixture_Prior_w , eval = T}

shape_parameter <- 1 

control_medians <- c(3.5,3.75,4,4.25,4.5,4.75,5,5.25,5.5,5.75,6,6.25,6.5,6.75,7,7.25,7.5)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "Hazard Ratio")))

desired_HR = 0.55

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "Hazard Ratio"] = (lambda_control / lambda_treatment)^shape_parameter
}

medians <- medians[,1:2]
sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(medians), ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- TRUE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_bayes, beta_map_bayes, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Mixture prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_mixture_gamma_def_ga_robust.stan" ,verbose = F) # Este es el STAN con Ga(1, 1/6)
#modelo_weibull_sep <- rstan::stan_model("weibull_mixture_gamma_mult_def.stan" ,verbose = F) # Aquí evitamos el logmix


modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = 0.15

# sim14 <-  sim_trials(n_sim = n_sim,
#                      analysis = "bayes",
#                      sample_size = sample_size,
#                      ratio = ratio,
#                      rand_type = rand_type,
#                      Tmax = Tmax,
#                      scenarios_eff = scenarios_eff,
#                      shape_parameter = shape_parameter,
#                      censor = censor,
#                      test.type = test.type,
#                      alpha = alpha,
#                      method_IA = method_IA,
#                      IA = IA,
#                      n_exp_events = n_exp_events,
#                      HR_1 = HR_1,
#                      Plot_Power = Plot_Power,
#                      modelo = modelo,
#                      modelo_bayes_test = modelo_bayes_test,
#                      prior_type = prior_type,
#                      P_HR_data_Boundary = P_HR_data_Boundary,
#                      prior_gamma = prior_gamma,
#                      Plot_Power_scenarios = Plot_Power_scenarios,
#                      desired_HRs = desired_HRs,
#                      plot_pvalues = plot_pvalues,
#                      Plot_Control_Scenarios = Plot_Control_Scenarios,
#                      w = w,
#                      seed = seed)
# 
# sim14[[2]]
# sim14[[3]]
# sim14[[4]]
# 
# sim140 <- sim14[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim140)
# saveWorkbook(wd, "MAP_bayes_bayes_mixture_w_0.15_300_2000_HR_changing.xlsx", overwrite = TRUE)

## Leemos los datos de las tablas obtenidas con el ordenador de Ferran ##

sheet_names <- c("Sheet1")

MAP_bayes_mixture_w_0.05_Ga_Robust <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.05_300_2000_Ninf11.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.1_Ga_Robust <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.1_300_2000_Ninfo11.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.15_Ga_Robust <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.15_300_2000_Ninfo11.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.2_Ga_Robust <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.2_300_2000_Ninf11.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.25_Ga_Robust <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.25_300_2000_Ninfo11_HR_changing.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.3_Ga_Robust <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.3_300_2000_Ninfo11.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.35_Ga_Robust <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.35_300_2000_Ninf11.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.4_Ga_Robust <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.4_300_2000_Ninfo11.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.45_Ga_Robust <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.45_300_2000_Ninfo11.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.5_Ga_Robust <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.5_300_2000_Ninfo11.xlsx", sheet = sheet_names[1])


## Ahora juntamos los datos para graficarlos ##

# MAP Bayes Mixture prior with weight 0.05

data_map_bayes_mixture_w_0.05 <- MAP_bayes_mixture_w_0.05_Ga_Robust %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.05")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.05 <- data_map_bayes_mixture_w_0.05 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.05 <- data_map_bayes_mixture_w_0.05 %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.1

data_map_bayes_mixture_w_0.1 <- MAP_bayes_mixture_w_0.1_Ga_Robust %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.1")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.1 <- data_map_bayes_mixture_w_0.1 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.1 <- data_map_bayes_mixture_w_0.1 %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.15

data_map_bayes_mixture_w_0.15 <- MAP_bayes_mixture_w_0.15_Ga_Robust %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.15")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.15 <- data_map_bayes_mixture_w_0.15 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.15 <- data_map_bayes_mixture_w_0.15 %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.2

data_map_bayes_mixture_w_0.2 <- MAP_bayes_mixture_w_0.2_Ga_Robust %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.2")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.2 <- data_map_bayes_mixture_w_0.2 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.2 <- data_map_bayes_mixture_w_0.2 %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.25

data_map_bayes_mixture_w_0.25 <- MAP_bayes_mixture_w_0.25_Ga_Robust %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.25")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.25 <- data_map_bayes_mixture_w_0.25 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.25 <- data_map_bayes_mixture_w_0.25 %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.3

data_map_bayes_mixture_w_0.3 <- MAP_bayes_mixture_w_0.3_Ga_Robust %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.3")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.3 <- data_map_bayes_mixture_w_0.3 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.3 <- data_map_bayes_mixture_w_0.3 %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.35

data_map_bayes_mixture_w_0.35 <- MAP_bayes_mixture_w_0.35_Ga_Robust %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.35")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.35 <- data_map_bayes_mixture_w_0.35 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.35 <- data_map_bayes_mixture_w_0.35 %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.4

data_map_bayes_mixture_w_0.4 <- MAP_bayes_mixture_w_0.4_Ga_Robust %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.4")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.4 <- data_map_bayes_mixture_w_0.4 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.4 <- data_map_bayes_mixture_w_0.4 %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.45

data_map_bayes_mixture_w_0.45 <- MAP_bayes_mixture_w_0.45_Ga_Robust %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.45")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.45 <- data_map_bayes_mixture_w_0.45 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.45 <- data_map_bayes_mixture_w_0.45 %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.5

data_map_bayes_mixture_w_0.5 <- MAP_bayes_mixture_w_0.5_Ga_Robust %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.5")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.5 <- data_map_bayes_mixture_w_0.5 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.5 <- data_map_bayes_mixture_w_0.5 %>%
  filter(median_control == median_treatment)


combined_data_diff_not_zero <- bind_rows(data_filtered_diff_not_zero_frequentist,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.05,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.1,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.15,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.2,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.25,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.3,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.35,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.4,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.45,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.5)

combined_data_diff_zero <- bind_rows(data_filtered_diff_zero_frequentist,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.05,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.1,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.15,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.2,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.25,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.3,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.35,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.4,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.45,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.5)

## Ahora sacamos las gráficas de las características operantes usando como prior el MAP Bayesiano con pesos ##

# Este es el gráfico de las densidades de las prior que se han considerado 

# Parte informativa: MAP Bayes
gamma_density_1 <- function(x, alpha, beta) {
  dgamma(x, shape=alpha, rate=beta)
}

# Parte no informativa (robusta)
gamma_density_2 <- function(x) {
 dgamma(x, shape=1, rate=1/6)
}

# La unión de las dos prior con peso prespecificado
mixture_density <- function(x, w1, alpha, beta) {
  w1 * gamma_density_1(x, alpha, beta) + (1-w1) * gamma_density_2(x)
}

# Parámetros obtenidos del MAP frecuentista para la distribución Gamma
alpha = alpha_map_bayes
beta = beta_map_bayes

w1 = 0.05; w2 = 0.1; w3 = 0.15; w4 = 0.2; w5 = 0.25; w6 = 0.3; w7 = 0.35; w8 = 0.4; w9 = 0.45; w10 = 0.5

x_seq <- seq(0, 20, length.out = 400)
gamma1_densities <- gamma_density_1(x_seq, alpha, beta)
gamma2_densities <- gamma_density_2(x_seq)
mixture1_densities <- sapply(x_seq, function(x) mixture_density(x, w1, alpha, beta))
mixture2_densities <- sapply(x_seq, function(x) mixture_density(x, w2, alpha, beta))
mixture3_densities <- sapply(x_seq, function(x) mixture_density(x, w3, alpha, beta))
mixture4_densities <- sapply(x_seq, function(x) mixture_density(x, w4, alpha, beta))
mixture5_densities <- sapply(x_seq, function(x) mixture_density(x, w5, alpha, beta))
mixture6_densities <- sapply(x_seq, function(x) mixture_density(x, w6, alpha, beta))
mixture7_densities <- sapply(x_seq, function(x) mixture_density(x, w7, alpha, beta))
mixture8_densities <- sapply(x_seq, function(x) mixture_density(x, w8, alpha, beta))
mixture9_densities <- sapply(x_seq, function(x) mixture_density(x, w9, alpha, beta))
mixture10_densities <- sapply(x_seq, function(x) mixture_density(x, w10, alpha, beta))

df <- data.frame(x = x_seq, 
                 MAP_Bayes = gamma1_densities, 
                 Non_Informative = gamma2_densities,
                 Mixture_weight_0.05 = mixture1_densities,
                 Mixture_weight_0.1 = mixture2_densities,
                 Mixture_weight_0.15 = mixture3_densities,
                 Mixture_weight_0.2 = mixture4_densities,
                 Mixture_weight_0.25 = mixture5_densities,
                 Mixture_weight_0.3 = mixture6_densities,
                 Mixture_weight_0.35 = mixture7_densities,
                 Mixture_weight_0.4 = mixture8_densities,
                 Mixture_weight_0.45 = mixture9_densities,
                 Mixture_weight_0.5 = mixture10_densities)

ggplot(df, aes(x = x)) + 
  geom_line(aes(y = MAP_Bayes, color = "MAP_Bayes")) + 
  geom_line(aes(y = Non_Informative, color = "Non-Informative")) +
  geom_line(aes(y = Mixture_weight_0.05, color = "Mixture_weight_0.05")) +
  geom_line(aes(y = Mixture_weight_0.1, color = "Mixture_weight_0.1")) +
  geom_line(aes(y = Mixture_weight_0.15, color = "Mixture_weight_0.15")) +
  geom_line(aes(y = Mixture_weight_0.2, color = "Mixture_weight_0.2")) +
  geom_line(aes(y = Mixture_weight_0.25, color = "Mixture_weight_0.25")) +
  geom_line(aes(y = Mixture_weight_0.3, color = "Mixture_weight_0.3")) +
  geom_line(aes(y = Mixture_weight_0.35, color = "Mixture_weight_0.35")) +
  geom_line(aes(y = Mixture_weight_0.4, color = "Mixture_weight_0.4")) +
  geom_line(aes(y = Mixture_weight_0.45, color = "Mixture_weight_0.45")) +
  geom_line(aes(y = Mixture_weight_0.5, color = "Mixture_weight_0.5")) +
  labs(title = "Gamma Pior Distributions and Their Mixtures", 
       x = "x", 
       y = "Density", 
       color = "Density Legend") + 
  theme_minimal()

# MSE

p6 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = mean_MSE, color = Prior, linetype = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "MSE", x = "Median Control", y = "Mean MSE") +
  theme_minimal()  + 
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.075))
p6

# Power

p7 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "Power", x = "Median Control", y = "Prop. Significant") +
  theme_minimal() + 
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 1))
p7

# ET1

p8 <- ggplot(combined_data_diff_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  geom_hline(yintercept = 0.025, linetype = "dashed", color = "green") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(title = "Type I Error", x = "Median Control", y = "Prop. Significant") +
  theme_minimal() +
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_zero$median_control), max(combined_data_diff_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.5))
p8

```

En los resultados anteriores hemos visto un amplio rango de priors donde mediante una mixture, se ha propuesto una prior híbrida uniendo la parte informativa obtenida del MAP Bayesiano y la Parte no informativa con una Ga(1, 1/6). Para evaluar este rango, se han propuesto diferentes pesos que van desde el 0 (siendo esto simplemente una prior no informativa), hasta un peso del 0.5 yendo de 0.05 en 0.05. 

No se han valorado pesos más altos para la parte informativo porque no es algo que sea realista a la hora de hacer una propuesta seria, ya que no se aceptaría desde un punto de vista regulatorio. De todos modos, viendo valores de w con 0.45 y 0.5, vemos que el ET1 se infla en algunos puntos cuando la mediana es mayor de lo previsto en el tamaño muestral.

Con estos resultados, escoger una prior con pesos inferiores a 0.4 es realista ya que en estos escenarios cogiendo las asunciones del tamaño muestral, no se inflaría para un rango considerable.

Ahora hacemos lo mismo pero moviendo el HR en vez de la mediana del brazo control:

```{r `Mixture_Cambiando_HRs `, eval = T}

# Por ejemplo, con peso 0.15

shape_parameter <- 1 

desired_HRs <- c(1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(5.5, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = dim(medians_control_fijo)[1], ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_bayes, beta_map_bayes, 1, 1/6), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Mixture prior with w 0.15") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = 0.15

# sim16 <-  sim_trials(n_sim = n_sim,
#                      analysis = "bayes",
#                      sample_size = sample_size,
#                      ratio = ratio,
#                      rand_type = rand_type,
#                      Tmax = Tmax,
#                      scenarios_eff = scenarios_eff,
#                      shape_parameter = shape_parameter,
#                      censor = censor,
#                      test.type = test.type,
#                      alpha = alpha,
#                      method_IA = method_IA,
#                      IA = IA,
#                      n_exp_events = n_exp_events,
#                      HR_1 = HR_1,
#                      Plot_Power = Plot_Power,
#                      modelo = modelo,
#                      modelo_bayes_test = modelo_bayes_test,
#                      prior_type = prior_type,
#                      P_HR_data_Boundary = P_HR_data_Boundary,
#                      prior_gamma = prior_gamma,
#                      Plot_Power_scenarios = Plot_Power_scenarios,
#                      desired_HRs = desired_HRs,
#                      plot_pvalues = plot_pvalues,
#                      Plot_Control_Scenarios = Plot_Control_Scenarios,                     
#                      w = w,
#                      seed = seed)

## Leemos los datos de las tablas obtenidas con el ordenador de Ferran ##

sheet_names <- c("Sheet1")

MAP_bayes_mixture_w_0.05_Ga_Robust_HR_changing <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.05_300_2000_Ninfo11_diferente_HR.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.1_Ga_Robust_HR_changing <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.1_300_2000_Ninfo11_diferente_HR.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.15_Ga_Robust_HR_changing <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.15_300_2000_Ninfo11_diferente_HR.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.2_Ga_Robust_HR_changing <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.2_300_2000_Ninfo11_diferente_HR.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.25_Ga_Robust_HR_changing <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.25_300_2000_Ninfo11_diferente_HR.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.3_Ga_Robust_HR_changing <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.3_300_2000_Ninfo11_diferente_HR.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.35_Ga_Robust_HR_changing <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.35_300_2000_Ninfo11_diferente_HR.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.4_Ga_Robust_HR_changing <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.4_300_2000_Ninfo11_diferente_HR.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.45_Ga_Robust_HR_changing <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.45_300_2000_Ninfo11_diferente_HR.xlsx", sheet = sheet_names[1])

MAP_bayes_mixture_w_0.5_Ga_Robust_HR_changing <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.5_300_2000_Ninfo11_diferente_HR.xlsx", sheet = sheet_names[1])

## Ahora juntamos los datos para graficarlos ##

# MAP Bayes Mixture prior with weight 0.05

data_map_bayes_mixture_w_0.05_HR_changing <- MAP_bayes_mixture_w_0.05_Ga_Robust_HR_changing %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.05")

# MAP Bayes Mixture prior with weight 0.1

data_map_bayes_mixture_w_0.1_HR_changing <- MAP_bayes_mixture_w_0.1_Ga_Robust_HR_changing %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.1")

# MAP Bayes Mixture prior with weight 0.15

data_map_bayes_mixture_w_0.15_HR_changing <- MAP_bayes_mixture_w_0.15_Ga_Robust_HR_changing %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.15")

# MAP Bayes Mixture prior with weight 0.2

data_map_bayes_mixture_w_0.2_HR_changing <- MAP_bayes_mixture_w_0.2_Ga_Robust_HR_changing %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.2")


# MAP Bayes Mixture prior with weight 0.25

data_map_bayes_mixture_w_0.25_HR_changing <- MAP_bayes_mixture_w_0.25_Ga_Robust_HR_changing %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.25")


# MAP Bayes Mixture prior with weight 0.3

data_map_bayes_mixture_w_0.3_HR_changing <- MAP_bayes_mixture_w_0.3_Ga_Robust_HR_changing %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.3")


# MAP Bayes Mixture prior with weight 0.35

data_map_bayes_mixture_w_0.35_HR_changing <- MAP_bayes_mixture_w_0.35_Ga_Robust_HR_changing %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.35")


# MAP Bayes Mixture prior with weight 0.4

data_map_bayes_mixture_w_0.4_HR_changing <- MAP_bayes_mixture_w_0.4_Ga_Robust_HR_changing %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.4")

# MAP Bayes Mixture prior with weight 0.45

data_map_bayes_mixture_w_0.45_HR_changing <- MAP_bayes_mixture_w_0.45_Ga_Robust_HR_changing %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.45")


# MAP Bayes Mixture prior with weight 0.5

data_map_bayes_mixture_w_0.5_HR_changing <- MAP_bayes_mixture_w_0.5_Ga_Robust_HR_changing %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.5")


combined_data_mixture_HR_changing <- bind_rows(data_sim3_results,
                                               data_informative_gamma_3.5, 
                                               data_informative_gamma_5.5, 
                                               data_informative_gamma_7.5,
                                               data_map_bayes_mixture_w_0.05_HR_changing,
                                               data_map_bayes_mixture_w_0.1_HR_changing,
                                               data_map_bayes_mixture_w_0.15_HR_changing,
                                               data_map_bayes_mixture_w_0.2_HR_changing,
                                               data_map_bayes_mixture_w_0.25_HR_changing,
                                               data_map_bayes_mixture_w_0.3_HR_changing,
                                               data_map_bayes_mixture_w_0.35_HR_changing,
                                               data_map_bayes_mixture_w_0.4_HR_changing,
                                               data_map_bayes_mixture_w_0.45_HR_changing,
                                               data_map_bayes_mixture_w_0.5_HR_changing)

combined_data_mixture_HR_changing <- combined_data_mixture_HR_changing %>%
  group_by(Prior) %>%
  mutate(desired_HRs = (desired_HRs))

combined_data_mixture_HR_changing <- combined_data_mixture_HR_changing %>%
  mutate(diff_HR = abs(mean_HR - desired_HRs))

## Ahora sacamos las gráficas de las características operantes usando como prior el MAP Bayesiano con pesos ##

p6 <- ggplot(combined_data_mixture_HR_changing, aes(x = desired_HRs, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.55, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_mixture_HR_changing$desired_HRs), max(combined_data_mixture_HR_changing$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_mixture_HR_changing$desired_HRs), max(combined_data_mixture_HR_changing$desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio w mixture prior",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 10),
        legend.position = "right")

p6 <- p6 +
  annotate("text", x = max(combined_data_mixture_HR_changing$desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(combined_data_mixture_HR_changing$desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green") +
  geom_label(aes(x = 1, y = Inf, label = "H0"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.55, y = Inf, label = "H1"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.5, y = Inf, label = "Real HR"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold")

p6

# Graficamos las diferencias

p7 <- ggplot(combined_data_mixture_HR_changing, aes(x = desired_HRs, y = diff_HR, color = Prior)) +
  geom_point(size = 3) +
  geom_line(aes(group = Prior), linewidth = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_mixture_HR_changing$desired_HRs), max(combined_data_mixture_HR_changing$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_mixture_HR_changing$desired_HRs), max(combined_data_mixture_HR_changing$desired_HRs), 0.1))) +
  labs(title = "Diff btw Simulated HR and Theoretical HRs w prior mixture",
       x = "Theoretical Hazard Ratio",
       y = "Difference") +
  theme_minimal() +
  theme(plot.title = element_text(size = 8, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 6, face = "bold"),
        axis.text = element_text(size = 6),
        legend.position = "right")

p7


```

Como era de esperar, todos los resultados son iguales porque estamos usando la misma prior moviendo los HRs a diferencia de los anteriores gráficos donde hacíamos la prior informativa en diferentes puntos de la mediana para el brazo control. Sólo se ve rosa porque es el último en dibujarse pero el resto están debajo porque todos los resultados son iguales.

Como nota, se ha investigado modificar los parámetros de la prior para el brazo tratamiento, usando ahora una Ga(1,1/6) en vez de la Ga(0.00001, 0.00001) como se ha hecho hasta ahora.

Por otro lado, he hecho los mismos análisis para todos los pesos usando una Ga(1,1/6) para los dos brazos para la parte no informativa y también, una distribución más robusta y realista del parametro shape. Ahora para el shape de la regresión Weibull usamos una Uniforme(0.1, 4) en vez de la Ga(0.00001, 0.00001). Esta distribución es mucho más robusta y facilita los cálculos disminuyendo hasta en 1h30 los cálculos. Recordar que todos los valores del shape van a estar en el 0.7 y el 1.3 por lo que es factible.

Las tablas para todos los pesos para ver cuál protege más el ET1 están en Excel en la carpeta "PEMBROLIZUMAB_NSCLC_TABLAS" de este entorno.

He comparado viendo los diferentes resultados para cada uno de los pesos y haciendo un análisis descriptivo. Las diferencias son mínimas (de 3-4 décimas) pero sale un poquito mejor en términos de ET1 con una pérdida pequeñísima de poder usando una Ga(1,1/6) para los dos brazos y una U(0.1, 4) para el shape. Sin embargo, como parte muy positiva es que tiene mucho mejor rendimiento en términos de tiempo estas nuevas priors (1h30 menos que es bastante).

A continuación muestro las características operantes usando estos parámetros para las 3 variables aleatorias que entran en juego, brazo control, experimental y shape parameter de la weibull.

```{r MAP_Bayes_Mixture_Prior_w_parametros_robustos , eval = T}

shape_parameter <- 1 

control_medians <- c(3.5,3.75,4,4.25,4.5,4.75,5,5.25,5.5,5.75,6,6.25,6.5,6.75,7,7.25,7.5)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "Hazard Ratio")))

desired_HR = 0.55

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "Hazard Ratio"] = (lambda_control / lambda_treatment)^shape_parameter
}

medians <- medians[,1:2]
sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(medians), ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- TRUE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_bayes, beta_map_bayes, 0.00001, 0.00001), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Mixture prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_mixture_gamma_sh_uniform.stan" ,verbose = F) # Este es el STAN con Ga(1, 1/6)
#modelo_weibull_sep <- rstan::stan_model("weibull_mixture_gamma_mult_def.stan" ,verbose = F) # Aquí evitamos el logmix


modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = 0.15

# sim14 <-  sim_trials(n_sim = n_sim,
#                      analysis = "bayes",
#                      sample_size = sample_size,
#                      ratio = ratio,
#                      rand_type = rand_type,
#                      Tmax = Tmax,
#                      scenarios_eff = scenarios_eff,
#                      shape_parameter = shape_parameter,
#                      censor = censor,
#                      test.type = test.type,
#                      alpha = alpha,
#                      method_IA = method_IA,
#                      IA = IA,
#                      n_exp_events = n_exp_events,
#                      HR_1 = HR_1,
#                      Plot_Power = Plot_Power,
#                      modelo = modelo,
#                      modelo_bayes_test = modelo_bayes_test,
#                      prior_type = prior_type,
#                      P_HR_data_Boundary = P_HR_data_Boundary,
#                      prior_gamma = prior_gamma,
#                      Plot_Power_scenarios = Plot_Power_scenarios,
#                      desired_HRs = desired_HRs,
#                      plot_pvalues = plot_pvalues,
#                      Plot_Control_Scenarios = Plot_Control_Scenarios,
#                      w = w,
#                      seed = seed)
# 
# sim14[[2]]
# sim14[[3]]
# sim14[[4]]
# 
# sim140 <- sim14[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim140)
# saveWorkbook(wd, "MAP_bayes_bayes_mixture_w_0.15_300_2000_HR_changing.xlsx", overwrite = TRUE)

## Leemos los datos de las tablas obtenidas con el ordenador de Ferran ##

sheet_names <- c("Sheet1")

# Tiempo = 08h36m43s
MAP_bayes_mixture_w_0.05_def <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.05_300_2000_Ninfo112_Shape_uni.xlsx", sheet = sheet_names[1])

# Tiempo = 09h48m28s
MAP_bayes_mixture_w_0.1_def <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.1_300_2000_Ninfo112_Shape_uni.xlsx", sheet = sheet_names[1])

# Tiempo = 08h23m03s
MAP_bayes_mixture_w_0.15_def <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.15_300_2000_Ninfo112_Shape_uni.xlsx", sheet = sheet_names[1])

# Tiempo = 08h41m02s
MAP_bayes_mixture_w_0.2_def <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.2_300_2000_Ninfo112_Shape_uni.xlsx", sheet = sheet_names[1])

# Tiempo = 09h50m23s
MAP_bayes_mixture_w_0.25_def <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.25_300_2000_Ninfo112_Shape_uni.xlsx", sheet = sheet_names[1])

# Tiempo = 08h31m59s
MAP_bayes_mixture_w_0.3_def <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.3_300_2000_Ninfo112_Shape_uni.xlsx", sheet = sheet_names[1])

# Tiempo = 08h34m33s
MAP_bayes_mixture_w_0.35_def <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.35_300_2000_Ninfo112_Shape_uni.xlsx", sheet = sheet_names[1])

# Tiempo = 09h50m57s
MAP_bayes_mixture_w_0.4_def <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.4_300_2000_Ninfo112_Shape_uni.xlsx", sheet = sheet_names[1])

# Tiempo = 08h25m10s
MAP_bayes_mixture_w_0.45_def <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.45_300_2000_Ninfo112_Shape_uni.xlsx", sheet = sheet_names[1])

# Tiempo = 06h45m04s
MAP_bayes_mixture_w_0.5_def <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_w_0.5_300_2000_Ninfo112_Shape_uni.xlsx", sheet = sheet_names[1])


## Ahora juntamos los datos para graficarlos ##

# MAP Bayes Mixture prior with weight 0.05

data_map_bayes_mixture_w_0.05_def <- MAP_bayes_mixture_w_0.05_def %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.05")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.05_def <- data_map_bayes_mixture_w_0.05_def %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.05_def <- data_map_bayes_mixture_w_0.05_def %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.1

data_map_bayes_mixture_w_0.1_def <- MAP_bayes_mixture_w_0.1_def %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.1")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.1_def <- data_map_bayes_mixture_w_0.1_def %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.1_def <- data_map_bayes_mixture_w_0.1_def %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.15

data_map_bayes_mixture_w_0.15_def <- MAP_bayes_mixture_w_0.15_def %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.15")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.15_def <- data_map_bayes_mixture_w_0.15_def %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.15_def <- data_map_bayes_mixture_w_0.15_def %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.2

data_map_bayes_mixture_w_0.2_def <- MAP_bayes_mixture_w_0.2_def %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.2")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.2_def <- data_map_bayes_mixture_w_0.2_def %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.2_def <- data_map_bayes_mixture_w_0.2_def %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.25

data_map_bayes_mixture_w_0.25_def <- MAP_bayes_mixture_w_0.25_def %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.25")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.25_def <- data_map_bayes_mixture_w_0.25_def %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.25_def <- data_map_bayes_mixture_w_0.25_def %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.3

data_map_bayes_mixture_w_0.3_def <- MAP_bayes_mixture_w_0.3_def %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.3")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.3_def <- data_map_bayes_mixture_w_0.3_def %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.3_def <- data_map_bayes_mixture_w_0.3_def %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.35

data_map_bayes_mixture_w_0.35_def <- MAP_bayes_mixture_w_0.35_def %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.35")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.35_def <- data_map_bayes_mixture_w_0.35_def %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.35_def <- data_map_bayes_mixture_w_0.35_def %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.4

data_map_bayes_mixture_w_0.4_def <- MAP_bayes_mixture_w_0.4_def %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.4")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.4_def <- data_map_bayes_mixture_w_0.4_def %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.4_def <- data_map_bayes_mixture_w_0.4_def %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.45

data_map_bayes_mixture_w_0.45_def <- MAP_bayes_mixture_w_0.45_def %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.45")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.45_def <- data_map_bayes_mixture_w_0.45_def %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.45_def <- data_map_bayes_mixture_w_0.45_def %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with weight 0.5

data_map_bayes_mixture_w_0.5_def <- MAP_bayes_mixture_w_0.5_def %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w weight of 0.5")

data_filtered_diff_not_zero_map_bayes_mixture_w_0.5_def <- data_map_bayes_mixture_w_0.5_def %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_w_0.5_def <- data_map_bayes_mixture_w_0.5_def %>%
  filter(median_control == median_treatment)


combined_data_diff_not_zero <- bind_rows(data_filtered_diff_not_zero_frequentist,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.05_def,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.1_def,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.15_def,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.2_def,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.25_def,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.3_def,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.35_def,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.4_def,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.45_def,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.5_def)

combined_data_diff_zero <- bind_rows(data_filtered_diff_zero_frequentist,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.05_def,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.1_def,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.15_def,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.2_def,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.25_def,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.3_def,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.35_def,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.4_def,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.45_def,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.5_def)

## Ahora sacamos las gráficas de las características operantes usando como prior el MAP Bayesiano con pesos ##

# MSE

p6 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = mean_MSE, color = Prior, linetype = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "MSE", x = "Median Control", y = "Mean MSE") +
  theme_minimal()  + 
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.075))
p6

# Power

p7 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "Power", x = "Median Control", y = "Prop. Significant") +
  theme_minimal() + 
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 1))
p7

# ET1

p8 <- ggplot(combined_data_diff_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  geom_hline(yintercept = 0.025, linetype = "dashed", color = "green") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(title = "Type I Error", x = "Median Control", y = "Prop. Significant") +
  theme_minimal() +
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_zero$median_control), max(combined_data_diff_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.5))
p8

```


A continuación, en vez de seleccionar el peso de antemano, dejamos la variable peso (w) como otra variable aleatoria más. De esto modo, se va a elegir un peso u otro en función de los datos que se van obteniendo.

Para testar lo de antes se han cogido 4 modelos diferentes:

1) Brazo Control: Ga(1, 1/6), Brazo Tratamiento: Ga(0.00001, 0.00001) y Shape: Ga(0.00001, 0.00001). T=11h40min
2) Brazo Control: Ga(1, 1/6), Brazo Tratamiento: Ga(1, 1/6) y Shape: Ga(0.00001, 0.00001). T=9h31min
3) Brazo Control: Ga(1, 1/6), Brazo Tratamiento: Ga(0.00001, 0.00001) y Shape: U(0.1, 4). T=8h03min
4) Brazo Control: Ga(1, 1/6), Brazo Tratamiento: Ga(1, 1/6) Y Shape:  U(0.1, 4). T=8h01min -> BUENO

Como ha pasado arriba, protege mejor contra el ET1 una vez se pasa la mediana de 5.5 el modelo 4, ya que aunque se infla (sin pasar) en valores anteriores de 5.5, tiene mejor resultados una vez se pasa de 5.5 (que es lo preocupante). Aún así, las diferencias son mínimas entre los 4. Donde esta la mayor diferencia es en términos de tiempo como se ve arriba.

# Robust Mixture Prior with restricted and non-restricted random weight.

A continuación propongo 3 modelos más donde, en vez de prespecificar el peso que se va a usar para la parte informativa y la no informativa para cada uno de los ensayos simulados, se deja el peso como variable aleatoria. 

El peso va a seguir una distribución uniforme con valores que, como máximo, van a ir de 0 a 1. También se van a proponer modelos restringiendo el rango de valores de w que puede tomar esta variable en cada una de las simulaciones.

Los modelos son los siguientes:

1) Robust Mixture Prior con random weight siguiendo una Uniforme(0, 1)
2) Robust Mixture Prior con random weight siguiendo una Uniforme(0, 0.5)
3) Robust Mixture Prior con random weight siguiendo una Uniforme(0, 0.25)

Como puntualización, por simplicidad en este caso no se da una opción para el rango de pesos que se va a considerar para la variable aleatoria w. Es mucho más rápido cambiar el código STAN especificando estos rangos. Hay que fijarse cuándo uso un modelo STAN u otro para cada uno de los modelos.

```{r MAP_Bayes_Mixture_Prior_random_w , eval = T}

shape_parameter <- 1 

control_medians <- c(3.5,3.75,4,4.25,4.5,4.75,5,5.25,5.5,5.75,6,6.25,6.5,6.75,7,7.25,7.5)

medians <- matrix(NA, nrow = length(control_medians), ncol = 3, 
                  dimnames = list(NULL, c("Control", "Treatment", "Hazard Ratio")))

desired_HR = 0.55

for (i in 1:length(control_medians)) {
  medians[i, "Control"] <- control_medians[i]
  
  lambda_control = control_medians[i] / (log(2)^(1/shape_parameter))
  lambda_treatment = lambda_control / desired_HR^(1/shape_parameter)
  
  medians[i, "Treatment"] = lambda_treatment * (log(2)^(1/shape_parameter))
  medians[i, "Hazard Ratio"] = (lambda_control / lambda_treatment)^shape_parameter
}

medians <- medians[,1:2]
sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians, nrow = length(medians), ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- TRUE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = TRUE

analysis = "bayes"
n_sim <- 2
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_bayes, beta_map_bayes, 1, 1/6), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Mixture prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_mixture_gamma_random_w_def_ga_robust.stan" ,verbose = F)
# modelo_weibull_sep <- rstan::stan_model("weibull_mixture_gamma_random_w_0_0.5_shape_014_def.stan" ,verbose = F)
# modelo_weibull_sep <- rstan::stan_model("weibull_mixture_gamma_random_w_0_0.25_shape_014_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL

# sim16 <-  sim_trials(n_sim = n_sim,
#                      analysis = "bayes",
#                      sample_size = sample_size,
#                      ratio = ratio,
#                      rand_type = rand_type,
#                      Tmax = Tmax,
#                      scenarios_eff = scenarios_eff,
#                      shape_parameter = shape_parameter,
#                      censor = censor,
#                      test.type = test.type,
#                      alpha = alpha,
#                      method_IA = method_IA,
#                      IA = IA,
#                      n_exp_events = n_exp_events,
#                      HR_1 = HR_1,
#                      Plot_Power = Plot_Power,
#                      modelo = modelo,
#                      modelo_bayes_test = modelo_bayes_test,
#                      prior_type = prior_type,
#                      P_HR_data_Boundary = P_HR_data_Boundary,
#                      prior_gamma = prior_gamma,
#                      Plot_Power_scenarios = Plot_Power_scenarios,
#                      desired_HRs = desired_HRs,
#                      plot_pvalues = plot_pvalues,
#                      Plot_Control_Scenarios = Plot_Control_Scenarios,
#                      w = w,
#                      seed = seed)
# 
# sim16[[2]]
# sim16[[3]]
# sim16[[4]]
# 
# sim160 <- sim16[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim140)
# saveWorkbook(wd, "MAP_bayes_bayes_mixture_w_0.15_300_2000_HR_changing.xlsx", overwrite = TRUE)

## Leemos los datos de las tablas obtenidas con el ordenador de Ferran ##

sheet_names <- c("Sheet1")

# 1) Robust Mixture Prior con random weight siguiendo una Uniforme(0, 1)
# Tiempo = 08h38m10s
MAP_bayes_mixture_Random_w <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_Random_w_300_2000_Exp116_Shape014.xlsx", sheet = sheet_names[1])

# 2) Robust Mixture Prior con random weight siguiendo una Uniforme(0, 0.5)
# Tiempo = 14h41m58s
MAP_bayes_mixture_Random_w_restriccion_0.5 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_Random_w_restriccion_0.5_300_2000_Exp116_Shape014.xlsx", sheet = sheet_names[1])

# 3) Robust Mixture Prior con random weight siguiendo una Uniforme(0, 0.25)
# Tiempo = 16h01m23s
MAP_bayes_mixture_Random_w_restriccion_0.25 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_Random_w_restriccion_0.25_300_2000_Exp116_Shape014.xlsx", sheet = sheet_names[1])


# MAP Bayes Mixture prior with random weight

data_map_bayes_mixture_Random_w <- MAP_bayes_mixture_Random_w %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior Random weight")

data_filtered_diff_not_zero_map_bayes_mixture_random_w <- data_map_bayes_mixture_Random_w %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_random_w <- data_map_bayes_mixture_Random_w %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with random weight restriction to 0.5

data_map_bayes_mixture_Random_w_0.5 <- MAP_bayes_mixture_Random_w_restriccion_0.5 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior Random weight restricted to 0.5")

data_filtered_diff_not_zero_map_bayes_mixture_random_w_0.5 <- data_map_bayes_mixture_Random_w_0.5 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_random_w_0.5 <- data_map_bayes_mixture_Random_w_0.5 %>%
  filter(median_control == median_treatment)

# MAP Bayes Mixture prior with random weight restriction to 0.25

data_map_bayes_mixture_Random_w_0.25 <- MAP_bayes_mixture_Random_w_restriccion_0.25 %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior Random weight restricted to 0.25")

data_filtered_diff_not_zero_map_bayes_mixture_random_w_0.25 <- data_map_bayes_mixture_Random_w_0.25 %>%
  filter(median_control != median_treatment)

data_filtered_diff_zero_map_bayes_mixture_random_w_0.25 <- data_map_bayes_mixture_Random_w_0.25 %>%
  filter(median_control == median_treatment)



combined_data_diff_not_zero <- bind_rows(data_filtered_diff_not_zero_frequentist,
                                         data_filtered_diff_not_zero_map_bayes_mixture_random_w,
                                         data_filtered_diff_not_zero_map_bayes_mixture_random_w_0.25,
                                         data_filtered_diff_not_zero_map_bayes_mixture_random_w_0.5,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.15,
                                         data_filtered_diff_not_zero_map_bayes_mixture_w_0.5)

combined_data_diff_zero <- bind_rows(data_filtered_diff_zero_frequentist,
                                     data_filtered_diff_zero_map_bayes_mixture_random_w,
                                     data_filtered_diff_zero_map_bayes_mixture_random_w_0.25,
                                     data_filtered_diff_zero_map_bayes_mixture_random_w_0.5,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.15,
                                     data_filtered_diff_zero_map_bayes_mixture_w_0.5)

## Ahora sacamos las gráficas de las características operantes usando como prior el MAP Bayesiano con pesos ##

# MSE

p6 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = mean_MSE, color = Prior, linetype = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "MSE", x = "Median Control", y = "Mean MSE") +
  theme_minimal()  + 
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.075))
p6

# Power

p7 <- ggplot(combined_data_diff_not_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  labs(title = "Power", x = "Median Control", y = "Prop. Significant") +
  theme_minimal() + 
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_not_zero$median_control), max(combined_data_diff_not_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 1))
p7

# ET1

p8 <- ggplot(combined_data_diff_zero, aes(x = median_control, y = prop_significant, color = Prior)) +
  geom_line() +
  geom_vline(xintercept = median_control_center, linetype = "dashed") +
  geom_hline(yintercept = 0.025, linetype = "dashed", color = "green") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(title = "Type I Error", x = "Median Control", y = "Prop. Significant") +
  theme_minimal() +
      theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            axis.title = element_text(size = 14, face = "bold"),
            axis.text = element_text(size = 12)) +
  scale_color_discrete(name = "Priors") +
  scale_linetype_discrete(name = "Priors") +
  scale_x_continuous(breaks = seq(min(combined_data_diff_zero$median_control), max(combined_data_diff_zero$median_control), by = 0.5)) +
  scale_y_continuous(limits = c(0, 0.5))
p8


```

El random w sin restriccines infla el ET1 en uno de los puntos. Por otro lado, los modelos restringidos tienen mejor comportamiento que con el peso preespecificado a 0.15. Son modelos a tener en cuenta.

Por último, repetimos lo de mover los HRs.

```{r `Mixture_Cambiando_HRs_random_w`, eval = T}

# Por ejemplo, con peso 0.15

shape_parameter <- 1 

desired_HRs <- c(1.3, 1.2, 1.1, 1, 0.95, 0.9, 0.85, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5, 0.45, 0.4, 0.35, 0.3)
control_medians <- rep(5.5, times = length(desired_HRs))

medians_control_fijo <- matrix(NA, nrow = length(desired_HRs), ncol = 2, dimnames = list(paste0("HR ", desired_HRs), c("Control", "Treatment")))

for (i in 1:length(desired_HRs)) {
  medians_control_fijo[i, "Treatment"] <- control_medians[i] / (desired_HRs[i]^(1/shape_parameter))
  medians_control_fijo[i, "Control"] <- control_medians[i]
}

sample_size <- 300  
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- as.matrix(medians_control_fijo, nrow = dim(medians_control_fijo)[1], ncol = 2, byrow = TRUE)
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- FALSE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE

analysis = "bayes"
n_sim <- 2000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(alpha_map_bayes, beta_map_bayes, 1, 1/6), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Mixture prior with w 0.15") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_mixture_gamma_random_w_shape_014_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"
w = NULL

# sim16 <-  sim_trials(n_sim = n_sim,
#                      analysis = "bayes",
#                      sample_size = sample_size,
#                      ratio = ratio,
#                      rand_type = rand_type,
#                      Tmax = Tmax,
#                      scenarios_eff = scenarios_eff,
#                      shape_parameter = shape_parameter,
#                      censor = censor,
#                      test.type = test.type,
#                      alpha = alpha,
#                      method_IA = method_IA,
#                      IA = IA,
#                      n_exp_events = n_exp_events,
#                      HR_1 = HR_1,
#                      Plot_Power = Plot_Power,
#                      modelo = modelo,
#                      modelo_bayes_test = modelo_bayes_test,
#                      prior_type = prior_type,
#                      P_HR_data_Boundary = P_HR_data_Boundary,
#                      prior_gamma = prior_gamma,
#                      Plot_Power_scenarios = Plot_Power_scenarios,
#                      desired_HRs = desired_HRs,
#                      plot_pvalues = plot_pvalues,
#                      Plot_Control_Scenarios = Plot_Control_Scenarios,                     
#                      w = w,
#                      seed = seed)

## Leemos los datos de las tablas obtenidas con el ordenador de Ferran ##

sheet_names <- c("Sheet1")

# 1) Robust Mixture Prior con random weight siguiendo una Uniforme(0, 1)

MAP_bayes_mixture_random_w_HR_changing <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_Random_w_300_2000_HR_changing.xlsx", sheet = sheet_names[1])

data_map_bayes_mixture_random_w_HR_changing <- MAP_bayes_mixture_random_w_HR_changing %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior w random weight")

# 2) Robust Mixture Prior con random weight siguiendo una Uniforme(0, 0.5)
MAP_bayes_mixture_Random_w_restriccion_0.5_HR_changing <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_Random_w_restriccion_0.5_300_2000_HR_changing.xlsx", sheet = sheet_names[1])

data_map_bayes_mixture_random_w_restriccion_0.5_HR_changing <- MAP_bayes_mixture_Random_w_restriccion_0.5_HR_changing %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior Random weight restricted to 0.5")

# 3) Robust Mixture Prior con random weight siguiendo una Uniforme(0, 0.25)
MAP_bayes_mixture_Random_w_restriccion_0.25_HR_changing <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/MAP_bayes_bayes_mixture_Random_w_restriccion_0.25_300_2000_HR_changing.xlsx", sheet = sheet_names[1])

data_map_bayes_mixture_random_w_restriccion_0.25_HR_changing <- MAP_bayes_mixture_Random_w_restriccion_0.25_HR_changing %>%
  group_by(scenario) %>%
  summarize(prop_significant = sum(count_significant) / (sum(count_significant) + sum(count_not_significant)),
            mean_HR = mean(mean_HR), mean_MSE = mean(MSE)) %>%
  left_join(scenarios_eff_df, by = "scenario") %>%
  mutate(Prior = "MAP Bayes Mixture prior Random weight restricted to 0.25")

combined_data_mixture_HR_changing <- bind_rows(data_sim3_results,
                            data_informative_gamma_3.5, 
                            data_informative_gamma_5.5, 
                            data_informative_gamma_7.5,
                            data_map_bayes_mixture_w_0.5_HR_changing,
                            data_map_bayes_mixture_random_w_HR_changing,
                            data_map_bayes_mixture_random_w_restriccion_0.25_HR_changing,
                            data_map_bayes_mixture_random_w_restriccion_0.5_HR_changing)

combined_data_mixture_HR_changing <- combined_data_mixture_HR_changing %>%
  group_by(Prior) %>%
  mutate(desired_HRs = (desired_HRs))

combined_data_mixture_HR_changing <- combined_data_mixture_HR_changing %>%
  mutate(diff_HR = abs(mean_HR - desired_HRs))

## Ahora sacamos las gráficas de las características operantes usando como prior el MAP Bayesiano con pesos ##

p6 <- ggplot(combined_data_mixture_HR_changing, aes(x = desired_HRs, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.55, linetype = "dashed", color = "red", size = 1) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  geom_hline(yintercept = c(0.8, 0.9), linetype = "dotted", color = c("purple", "green"), size = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_mixture_HR_changing$desired_HRs), max(combined_data_mixture_HR_changing$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_mixture_HR_changing$desired_HRs), max(combined_data_mixture_HR_changing$desired_HRs), 0.1))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs Hazard Ratio w mixture prior",
       x = "Theoretical Hazard Ratio",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10, face = "bold"),
        axis.text = element_text(size = 10),
        legend.position = "right")

p6 <- p6 +
  annotate("text", x = max(combined_data_mixture_HR_changing$desired_HRs), y = 0.8, label = "80% power", hjust = 1, vjust = -0.5, size = 3.5, color = "purple") +
  annotate("text", x = max(combined_data_mixture_HR_changing$desired_HRs), y = 0.9, label = "90% power", hjust = 1, vjust = -0.5, size = 3.5, color = "green") +
  geom_label(aes(x = 1, y = Inf, label = "H0"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.55, y = Inf, label = "H1"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold") +
  geom_label(aes(x = 0.5, y = Inf, label = "Real HR"), vjust = 2, color = "black", fill = "white", size = 3.5, label.size = 0, fontface = "bold")

p6

# Graficamos las diferencias

p7 <- ggplot(combined_data_mixture_HR_changing, aes(x = desired_HRs, y = diff_HR, color = Prior)) +
  geom_point(size = 3) +
  geom_line(aes(group = Prior), linewidth = 1) +
  scale_x_continuous(breaks = seq(min(combined_data_mixture_HR_changing$desired_HRs), max(combined_data_mixture_HR_changing$desired_HRs), 0.1), labels = sprintf("%.2f", seq(min(combined_data_mixture_HR_changing$desired_HRs), max(combined_data_mixture_HR_changing$desired_HRs), 0.1))) +
  labs(title = "Diff btw Simulated HR and Theoretical HRs w prior mixture",
       x = "Theoretical Hazard Ratio",
       y = "Difference") +
  theme_minimal() +
  theme(plot.title = element_text(size = 8, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 6, face = "bold"),
        axis.text = element_text(size = 6),
        legend.position = "right")

p7


```

# Selección de Modelos para calcular el tamaño muestral con diferentes efectos de tratamiento.

Ahora vamos a elegir modelos tipos para ver cuántos pacientes nos ahorramos seleccionando algunos de los modelos anteriormente vistos. En todos los modelos se han hecho 5K simulaciones por cada tamaño muestral, mientras que en el frecuentista se ha hecho 100K (también hay otro de 5K pero ya que hecho el otro lo dejo):

1) Frecuentista (100K) -> T = 15h42m48s
2) Prior poco informativa centered at median 5.5 months:  Gamma(7.213475, 1) -> T = 44h07m59s
3) MAP Bayesiano -> T = 44h04m30s
4) Robust Mixture Prior: 0.15 Prior MAP + 0.85 Prior no informativa -> T = 41h32m37s
5) Robust Mixture Prior: 0.25 Prior MAP + 0.75 Prior no informativa -> T = 38h00m50s
6) Robust Mixture Prior: 0.4 Prior MAP + 0.6 Prior no informativa -> T = 37h53m44s
7) Robust Mixture Prior: Random weight -> T = 46m02m32s
8) Robust Mixture Prior: Random weight restringido (0-0.5) -> T = 70h19m01s

Usando estos modelos, vamos a ver cuántos pacientes son necesarios para obtener un poder del 80%, 85% y 90% de poder. Como para evaluar estos modelos se han usado diferentes asunciones para el efecto del brazo control, vamos a asumir 2 tipos de efectos para cada uno de los modelos a la hora de generar los datos:

1) Efecto que se especificó en el SAP para el brazo experimental y control
2) Efecto obtenido al final del estudio para el brazo experimental y control. Para este, tengo que modificar el código para poder tener 2 shapes diferentes a la hora de general los datos para cada uno de los brazos. Primero hago una regresión weibull para cada uno de los brazos por separado y ahí, lo incorporo en el código.

Luego hay que crear una tabla resumen con cada uno de los modelos y los números de pacientes necesarios para cada uno de los 3 poderes para cada uno de los efectos diferentes. Así mismo, se especificarán los valores de MSE, poder y T1E obtenidos anteriormente.

Sin necesidad de poner los inputs para todos los modelos ya que se han hecho anteriormente, aquí sólo muestro como son los inputs para el modelo frecuentista y uno de los modelos Bayesianos.

Como ya se han ejecutado, voy a leer los resultados directamente de todos los modelos moviendo todos los tamaños muestrales.

1) Frecuentista - SAP

```{r `Final models: Frequentist - SAP`, eval = T, echo = F}

shape_parameter <- 1
sample_size <- seq(from = 50, to= 300, by = 5)
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- matrix(c(5.5,10), nrow = 1, ncol = 2, byrow = TRUE) # HR=0.55
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- TRUE
IA <- NULL
Plot_Power = TRUE
plot_pvalues = FALSE
analysis = "freq" 
n_sim <- 100000
seed <- 24

# sim2 <- sim_trials(n_sim = n_sim, 
#                    analysis = "freq",
#                    sample_size = sample_size,
#                    ratio = ratio,
#                    rand_type = rand_type,
#                    Tmax = Tmax, 
#                    shape_parameter = shape_parameter,
#                    scenarios_eff = scenarios_eff,
#                    censor = censor,
#                    alpha = alpha,
#                    test.type = test.type,
#                    IA = IA,
#                    method_IA = method_IA,
#                    n_exp_events = n_exp_events,
#                    HR_1 = HR_1,
#                    seed=seed,
#                    Plot_Power = Plot_Power,
#                    plot_pvalues = plot_pvalues)
# 
# sim2_power <- sim2[[2]]
# sim2_power_T1E <- sim2[[3]]
# sim2_power_MSE <- sim2[[4]]
# sim2_results <- sim2[[1]]

# Aquí los datos se leen del Excel con los datos simulados en el ordenador de Ferran.

sheet_names <- c("Sheet1")
SAP_Freq_5K_SampleSizes <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/DEF_SAP_Freq_5K_SampleSizes.xlsx", sheet = sheet_names[1])

    
```


2) Bayes: Weak Prior centered at median 5.5 months, Gamma(7.213475, 1) - SAP

```{r `Final models: Bayes - Weak prior Gamma(7.213475, 1) - SAP`, eval = T}

sample_size <- seq(from = 50, to= 300, by = 5)
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- matrix(c(5.5,10), nrow = 1, ncol = 2, byrow = TRUE) # HR=0.55
censor <- c(0.1,0.1)
Tmax <- 20
alpha <- 0.05 
test.type <- 2
HR_1 <- TRUE
IA <- NULL
method_IA <- "bayes"
n_exp_events <- 175
Plot_Power <- TRUE
plot_pvalues = FALSE 
Plot_Power_scenarios = FALSE
Plot_Control_Scenarios = FALSE

analysis = "bayes"
n_sim <- 5000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(7.213475, 1, 1, 1/6), nrow = 2, ncol = 2, byrow = TRUE) 
prior_type <- c("Weak informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim6 <-  sim_trials(n_sim = n_sim,  
#                     analysis = "bayes",
#                     sample_size = sample_size, 
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax, 
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# sim6[[2]]
# sim6[[3]]
# sim6[[4]]
# 
# sim66 <- sim6[[1]]
# 
# wd <- createWorkbook()
# addWorksheet(wd, "Sheet1")
# writeData(wd, "Sheet1", sim66)
# saveWorkbook(wd, "weak_informative_gamma_300_2000_HR_055.xlsx", overwrite = TRUE)


sheet_names <- c("Sheet1")
weak_informative_gamma <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/weak_informative_gamma_300_2000_HR_055.xlsx", sheet = sheet_names[1])


```

# LECTURA RESULTADOS TAMAÑOS MUESTRALES DEL SAP

```{r `Final models: Lectura datos SAP`, eval = T}

sheet_names <- c("Sheet1")

# 1) Frecuentista (100K) -> T = 15h42m48s
DEF_SAP_Freq <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/DEF_SAP_Freq_5K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_SAP_Freq_Power <- subset(DEF_SAP_Freq, scenario == 1,
                                         select = c(scenario, sample_size, prop_significant)) %>%
                                         mutate(Prior = "Frequentist")
DEF_SAP_Freq_T1E <- subset(DEF_SAP_Freq, scenario == 2,
                                         select = c(scenario, sample_size, prop_significant)) %>%
                                         mutate(Prior = "Frequentist")

# 2) Prior weak informative centered at median 5.5 months:  Gamma(7.213475, 1) -> T = 44h07m59s
DEF_SAP_Weak_informative <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/DEF_SAP_Bayes_Weak_informative_5K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_SAP_Weak_informative_Power <- subset(DEF_SAP_Weak_informative, scenario == 1,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Weak Informative")

DEF_SAP_Weak_informative_T1E <- subset(DEF_SAP_Weak_informative, scenario == 2,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Weak Informative Prior")

# 3) MAP Bayesiano -> T = 44h04m30s
DEF_SAP_MAP <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/DEF_SAP_Bayes_MAP_5K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_SAP_MAP_Power <- subset(DEF_SAP_MAP, scenario == 1,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Meta-Analytic Prior (MAP)")
DEF_SAP_MAP_T1E <- subset(DEF_SAP_MAP, scenario == 2,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Meta-Analytic Prior (MAP)")

# 4) Robust Mixture Prior: 0.15 Prior MAP + 0.85 Prior no informativa -> T = 41h32m37s
DEF_SAP_Mixture_0.15 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/DEF_SAP_Bayes_Mixture_0.15_5K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_SAP_Mixture_0.15_Power <- subset(DEF_SAP_Mixture_0.15, scenario == 1,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Robust Mixture Prior w=0.15")
DEF_SAP_Mixture_0.15_T1E <- subset(DEF_SAP_Mixture_0.15, scenario == 2,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Robust Mixture Prior w=0.15")

# 5) Robust Mixture Prior: 0.25 Prior MAP + 0.75 Prior no informativa -> T = 38h00m50s
DEF_SAP_Mixture_0.25 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/DEF_SAP_Bayes_Mixture_0.25_5K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_SAP_Mixture_0.25_Power <- subset(DEF_SAP_Mixture_0.25, scenario == 1,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Robust Mixture Prior w=0.25")
DEF_SAP_Mixture_0.25_T1E <- subset(DEF_SAP_Mixture_0.25, scenario == 2,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Robust Mixture Prior w=0.25")

# 6) Robust Mixture Prior: 0.4 Prior MAP + 0.6 Prior no informativa -> T = 37h53m44s
DEF_SAP_Mixture_0.4 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/DEF_SAP_Bayes_Mixture_0.4_5K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_SAP_Mixture_0.4_Power <- subset(DEF_SAP_Mixture_0.4, scenario == 1,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Robust Mixture Prior w=0.40")
DEF_SAP_Mixture_0.4_T1E <- subset(DEF_SAP_Mixture_0.4, scenario == 2,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Robust Mixture Prior w=0.40")

# 7) Robust Mixture Prior: Random weight -> T = 46m02m32s
DEF_SAP_Mixture_Random_W <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/DEF_SAP_Bayes_Mixture_Random_w_5K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_SAP_Mixture_Random_W_Power <- subset(DEF_SAP_Mixture_Random_W, scenario == 1,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Robust Mixture Prior Random w")
DEF_SAP_Mixture_Random_W_T1E <- subset(DEF_SAP_Mixture_Random_W, scenario == 2,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Robust Mixture Prior Random w")

# 8) Robust Mixture Prior: Random weight restringido (0-0.5) -> T = 70h19m01s
DEF_SAP_Mixture_Restricted_Random_W <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/DEF_SAP_Bayes_Mixture_Random_w_0_a_0.5_5K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_SAP_Mixture_Restricted_Random_W_Power <- subset(DEF_SAP_Mixture_Restricted_Random_W, scenario == 1,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Robust Mixture Prior Restricted Random w")
DEF_SAP_Mixture_Restricted_Random_W_T1E <- subset(DEF_SAP_Mixture_Restricted_Random_W, scenario == 2,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Robust Mixture Prior Restricted Random w")

# Combinamos los datos 



combined_data_DEF_SAP_Power <- bind_rows(DEF_SAP_Freq_Power,
                                         DEF_SAP_Weak_informative_Power,
                                         DEF_SAP_MAP_Power,
                                         DEF_SAP_Mixture_0.15_Power,
                                         DEF_SAP_Mixture_0.25_Power,
                                         DEF_SAP_Mixture_0.4_Power,
                                         DEF_SAP_Mixture_Random_W_Power,
                                         DEF_SAP_Mixture_Restricted_Random_W_Power)

combined_data_DEF_SAP_T1E <- bind_rows(DEF_SAP_Freq_T1E,
                                     DEF_SAP_Weak_informative_T1E,
                                     DEF_SAP_MAP_T1E,
                                     DEF_SAP_Mixture_0.15_T1E,
                                     DEF_SAP_Mixture_0.25_T1E,
                                     DEF_SAP_Mixture_0.4_T1E,
                                     DEF_SAP_Mixture_Random_W_T1E,
                                     DEF_SAP_Mixture_Restricted_Random_W_T1E)

# Dibujamos el plot del poder:

desired_powers <- c(0.8, 0.85, 0.9, 0.95)
colors_for_dotted_lines <- c("purple", "blue", "green", "red") 

p1 <- ggplot(combined_data_DEF_SAP_Power, aes(x = sample_size, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +  
  geom_line(linewidth = 1) +  
  geom_hline(yintercept = desired_powers, linetype = "dotted", color = colors_for_dotted_lines, size = 1) + 
  scale_x_continuous(breaks = seq(min(combined_data_DEF_SAP_Power$sample_size)-5, max(combined_data_DEF_SAP_Power$sample_size)+5, 25)) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs. Sample Size",
       x = "Sample Size",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

rightmost_x <- max(combined_data_DEF_SAP_Power$sample_size) - 5  
for (i in 1:length(desired_powers)) {
  p1 <- p1 + annotate("text", 
                      x = rightmost_x, 
                      y = desired_powers[i], 
                      label = sprintf("%.0f%% power", desired_powers[i] * 100), 
                      hjust = 1,  
                      vjust = -0.3,  
                      size = 3.5, 
                      color = colors_for_dotted_lines[i])
}

p1

#Ahora hacemos una tabla para identificar cuál alcanza el poder antes.

desired_powers <- c(0.8, 0.85, 0.9, 0.95)

tabla <- function(df, target) {
  df %>%
    group_by(Prior) %>%
    filter(prop_significant >= target) %>%
    arrange(sample_size) %>%
    slice(1) %>%
    summarize(sample_size = first(sample_size), .groups = 'drop') %>%
    mutate(Power = as.character(target))  
}

results <- lapply(desired_powers, function(p) tabla(combined_data_DEF_SAP_Power, p))
results_df <- do.call(rbind, results)

Table_Power <- pivot_wider(
  results_df, 
  names_from = Prior, 
  values_from = sample_size, 
  id_cols = Power,
  values_fill = list(sample_size = NA)
)

Table_Power_df <- as.data.frame(Table_Power)

gt_table <- gt(Table_Power_df) %>%
  tab_header(title = "Sample Sizes using different Bayesian models") %>%
  cols_label(
  ) %>%
  fmt_number(
    columns = vars(-Power),  
    decimals = 0  
  ) %>%
  tab_spanner(
    label = "Prior Types",
    columns = vars(-Power)
  ) %>%
  tab_style(
    style = cell_text(weight = "bold", align = "center"),
    locations = cells_column_labels(columns = everything())
  ) %>%
  tab_options(
    column_labels.background.color = "lightblue",
    table.font.size = px(12),
    table.background.color = "white"
  ) %>%
  tab_style(
    style = cell_fill(color = "lightblue"),
    locations = cells_body(rows = seq(2, nrow(Table_Power_df), by = 2))
  ) %>%
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(1, nrow(Table_Power_df), by = 2))
  ) %>%
  tab_style(
    style = cell_text(align = "center"),
    locations = cells_body(columns = everything())
  )
gt_table


# Ahora tamaños muestrales para datos reales.

sample_sizes <- c(75, 90, 110, 115, 120, 125, 130, 135, 140, 150, 155, 160, 180, 185, 190, 200, 220, 300, 305)

# Dibujamos el plot del ET1:

desired_powers <- c(0.025, 0.05)
colors_for_dotted_lines <- c("green", "red") 

p2 <- ggplot(combined_data_DEF_SAP_T1E, aes(x = sample_size, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +  
  geom_line(linewidth = 1) +  
  geom_hline(yintercept = desired_powers, linetype = "dotted", color = colors_for_dotted_lines, size = 1) + 
  scale_x_continuous(breaks = seq(min(combined_data_DEF_SAP_T1E$sample_size)-5, max(combined_data_DEF_SAP_T1E$sample_size)+5, 25)) +
  scale_y_continuous(limits = c(0, 0.15)) +  
  labs(title = "T1E vs. Sample Size",
       x = "Sample Size",
       y = "T1E") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))


p2


```

# Datos Reales obtenidos en el ensayo

Para entender qué hubiera pasado si se hubieran aplicado estos modelos en el momento del diseño del estudio, es interesante analizar los datos reales obtenidos y ver cómo estos modelos se comportan con los datos obtenidos.

Tengo que mirar publicaciones de otros paquetes que se han hecho para ver qué más resultados puedo tener

Para ello se van a cambiar los parámetros necesarios, por ejemplo el tamaño muestral, el efecto obtenido finalmente, etc.

En los datos reales mirando los resultados en OS. Aquí está el resumen:

1) HR: 0.50 (95% CI: 0.37, 0.68)
2) Mediana real en meses del brazo control: 6.0 (4.2, 6.2) 
3) Mediana real en meses del brazo experimental: 10.3 (6.7, -) 

Por otro lado, para generar los datos simulados lo más realistas posibles se ha hecho lo siguiente:

1) Se han obtenido los datos paciente a paciente digitalizando las curvas para cada uno de los brazos
2) Se han sacado el fit para obtener los parámetros Shape y Scala de cada uno de los brazos.
3) Sólo se modifica el código de la función gen_surv_data.r común tanto para la simulación frecuentista como    Bayesiana. Hay que tener en cuenta que estoy forzando de manera artifical al poner estos parámetros dentro    de la función ya que el parámetro shape común y el parámetro de las eficacias se usan para diferentes    c    cosas y métricas, por lo que iba a ser muy jaleo modificar todo esto para ponerlo como input.
4) En el ordenador de Ferran también se actualiza ya que de ahí es de donde saco los resultados finales.
5) Se obtienen los resultados como siempre, la diferencia radica en que cada uno de los dataset obtenidos por    cada simulación van a ser mucho más realistas a los obtenidos finalemente.

Ahora tenemos un ejemplo de los nuevos parámetros que se van a considerar para la simulación frecuentista:

```{r `Freq ejemplo con datos reales`, eval = T}

shape_parameter <- 1 # Esto está obsoleto porque he puesto el bueno en la función de gen_surv_data.r
sample_size <- c(75, 90, 110, 115, 120, 125, 130, 135, 140, 150, 155, 160, 180, 185, 190, 200, 220, 300, 305)
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- matrix(c(6, 10.3), nrow = 1, ncol = 2, byrow = TRUE) # Esto quedará obsoleto por la incorporación en la función de gen_surv_data.r
censor <- c(0.15,0.2) # Las censuras se han hecho mirando la Tabla 10-2 de la disposición de los pacientes del CSR de este ensayo. He mirado las reglas de censura para el análisis primario y la tabla y sumando las siguientes discontinuaciones sin contar con las administrativas porque estas no son "aleatorias" que se han censurado según las normas: Adverse Event + Physician Decision + Withdrawal by subject. Se supone que Status Not Recorded son las administrativas porque tienen 74 en Pembro y 15 en SoC.

# 1) Pembro: 17+1+4 = 22 -> 22/154 = 0.14. Por simplificar lo dejo como 0.15.
# 2) SoC: 16+7+5 = 28 -> 28/151 = 0.1854. Por simplificar lo dejo como 0.2

Tmax <- 17 # Esto cambió al final
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE
IA <- NULL
Plot_Power = FALSE
plot_pvalues = FALSE
analysis = "freq" 
n_sim <- 10000
seed <- 24

# sim2 <- sim_trials(n_sim = n_sim, 
#                    analysis = "freq",
#                    sample_size = sample_size,
#                    ratio = ratio,
#                    rand_type = rand_type,
#                    Tmax = Tmax, 
#                    shape_parameter = shape_parameter,
#                    scenarios_eff = scenarios_eff,
#                    censor = censor,
#                    alpha = alpha,
#                    test.type = test.type,
#                    IA = IA,
#                    method_IA = method_IA,
#                    n_exp_events = n_exp_events,
#                    HR_1 = HR_1,
#                    seed=seed,
#                    Plot_Power = Plot_Power,
#                    plot_pvalues = plot_pvalues)
# 
# sim2

```

Y un ejemplo de un Bayesiano, en este caso el Weak:

```{r `Final models: Bayes - Weak prior Gamma(7.213475, 1) - SAP2`, eval = T}

shape_parameter <- 1 # Esto está obsoleto porque he puesto el bueno en la función de gen_surv_data.r
sample_size <- c(75, 90, 110, 115, 120, 125, 130, 135, 140, 150, 155, 160, 180, 185, 190, 200, 220, 300, 305)
ratio <- c(0.5,0.5)
rand_type <- "CR" 
scenarios_eff <- matrix(c(6, 10.3), nrow = 1, ncol = 2, byrow = TRUE) # Esto quedará obsoleto por la incorporación en la función de gen_surv_data.r
censor <- c(0.15,0.2) # Las censuras se han hecho mirando la Tabla 10-2 de la disposición de los pacientes del CSR de este ensayo. He mirado las reglas de censura para el análisis primario y la tabla y sumando las siguientes discontinuaciones sin contar con las administrativas porque estas no son "aleatorias" que se han censurado según las normas: Adverse Event + Physician Decision + Withdrawal by subject. Se supone que Status Not Recorded son las administrativas porque tienen 74 en Pembro y 15 en SoC.

# 1) Pembro: 17+1+4 = 22 -> 22/154 = 0.14. Por simplificar lo dejo como 0.15.
# 2) SoC: 16+7+5 = 28 -> 28/151 = 0.1854. Por simplificar lo dejo como 0.2

Tmax <- 17 # Esto cambió al final
alpha <- 0.05 
test.type <- 2
HR_1 <- FALSE # Esto no hace falta porque ya sabemos los datos reales
IA <- NULL
Plot_Power = FALSE
plot_pvalues = FALSE
analysis = "freq" 
n_sim <- 10000
seed <- 24
# 1) Shape control, 2) rate control, 3) Shape experimental y 4) rate experimental
prior_gamma <- matrix(c(7.213475, 1, 1, 1/6), nrow = 2, ncol = 2, byrow = TRUE) # Evidentemente esto no cambia porque sabíamos lo que sabíamos en el momento de diseñar el estudio pero no después.

prior_type <- c("Weak informative prior") # Este vector es para que aparezca el nombre en las tablas
P_HR_data_Boundary <- 0.8 # P(HR < 0.8 | data)
modelo_weibull_sep <- rstan::stan_model("weibull_gamma_def.stan" ,verbose = F)
modelo = modelo_weibull_sep
modelo_bayes_test = "weibull"

# sim6 <-  sim_trials(n_sim = n_sim,  
#                     analysis = "bayes",
#                     sample_size = sample_size, 
#                     ratio = ratio,
#                     rand_type = rand_type,
#                     Tmax = Tmax, 
#                     scenarios_eff = scenarios_eff,
#                     shape_parameter = shape_parameter,
#                     censor = censor,
#                     test.type = test.type,
#                     alpha = alpha,
#                     method_IA = method_IA,
#                     IA = IA,
#                     n_exp_events = n_exp_events,
#                     HR_1 = HR_1,
#                     Plot_Power = Plot_Power,
#                     modelo = modelo,
#                     modelo_bayes_test = modelo_bayes_test,
#                     prior_type = prior_type,
#                     P_HR_data_Boundary = P_HR_data_Boundary,
#                     prior_gamma = prior_gamma,
#                     Plot_Power_scenarios = Plot_Power_scenarios,
#                     desired_HRs = desired_HRs,
#                     plot_pvalues = plot_pvalues,
#                     Plot_Control_Scenarios = Plot_Control_Scenarios,
#                     seed = seed)
# sim6


```


# LECTURA RESULTADOS TAMAÑOS MUESTRALES DEL REAL

```{r `Final models: Lectura datos REAL`, eval = T}

sheet_names <- c("Sheet1")

# 1) Frecuentista (100K) -> T = 15h42m48s
DEF_REAL_Freq <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/DEF_REAL_Freq_100K_DEVERDAD_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_REAL_Freq_Power <- subset(DEF_REAL_Freq, scenario == 1,
                                         select = c(scenario, sample_size, prop_significant)) %>%
                                         mutate(Prior = "Frequentist")


# 2) Prior weak informative centered at median 5.5 months:  Gamma(7.213475, 1) -> T = 44h07m59s
DEF_REAL_Weak_informative <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/DEF_REAL_Bayes_Weak_informative_10K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_REAL_Weak_informative_Power <- subset(DEF_REAL_Weak_informative, scenario == 1,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Weak Informative")

# 3) MAP Bayesiano -> T = 44h04m30s
DEF_REAL_MAP <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/DEF_REAL_Bayes_MAP_10K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_REAL_MAP_Power <- subset(DEF_REAL_MAP, scenario == 1,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Meta-Analytic Prior (MAP)")

# 4) Robust Mixture Prior: 0.15 Prior MAP + 0.85 Prior no informativa -> T = 41h32m37s
DEF_REAL_Mixture_0.15 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/DEF_REAL_Bayes_Mixture_0.15_10K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_REAL_Mixture_0.15_Power <- subset(DEF_REAL_Mixture_0.15, scenario == 1,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Robust Mixture Prior w=0.15")


# 5) Robust Mixture Prior: 0.25 Prior MAP + 0.75 Prior no informativa -> T = 38h00m50s
DEF_REAL_Mixture_0.25 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/DEF_REAL_Bayes_Mixture_0.25_10K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_REAL_Mixture_0.25_Power <- subset(DEF_REAL_Mixture_0.25, scenario == 1,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Robust Mixture Prior w=0.25")

# 6) Robust Mixture Prior: 0.4 Prior MAP + 0.6 Prior no informativa -> T = 37h53m44s
DEF_REAL_Mixture_0.4 <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/DEF_REAL_Bayes_Mixture_0.4_10K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_REAL_Mixture_0.4_Power <- subset(DEF_REAL_Mixture_0.4, scenario == 1,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Robust Mixture Prior w=0.40")

# 7) Robust Mixture Prior: Random weight -> T = 46m02m32s
DEF_REAL_Mixture_Random_W <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/DEF_REAL_Bayes_Mixture_Random_w_10K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_REAL_Mixture_Random_W_Power <- subset(DEF_REAL_Mixture_Random_W, scenario == 1,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Robust Mixture Prior Random w")

# 8) Robust Mixture Prior: Random weight restringido (0-0.5) -> T = 70h19m01s
DEF_REAL_Mixture_Restricted_Random_W <- read_excel("C:/Users/borja/Documents/PhD/Simulaciones/Bayes/Simulacion_Survival/PEMBROLIZUMAB_NSCLC_TABLAS/DEF_REAL_Bayes_Mixture_Random_w_0_a_0.5__10K_SampleSizes.xlsx", sheet = sheet_names[1])
DEF_REAL_Mixture_Restricted_Random_W_Power <- subset(DEF_REAL_Mixture_Restricted_Random_W, scenario == 1,
                                         select = c(scenario, sample_size, Prior, prop_significant)) %>%
                                         mutate(Prior = "Robust Mixture Prior Restricted Random w")

# Combinamos los datos 



combined_data_DEF_REAL_Power <- bind_rows(DEF_REAL_Freq_Power,
                                         DEF_REAL_Weak_informative_Power,
                                         DEF_REAL_MAP_Power,
                                         DEF_REAL_Mixture_0.15_Power,
                                         DEF_REAL_Mixture_0.25_Power,
                                         DEF_REAL_Mixture_0.4_Power,
                                         DEF_REAL_Mixture_Random_W_Power,
                                         DEF_REAL_Mixture_Restricted_Random_W_Power)



# Dibujamos el plot del poder:

desired_powers <- c(0.8, 0.85, 0.9, 0.95)
colors_for_dotted_lines <- c("purple", "blue", "green", "red") 

p1 <- ggplot(combined_data_DEF_REAL_Power, aes(x = sample_size, y = prop_significant, group = Prior, color = Prior)) +
  geom_point(size = 3) +  
  geom_line(linewidth = 1) +  
  geom_hline(yintercept = desired_powers, linetype = "dotted", color = colors_for_dotted_lines, size = 1) + 
  scale_x_continuous(breaks = seq(min(combined_data_DEF_REAL_Power$sample_size)-5, max(combined_data_DEF_REAL_Power$sample_size)+5, 25)) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Power vs. Sample Size",
       x = "Sample Size",
       y = "Power") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12))

rightmost_x <- max(combined_data_DEF_REAL_Power$sample_size) - 5  
for (i in 1:length(desired_powers)) {
  p1 <- p1 + annotate("text", 
                      x = rightmost_x, 
                      y = desired_powers[i], 
                      label = sprintf("%.0f%% power", desired_powers[i] * 100), 
                      hjust = 1,  
                      vjust = -0.3,  
                      size = 3.5, 
                      color = colors_for_dotted_lines[i])
}

p1

#Ahora hacemos una tabla para identificar cuál alcanza el poder antes.

combined_data_DEF_REAL_Power <- combined_data_DEF_REAL_Power %>%
  mutate(sample_size = as.integer(sample_size)) %>%
  select(-scenario)

Table_Power <- combined_data_DEF_REAL_Power %>%
  pivot_wider(
    names_from = Prior,
    values_from = prop_significant,
    id_cols = sample_size,
    values_fill = list(prop_significant = NA)
  )

Table_Power_df <- as.data.frame(Table_Power)

gt_table2 <- gt(Table_Power_df) %>%
  tab_header(title = "Proportion Significant by Sample Size and Prior Type") %>%
  cols_label(
  ) %>%
  fmt_number(
    columns = vars(-sample_size), 
    decimals = 4  
  ) %>%
  tab_spanner(
    label = "Prior Types",
    columns = vars(-sample_size)
  ) %>%
  tab_style(
    style = cell_text(weight = "bold", align = "center"),
    locations = cells_column_labels(columns = everything())
  ) %>%
  tab_options(
    column_labels.background.color = "lightblue",
    table.font.size = px(12),
    table.background.color = "white"
  ) %>%
  tab_style(
    style = cell_fill(color = "lightblue"),
    locations = cells_body(rows = seq(2, nrow(Table_Power_df), by = 2))
  ) %>%
  tab_style(
    style = cell_fill(color = "white"),
    locations = cells_body(rows = seq(1, nrow(Table_Power_df), by = 2))
  ) %>%
  tab_style(
    style = cell_text(align = "center"),
    locations = cells_body(columns = everything())
  )

gt_table2


```


Se está haciendo.

Sólo queda hacer gráficos y tablas para el ET1. Mientras salen los resultados de los reales voy buscando nuevos estudios y voy repitiendo los procesos!!

El tercer estudio va a ser el BlenRep. El segundo tengo que ver algo de GIST.

POTENCIALES PRÓXIMOS PASOS:

1) Se está ejecutando ahora otros modelos donde el peso (w) es una variable aleatoria pero en vez de usar una dist uniforme de 0 a 1, lo restrinjo de 0 a 0.5 y de 0 a 0.25. Seguramente esto solucione la pequeña inflación del error de tipo I. HECHO

2) Usar Power prior incorporando pseudo datos paciente a paciente (digitalizando curvas KM). Esto serviría para meter datos externos dentro del análisis en función de cómo se parece la asunción previa (prior distribution) y los datos obtenidos en la muestra (likelihood). Esto es diferente ya que las priors no se tocan, se modifica la likelihood, es decir, la manera en que se analizan los datos. NO HACE FALTA

3) Quizás en vez de analizar los datos usando una regresión weibull, usar un Cox. Tendría que volver a simular todo y cambiar las priors (de gamma a log-normales) pero bueno. No espero que los resultados cambien mucho con respecto a lo que está hecho. Básicamente elegí usar una regresión Weibull porque mis datos simulados vienen de la dist weibull (en la vida real no sabemos qué dist. tienen pero bueno, es un ejercicio de simulación). DE MOMENTO NO

4) Tenía pensado incorporar una medida para traducir cualquier prior en términos de nº de pacientes, el término es Effective Sample Size (ESS). Sin embargo, como voy a hacer el siguiente punto, que es lo mismo pero más robusto, ahora no le veo mucho sentido complicarme más. NO HACE FALTA

5) Elegir mejores modelos y hacer simulación con diferentes tamaños muestrales para ver cuánto nos podemos ahorrar en términos de tamaño muestral. El criterio para elegir sería aquellos que no inflen el ET1, tengan más poder (o distintos tipos de métodos) y un MSE que esté bien. HECHO

6) Hacer otro estudio con otros parámetros (al estar construida la herramienta ya es sólo esperar los resultados que simule).

7) Necesario traducir todo esto a variables de respuesta o binarias? Es suficiente con lo que he hecho? NO HACE FALTA

